## What is PISA?

## WHAT IS PISA?

"What is important for citizens to know and be able to do?" In response to that question and to the need for cross-nationally comparable evidence on student performance, the Organisation for Economic Co-operation and Development (OECD) launched the Programme for International Student Assessment (PISA) in 1997. PISA assesses the extent to which 15-year-old students, near the end of their compulsory education, have acquired key knowledge and skills that are essential for full participation in modern societies.

The triennial assessment focuses on the core school subjects of science, reading and mathematics. Students' proficiency in an innovative domain is also assessed (in 2015, this domain is collaborative problem solving). The assessment does not just ascertain whether students can reproduce knowledge; it also examines how well students can extrapolate from what they have learned and can apply that knowledge in unfamiliar settings, both in and outside of school. This approach reflects the fact that modern economies reward individuals not for what they know, but for what they can do with what they know.

PISA is an ongoing programme that, over the longer term, will lead to the development of a body of information for monitoring trends in the knowledge and skills of students in various countries as well as in different demographic subgroups of each country. In each round of PISA, one of the core domains is tested in detail, taking up nearly two-thirds of the total testing time. The major domain in 2015 is science, as it was in 2006. Reading was the major domain in 2000 and 2009, and mathematics was the major domain in 2003 and 2012.

Through questionnaires distributed to students, parents, school principals and teachers, PISA also gathers information about students' home background, their approaches to learning and their learning environments.

With this alternating schedule of major domains, a thorough analysis of achievement in each of the three core areas is presented every nine years; an analysis of trends is offered every three years. Combined with the information gathered through the various questionnaires, the PISA assessment provides three main types of outcomes:

- Basic indicators that provide a baseline profile of the knowledge and skills of students.
- Indicators derived from the questionnaires that show how such skills relate to various demographic, social, economic and educational variables.
- Indicators on trends that show changes in outcome levels and distributions, and in relationships between student-level, school-level and system-level background variables and outcomes.

Policy makers around the world use PISA findings to gauge the knowledge and skills of students in their own country/economy in comparison with those in other participating countries/economies, establish benchmarks for improvements in the education provided and/or in learning outcomes, and understand the relative strengths and weaknesses of their own education systems.

This publication presents the theory underlying the PISA 2015 assessment – the sixth since the programme's inception. It includes frameworks for assessing the three core subjects – science, reading and mathematics (Chapters 2, 3 and 4, respectively) – and the framework for an assessment of students' financial literacy (Chapter 5). The framework for the assessment of collaborative problem solving – and the results of that assessment – will be published upon completion of a validation study. The chapters outline the knowledge content that students need to acquire in each domain, the processes that students need to be able to perform, and the contexts in which this knowledge and these skills are applied. They also discuss how each domain is assessed. Chapter 6 explains the theory underlying the context questionnaires distributed to students, their parents, school principals and teachers.

## WHAT MAKES PISA UNIQUE

PISA is the most comprehensive and rigorous international programme to assess student performance and to collect data on the student, family and institutional factors that can help to explain differences in performance. Decisions about the scope and nature of the assessments and the background information to be collected are made by leading experts in participating countries, and are steered jointly by governments on the basis of shared, policy-driven interests. Substantial efforts and resources are devoted to achieving cultural and linguistic breadth and balance in the assessment materials. Stringent quality-assurance mechanisms are applied in translation, sampling and data collection. As a consequence, the results of PISA have a high degree of validity and reliability.

PISA's unique features include its:

- **Policy orientation** , which connects data on student learning outcomes with data on students' backgrounds and attitudes towards learning, and on key factors that shape their learning in and outside school, in order to highlight differences in performance patterns and identify the characteristics of schools and education systems that perform well.
- **Innovative concept of “literacy”** , which refers to students’ capacity to apply knowledge and skills in key subjects, and to analyse, reason and communicate effectively as they identify, interpret and solve problems in a variety of situations.
- **Relevance to lifelong learning** , as PISA asks students to report on their motivation to learn, their beliefs about themselves and their learning strategies.
- **Regularity** , which enables countries to monitor their progress in meeting key learning objectives.
- **Breadth of coverage** , which, in PISA 2015, encompasses the 34 OECD countries and 38 partner countries and economies.

### Box 1.1 Key features of PISA 2015

#### The content

The PISA 2015 survey focused on science, with reading, mathematics and collaborative problem solving as minor areas of assessment. PISA 2015 also included an assessment of young people’s financial literacy, which was optional for countries and economies.

PISA assesses not only whether students can reproduce knowledge, but also whether they can extrapolate from what they have learned and apply their knowledge in new situations. It emphasises the mastery of processes, the understanding of concepts, and the ability to function in various types of situations.

#### The students

In PISA 2012, around 510 000 students completed the assessment, representing about 28 million 15-year-olds in the schools of the 65 participating countries and economies. An additional 7 countries participated in 2015.

#### The assessment

Computer-based tests were used, with assessments lasting a total of two hours for each student, in a range of countries and economies.

Test items were a mixture of multiple-choice questions and questions requiring students to construct their own responses. The items were organised in groups based on a passage setting out a real-life situation. About 390 minutes of test items were covered, with different students taking different combinations of test items.

Students also answered a background questionnaire, which took 35 minutes to complete. The questionnaire sought information about the students themselves, their homes, and the school and learning experiences. School principals completed a questionnaire that covered the school system and the learning environment. For additional information, some countries/economies decided to distribute a questionnaire to their teachers. It was the first time that this optional teacher questionnaire was offered to PISA-participating countries/economies. In some countries/economies optional questionnaires were distributed to parents, who were asked to provide information on their perceptions of and involvement in their child’s school, their support for learning in the home, and their child’s career expectations, particularly in science. Countries could choose two other optional questionnaires for students: one asked students about their familiarity with and use of information and communication technologies (ICT); and the second sought information about their education to date, including any interruptions in their schooling, and whether and how they are preparing for a future career.

The relevance of PISA results is confirmed by studies tracking young people in the years following the assessment. Studies in Australia, Canada and Denmark show a strong relationship between 15-year-old students’ performance in reading in the PISA 2000 assessment and the likelihood of a student completing secondary school and continuing with post-secondary studies at age of 19. For example, Canadian students who had attained reading proficiency Level 5 at the age of 15 were 16 times more likely to be enrolled in post-secondary studies when they were 19 years old than those who had not attained Level 1 in reading proficiency.

## THE PISA 2015 TEST

For the first time, PISA 2015 delivers the assessments of all subjects via computer. Paper-based assessment instruments were provided for countries that choose not to test their students by computer, but the paper-based assessment was limited to reading, mathematics and science trends items only. New items were developed only for the computer-based assessment. A field trial was used to study the effect of the change of mode of delivery. Data were collected and analysed to establish equivalence between the computer- and paper-based assessments.

### Box 1.2 2015 mode study

A study similar to the mode study for the OECD Programme for the International Assessment of Adult Competencies (PIAAC) was planned for the PISA 2015 field trial. Students were randomly assigned to either a computer-based or paper-based assessment of reading, mathematical and scientific literacy. Each domain included six clusters of paper-based trend items that were used in previous cycles of PISA. These items were adapted for computer delivery so that countries opting to take the computer-based assessment would be able to link back to previous cycles and would be comparable with countries choosing the paper-based option. Some two-thirds of the items from PISA use objective scoring, such as multiple-choice, true false, and simple open-ended response formats that are easily adapted and reliably scored by computer; the rest are scored by expert coders within each country. These more complex open-ended items were retained and scored in a similar fashion for PISA 2015. Analyses of the PISA field trial were used to determine the comparability between the two modes of presentation across all trend items. Results were presented to and agreed with the PISA Technical Advisory Group, the OECD and all participating countries in 2014.

The 2015 computer-based assessment was designed as a two-hour test. Each test form allocated to students comprised four 30-minute clusters of test material. This test design included six intact clusters from each of the domains of science, reading and mathematics to measure trends. For the major subject of science, an additional six clusters of items were developed to reflect the new features of the 2015 framework. In addition, three clusters of collaborative problem-solving items were developed for the countries that decided to participate in this assessment.

There were 66 different test forms. Students spent one hour on the science assessment (one cluster each of trends and new science items) plus one hour on another subject – reading, mathematics or collaborative problem solving – or 30 minutes on each of the two other subjects. For the countries/economies that chose not to participate in the collaborative problem-solving assessment, 36 test forms were prepared.

Countries that chose paper-based delivery for the main survey measured student performance with 30 paper-and-pencil forms containing trends items from two of the three core PISA domains.

Each test form was completed by a sufficient number of students for appropriate estimates to be made of the achievement levels on all items by students in each country and in relevant subgroups within a country (such as boys and girls, and students from different social and economic contexts).

The assessment of financial literacy is offered as an option in PISA 2015 based on the same framework as the one developed for PISA 2012. The financial literacy assessment was developed as a one-hour exercise, comprising two clusters distributed to a subsample of students in combination with the science, mathematics and reading assessments.

## AN OVERVIEW OF WHAT IS ASSESSED IN EACH DOMAIN

Box 1.3 presents definitions of the three domains assessed in PISA 2015. The definitions all emphasise functional knowledge and skills that allow one to participate fully in society. Such participation requires more than just being able to carry out tasks imposed externally by, for example, an employer; it also means being able to participate in decision making. The more complex tasks in PISA require students to reflect on and evaluate material, not just to answer questions that have one correct answer.

## Box 1.3 Definitions of the domains

### Scientific literacy

The ability to engage with science-related issues, and with the ideas of science, as a reflective citizen. A scientifically literate person is willing to engage in reasoned discourse about science and technology, which requires the competencies to:

- Explain phenomena scientifically – recognise, offer and evaluate explanations for a range of natural and technological phenomena.
- Evaluate and design scientific enquiry – describe and appraise scientific investigations and propose ways of addressing questions scientifically.
- Interpret data and evidence scientifically – analyse and evaluate data, claims and arguments in a variety of representations and draw appropriate scientific conclusions.

### Reading literacy

An individual’s capacity to understand, use, reflect on and engage with written texts, in order to achieve one's goals, to develop one’s knowledge and potential, and to participate in society.

### Mathematical literacy

An individual’s capacity to formulate, employ and interpret mathematics in a variety of contexts. It includes reasoning mathematically and using mathematical concepts, procedures, facts and tools to describe, explain and predict phenomena. It assists individuals to recognise the role that mathematics plays in the world and to make the well-founded judgments and decisions needed by constructive, engaged and reflective citizens.

## Scientific literacy (Chapter 2)

is defined as the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen. A scientifically literate person is willing to engage in reasoned discourse about science and technology, which requires the competencies to explain phenomena scientifically, evaluate and design scientific enquiry, and interpret data and evidence scientifically.

PISA assesses students’ performance in science through questions related to:

**Contexts:** Personal, local/national and global issues, both current and historical, which demand some understanding of science and technology. The contexts in PISA 2015 were changed from “personal, social and global” in the 2006 assessment to “personal, local/national and global” to make the headings more coherent.

**Knowledge:** An understanding of the major facts, concepts and explanatory theories that form the basis of scientific knowledge. Such knowledge includes knowledge of both the natural world and technological artefacts (content knowledge), knowledge of how such ideas are produced (procedural knowledge), and an understanding of the underlying rationale for these procedures and the justification for their use (epistemic knowledge). The major difference from PISA 2006 is that the notion of “knowledge about science” has been specified more clearly and split into two components: procedural knowledge and epistemic knowledge.

**Competencies:** The ability to explain phenomena scientifically, evaluate and design scientific enquiry, and interpret data and evidence scientifically.

**Attitudes:** A set of attitudes towards science indicated by an interest in science and technology, valuing scientific approaches to enquiry, where appropriate, and a perception and awareness of environmental issues. The second, “support for scientific enquiry” in the previous cycles, was changed to a measure of “valuing scientific approaches to enquiry”, which is essentially a change in terminology to better reflect what is measured.

## Reading literacy (Chapter 3)

is defined as students’ ability to understand, use and reflect on written text to achieve their purposes.

PISA assesses students’ performance in reading through questions related to:

**Text format:** PISA uses continuous texts or prose organised in sentences and paragraphs, and non-continuous texts that present information in other ways, such as in lists, forms, graphs or diagrams. The test uses a range of prose forms, such as narration, exposition and argumentation.

## WHAT IS PISA?

### Processes (aspects)

Students are not assessed on the most basic reading skills, as it is assumed that most 15-year-old students will have acquired these. Rather, students are expected to demonstrate their proficiency in accessing and retrieving information, forming a broad general understanding of the text, interpreting it, reflecting on its content, and reflecting on its form and features.

### Situations

These are defined by the use for which the text was constructed. For example, a novel, personal letter or biography is written for people's personal use; official documents or announcements for public use; a manual or report for occupational use; and a textbook or worksheet for educational use. Since some groups may perform better in one reading situation than in another, a range of types of reading is included in the test.

### Mathematical literacy

(Chapter 4) is defined as students' ability to analyse, reason and communicate ideas effectively as they pose, formulate, solve and interpret solutions to mathematical problems in a variety of situations.

PISA assesses students' performance in mathematics through questions related to:

#### Processes

These are defined in terms of three categories: formulating situations mathematically; employing mathematical concepts, facts, procedures and reasoning; and interpreting, applying and evaluating mathematical outcomes (herein referred to as “formulate”, “employ” and “interpret”). They describe what students do to connect the context of a problem with the mathematics involved and thus solve the problem. These three processes each draw on seven fundamental mathematical capabilities: communicating; mathematising; representing; reasoning and argument; devising strategies for solving problems; using symbolic, formal and technical language and operations; and using mathematical tools. All of these capabilities are based on the problem solver's detailed mathematical knowledge about individual topics.

#### Content

These are four ideas (quantity, space and shape, change and relationships, and uncertainty and data) that are related to familiar curricular subjects, such as numbers, algebra and geometry, in overlapping and complex ways.

#### Contexts

These are the settings in a student's world in which the problems are placed. The framework identifies four contexts: personal, educational, societal and scientific.

## THE EVOLUTION OF REPORTING STUDENT PERFORMANCE IN PISA

Results from PISA are reported using scales. Initially, the OECD average score for all three subjects was 500 with a standard deviation of 100, which meant that two-thirds of students across OECD countries scored between 400 and 600 points. These scores represent degrees of proficiency in a particular domain. In subsequent cycles of PISA, the OECD average score has fluctuated slightly around the original.

Reading literacy was the major domain in 2000, and the reading scales were divided into five levels of knowledge and skills. The main advantage of this approach is that it is useful for describing what substantial numbers of students can do with tasks at different levels of difficulty. Results were also presented through three “aspect” subscales of reading: accessing and retrieving information; integrating and interpreting texts; and reflecting and evaluating texts. A proficiency scale was also available for mathematics and science, though without described levels.

PISA 2003 built upon this approach by specifying six proficiency levels for the mathematics scale. There were four “content” subscales in mathematics: space and shape, change and relationships, quantity, and uncertainty.

Similarly, the reporting of science in PISA 2006 specified six proficiency levels. The three “competency” subscales in science related to identifying scientific issues, explaining phenomena scientifically and using scientific evidence. Country performance was compared on the bases of knowledge about science and knowledge of science. The three main areas of knowledge of science were physical systems, living systems, and earth and space systems.

PISA 2009 marked the first time that reading literacy was re-assessed as a major domain. Trend results were reported for all three domains. PISA 2009 added a Level 6 to the reading scale to describe very high levels of reading proficiency. The bottom level of proficiency, Level 1, was relabelled as Level 1a. Another level, Level 1b, was introduced to describe the performance of students who would previously have been rated as “below Level 1”, but who show proficiency in relation to new items that are easier than those included in previous PISA assessments. These changes allow countries to know more about what kinds of tasks students with very high and very low reading proficiency are capable of completing.

Mathematics was re-assessed as a major domain in PISA 2012. In addition to the “content” subscales (with the “uncertainty” scale re-named as “uncertainty and data” for improved clarity), three new subscales were developed to assess the three processes in which students, as active problem solvers, engage. These three “process” subscales are: formulating situations mathematically; employing mathematical concepts, facts, procedures and reasoning; and interpreting, applying and evaluating mathematical outcomes (known as “formulating”, “employing” and “interpreting”).

Science, which was the main subject of assessment in PISA 2006, is again the main domain in PISA 2015. The assessment measures students’ ability to: explain phenomena scientifically; evaluate and design scientific enquiry; and interpret data and evidence scientifically. The science scale has also been extended by the addition of a Level “1b” to better describe the proficiency of students at the lowest level of ability who demonstrate minimal scientific literacy and who would previously not have been included in the reporting scales.

## THE CONTEXT QUESTIONNAIRES

To gather contextual information, PISA asks students and the principals of their schools to respond to questionnaires. These take about 35 and 45 minutes, respectively, to complete. The responses to the questionnaires are analysed with the assessment results to provide at once a broader and more nuanced picture of student, school and system performance. Chapter 6 presents the questionnaire framework in detail. The questionnaires from all assessments since PISA’s inception are available on the PISA website: www.pisa.oecd.org.

The questionnaires seek information about:

- Students and their family backgrounds, including their economic, social and cultural capital.
- Aspects of students’ lives, such as their attitudes towards learning, their habits and life in and outside of school, and their family environment.
- Aspects of schools, such as the quality of the schools’ human and material resources, public and private management and funding, decision-making processes, staffing practices and the school’s curricular emphasis and extracurricular activities offered.
- Context of instruction, including institutional structures and types, class size, classroom and school climate, and reading activities in class.
- Aspects of learning, including students’ interest, motivation and engagement.

Four additional questionnaires are offered as options:

- A computer familiarity questionnaire, focusing on the availability and use of information and communications technology (ICT) and on students’ ability to carry out computer tasks and their attitudes towards computer use.
- An educational career questionnaire, which collects additional information on interruptions in schooling, on preparation for students’ future career, and on support with language learning.
- A parent questionnaire, focusing on parents’ perceptions of and involvement in their child’s school, their support for learning at home, school choice, their child’s career expectations, and their background (immigrant/non-immigrant).
- A teacher questionnaire, which is new to PISA, will help illustrate the similarities and differences between groups of teachers in order to better establish the context for students’ test results. The level of analysis of data gathered from the optional teacher questionnaire is the school level. Science teachers are asked to describe their teaching practices through a parallel questionnaire that also focuses on teacher-directed teaching and learning activities in science lessons, and a selected set of inquiry-based activities. The teacher questionnaire asks about the content of a school’s science curriculum and how it is communicated to parents too. The new optional teacher questionnaire gathers information on transformational leadership as well.

The contextual information collected through the student, school and optional questionnaires comprises only a part of the information available to PISA. Indicators describing the general structure of the education systems (their demographic and economic contexts – for example, costs, enrolments, school and teacher characteristics, and some classroom processes) and their effect on labour market outcomes are routinely developed and applied by the OECD (e.g. in the annual OECD publication, Education at a Glance).

## A COLLABORATIVE PROJECT

PISA is the result of a collaborative effort among OECD and partner governments. The assessments are developed co-operatively, agreed by participating countries/economies, and implemented by national organisations. The co-operation of students, teachers and principals in participating schools has been crucial to the success of PISA during all stages of development and implementation.

The PISA Governing Board (PGB), representing all countries/economies at senior policy levels, determines the policy priorities for PISA in the context of OECD objectives and oversees adherence to these priorities during the implementation of the programme. The PGB sets priorities for developing indicators, for establishing assessment instruments and for reporting results. Experts from participating countries/economies also serve on working groups tasked with linking PISA policy objectives with the best available technical expertise in the different assessment domains. By participating in these expert groups, countries/economies ensure that the instruments are internationally valid and take into account differences in the cultures and education systems.

Participating countries/economies implement PISA at the national level, through National Centres managed by National Project Managers, subject to the agreed administration procedures. National Project Managers play a vital role in ensuring that implementation is of high quality. They also verify and evaluate the survey results, analyses, reports and publications.

The development of the science and collaborative problem-solving frameworks and the adaptation of the frameworks for reading and mathematics are the responsibility of Pearson, while the design and development of the questionnaires are the responsibility of the Deutsches Institut für Pädagogische Forschung (DIPF). Management and oversight of this survey, the development of the instruments, scaling, and analysis are the responsibility of the Educational Testing Service (ETS) as is the development of the electronic platform. Other partners or subcontractors involved with ETS include cApStAn Linguistic Quality Control and the Department of Experimental and Theoretical Pedagogy at the University of Liège (SPe) in Belgium; the Center for Educational Technology (CET) in Israel; the Public Research Centre (CRP) Henri Tudor and the Educational Measurement and Research Center (EMACS) of the University of Luxembourg in Luxembourg; and GESIS – Leibniz-Institute for the Social Sciences in Germany. Westat assumed responsibility for survey operations and sampling with the subcontractor, the Australian Council for Educational Research (ACER).

The OECD Secretariat has overall managerial responsibility for the programme, monitors its implementation on a day-to-day basis, acts as the secretariat for the PGB, builds consensus among countries, and serves as the interlocutor between the PGB and the contractors charged with implementation. The OECD Secretariat is also responsible for the production of the indicators, and the analysis and preparation of the international reports and publications in co-operation with the contractors and in close consultation with member countries both at the policy level (PGB) and at the implementation level (National Project Managers).

## PISA 2015

### Science framework

Science is the main subject of assessment in the Programme for International Student Assessment (PISA) in 2015. This chapter defines "scientific literacy" as assessed in PISA. It describes the types of contexts, knowledge, competencies and attitudes towards science that are reflected in the assessment's science problems and provides several sample items. The chapter also discusses how student performance in science is measured and reported.

This document provides a description of and rationale for the framework that forms the basis of the instrument to assess scientific literacy – the major domain in PISA 2015. Previous PISA frameworks for the science assessment (OECD, 1999, 2004, 2006) have elaborated a conception of scientific literacy as the central construct for science assessment. These documents have established a broad consensus among science educators of the concept of scientific literacy. This framework for PISA 2015 refines and extends the previous construct, in particular by drawing on the PISA 2006 framework that was used as the basis for assessment in 2006, 2009 and 2012.

Scientific literacy matters at both the national and international levels as humanity faces major challenges in providing sufficient water and food, controlling diseases, generating sufficient energy and adapting to climate change (UNEP, 2012). Many of these issues arise, however, at the local level where individuals may be faced with decisions about practices that affect their own health and food supplies, the appropriate use of materials and new technologies, and decisions about energy use. Dealing with all of these challenges will require a major contribution from science and technology. Yet, as argued by the European Commission, the solutions to political and ethical dilemmas involving science and technology “cannot be the subject of informed debate unless young people possess certain scientific awareness” (European Commission, 1995: 28). Moreover, “this does not mean turning everyone into a scientific expert, but enabling them to fulfil an enlightened role in making choices which affect their environment and to understand in broad terms the social implications of debates between experts” (ibid.: 28). Given that knowledge of science and science-based technology contributes significantly to individuals’ personal, social, and professional lives, an understanding of science and technology is thus central to a young person’s “preparedness for life”.

The concept of scientific literacy in this framework refers to a knowledge of both science and science-based technology, even though science and technology do differ in their purposes, processes and products. Technology seeks the optimal solution to a human problem, and there may be more than one optimal solution. In contrast, science seeks the answer to a specific question about the natural, material world. Nevertheless, the two are closely related. For instance, new scientific knowledge leads to the development of new technologies (think of the advances in material science that led to the development of the transistor in 1948). Likewise, new technologies can lead to new scientific knowledge (think of how knowledge of the universe has been transformed through the development of better telescopes). Individuals make decisions and choices that influence the directions of new technologies (consider the decision to drive a smaller, more fuel-efficient car). Scientifically literate individuals should therefore be able to make more informed choices. They should also be able to recognise that, while science and technology are often a source of solutions, paradoxically, they can also be seen as a source of risk, generating new problems that can only be solved through the use of science and technology. Therefore, individuals need to be able to weigh the potential benefits and risks of applying scientific knowledge to themselves and society.

Scientific literacy also requires not just knowledge of the concepts and theories of science but also knowledge of the common procedures and practices associated with scientific enquiry and how these enable science to advance. Therefore, individuals who are scientifically literate have a knowledge of the major concepts and ideas that form the foundation of scientific and technological thought; how such knowledge has been derived; and the degree to which such knowledge is proved by evidence or theoretical explanations.

Undoubtedly, many of the challenges of the 21st century will require innovative solutions that have a basis in scientific thinking and scientific discovery. Societies will require a cadre of well-educated scientists to undertake the research and nurture the innovation that will be essential to meet the economic, social and environmental challenges that the world faces.

For all of these reasons, scientific literacy is perceived to be a key competency (Rychen and Salganik, 2003) and defined in terms of the ability to use knowledge and information interactively – that is “an understanding of how it [a knowledge of science] changes the way one can interact with the world and how it can be used to accomplish broader goals” (ibid.: 10). As such, it represents a major goal for science education for all students. Therefore, the view of scientific literacy that forms the basis for the 2015 international assessment of 15-year-old students is a response to the question: What is important for young people to know, value and be able to do in situations involving science and technology?

## DEFINING SCIENTIFIC LITERACY

Current thinking about the desired outcomes of science education is rooted strongly in a belief that an understanding of science is so important that it should be a feature of every young person’s education (American Association for the Advancement of Science, 1989; Confederacion de Sociedades Cientificas de España, 2011; Fensham, 1985; Millar and Osborne, 1998; National Research Council, 2012; Sekretariat der Ständigen Konferenz der Kultusminister der Länder in der Bundesrepublik Deutschland [KMK], 2005; Taiwan Ministry of Education, 1999). Indeed, in many countries science is an obligatory element of the school curriculum from kindergarten until the completion of compulsory education.

Many of the documents and policy statements cited above give pre-eminence to an education for citizenship. However, many of the curricula for school science across the world are based on a view that the primary goal of science education should be the preparation of the next generation of scientists (Millar and Osborne, 1998). These two goals are not always compatible. Attempts to resolve the tension between the needs of the majority of students who will not become scientists and the needs of the minority who will have led to an emphasis on teaching science through enquiry (National Academy of Science, 1995; National Research Council, 2000), and new curriculum models (Millar, 2006) that address the needs of both groups. The emphasis in these frameworks and their associated curricula lies not on producing individuals who will be “producers” of scientific knowledge, i.e. the future scientists; rather, it is on educating all young people to become informed, critical users of scientific knowledge.

To understand and engage in critical discussions about issues that involve science and technology requires three domain-specific competencies. The first is the ability to provide explanatory accounts of natural phenomena, technical artefacts and technologies, and their implications for society. Such an ability requires a knowledge of the fundamental ideas of science and the questions that frame the practice and goals of science. The second is the knowledge and understanding of scientific enquiry to: identify questions that can be answered by scientific enquiry; identify whether appropriate procedures have been used; and propose ways in which such questions might be answered. The third is the competency to interpret and evaluate data and evidence scientifically and evaluate whether the conclusions are justified. Thus, scientific literacy in PISA 2015 is defined by the three competencies to:

- explain phenomena scientifically
- evaluate and design scientific enquiry
- interpret data and evidence scientifically

All of these competencies require knowledge. Explaining scientific and technological phenomena, for instance, demands a knowledge of the content of science (hereafter, content knowledge). The second and third competencies, however, require more than a knowledge of what is known; they depend on an understanding of how scientific knowledge is established and the degree of confidence with which it is held. Some have argued for teaching what has variously been called “the nature of science” (Lederman, 2006), “ideas about science” (Millar and Osborne, 1998) or “scientific practices” (National Research Council, 2012). Recognising and identifying the features that characterise scientific enquiry requires a knowledge of the standard procedures that underlie the diverse methods and practices used to establish scientific knowledge (hereafter, procedural knowledge). Finally, the competencies require epistemic knowledge – an understanding of the rationale for the common practices of scientific enquiry, the status of the knowledge claims that are generated, and the meaning of foundational terms, such as theory, hypothesis and data.

### Box 2.1 Scientific knowledge: PISA 2015 terminology

This document is based upon a view of scientific knowledge as consisting of three distinguishable but related elements. The first of these and the most familiar is a knowledge of the facts, concepts, ideas and theories about the natural world that science has established. For instance, how plants synthesise complex molecules using light and carbon dioxide or the particulate nature of matter. This kind of knowledge is referred to as “content knowledge” or “knowledge of the content of science”.

Knowledge of the procedures that scientists use to establish scientific knowledge is referred to as “procedural knowledge”. This is a knowledge of the practices and concepts on which empirical enquiry is based such as repeating measurements to minimise error and reduce uncertainty, the control of variables, and standard procedures for representing and communicating data (Millar, Lubben, Gott and Duggan, 1995). More recently these have been elaborated as a set of “concepts of evidence” (Gott, Duggan and Roberts, 2008).

Furthermore, understanding science as a practice also requires “epistemic knowledge” which refers to an understanding of the role of specific constructs and defining features essential to the process of knowledge-building in science (Duschl, 2007). Epistemic knowledge includes an understanding of the function that questions, observations, theories, hypotheses, models and arguments play in science; a recognition of the variety of forms of scientific enquiry; and the role peer review plays in establishing knowledge that can be trusted.

A more detailed discussion of these three forms of knowledge is provided in the later section on scientific knowledge and in Figures 2.5, 2.6 and 2.7.

Both procedural and epistemic knowledge are necessary to identify questions that are amenable to scientific enquiry, to judge whether appropriate procedures have been used to ensure that the claims are justified, and to distinguish scientific issues from matters of values or economic considerations. This definition of scientific literacy assumes that, throughout their lives, individuals will need to acquire knowledge, not through scientific investigations, but through the use of resources such as libraries and the Internet. Procedural and epistemic knowledge are essential to decide whether the many claims of knowledge and understanding that pervade contemporary media are based on the use of appropriate procedures and are justified.

People need all three forms of scientific knowledge to perform the three competencies of scientific literacy. PISA 2015 focuses on assessing the extent to which 15-year-olds are capable of displaying the three aforementioned competencies appropriately within in a range of personal, local/national (grouped in one category) and global contexts. (For the purposes of the PISA assessment, these competencies are only tested using the knowledge that 15-year-old students can reasonably be expected to have already acquired.) This perspective differs from that of many school science programmes that are dominated by content knowledge. Instead, the framework is based on a broader view of the kind of knowledge of science required of fully engaged citizens.

In addition, the competency-based perspective also recognises that there is an affective element to a student's display of these competencies: students' attitudes or disposition towards science will determine their level of interest, sustain their engagement, and may motivate them to take action (Schibeci, 1984). Thus, the scientifically literate person would typically have an interest in scientific topics; engage with science-related issues; have a concern for issues of technology, resources and the environment; and reflect on the importance of science from a personal and social perspective. This requirement does not mean that such individuals are necessarily disposed towards becoming scientists themselves, rather such individuals recognise that science, technology and research in this domain are an essential element of contemporary culture that frames much of our thinking.

These considerations led to the definition of scientific literacy used in PISA 2015 (see Box 2.2). The use of the term “scientific literacy”, rather than “science”, underscores the importance that the PISA science assessment places on the application of scientific knowledge in the context of real-life situations.

### Box 2.2 The 2015 definition of scientific literacy

Scientific literacy is the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen.

A scientifically literate person is willing to engage in reasoned discourse about science and technology, which requires the competencies to:

- Explain phenomena scientifically – recognise, offer and evaluate explanations for a range of natural and technological phenomena.
- Evaluate and design scientific enquiry – describe and appraise scientific investigations and propose ways of addressing questions scientifically.
- Interpret data and evidence scientifically – analyse and evaluate data, claims and arguments in a variety of representations and draw appropriate scientific conclusions.

### The competencies required for scientific literacy

#### Competency 1: Explain phenomena scientifically

The cultural achievement of science has been to develop a set of explanatory theories that have transformed our understanding of the natural world (in this document, “natural world” refers to phenomena associated with any object or activity occurring in the living or the material world), such as the idea that day and night is caused by a rotating Earth, or the idea that diseases can be caused by invisible micro-organisms. Moreover, such knowledge has enabled us to develop technologies that support human life by, for example, preventing disease or enabling rapid human communication across the globe. The competency to explain scientific and technological phenomena is thus dependent on a knowledge of these major explanatory ideas of science.

Explaining scientific phenomena, however, requires more than the ability to recall and use theories, explanatory ideas, information and facts (content knowledge). Offering scientific explanations also requires an understanding of how such knowledge has been derived and the level of confidence we might hold about any scientific claims. For this competency, the individual requires a knowledge of the standard forms and procedures used in scientific enquiry to obtain such knowledge (procedural knowledge) and an understanding of their role and function in justifying the knowledge produced by science (epistemic knowledge).

## Competency 2: Evaluate and design scientific enquiry

Scientific literacy implies that students have some understanding of the goal of scientific enquiry, which is to generate reliable knowledge about the natural world (Ziman, 1979). Data collected and obtained by observation and experiment, either in the laboratory or in the field, lead to the development of models and explanatory hypotheses that enable predictions that can then be tested experimentally. New ideas, however, commonly build on previous knowledge. Scientists themselves rarely work in isolation; they are members of research groups or teams that engage, nationally and internationally, in extensive collaboration with colleagues. New knowledge claims are always perceived to be provisional and may lack justification when subjected to critical peer review – the mechanism through which the scientific community ensures the objectivity of scientific knowledge (Longino, 1990). Hence, scientists have a commitment to publish or report their findings and the methods used in obtaining their evidence. Doing so enables empirical studies, at least in principle, to be replicated and results confirmed or challenged. However, measurements can never be absolutely precise; they all contain a degree of error. Much of the work of the experimental scientist is thus devoted to resolving uncertainty by repeating measurements, collecting larger samples, building instruments that are more accurate and using statistical techniques that assess the degree of confidence in any result.

In addition, science has well-established procedures that are the foundations of any experiment to establish cause and effect. The use of controls enables the scientist to claim that any change in a perceived outcome can be attributed to a change in one specific feature. Failure to use such techniques leads to results where effects are confounded and cannot be trusted. Likewise, double-blind trials enable scientists to claim that the results have not been influenced either by the subjects of the experiment, or by the experimenter themselves. Other scientists, such as taxonomists and ecologists, are engaged in the process of identifying underlying patterns and interactions in the natural world that warrant a search for an explanation. In other cases, such as evolution, plate tectonics or climate change, scientists examine a range of hypotheses and eliminate those that do not fit with the evidence.

Facility with this competency draws on content knowledge, a knowledge of the common procedures used in science (procedural knowledge), and the function of these procedures in justifying any claims advanced by science (epistemic knowledge). Procedural and epistemic knowledge serve two functions. First, such knowledge is required by individuals to appraise scientific investigations and decide whether they have followed appropriate procedures and whether the conclusions are justified. Second, individuals who have this knowledge should be able to propose, at least in broad terms, how a scientific question might be investigated appropriately.

## Competency 3: Interpret data and evidence scientifically

Interpreting data is such a core activity of all scientists that some rudimentary understanding of the process is essential for scientific literacy. Initially, data interpretation begins with looking for patterns, constructing simple tables and graphical visualisations, such as pie charts, bar graphs, scatterplots or Venn diagrams. At a higher level, it requires the use of more complex data sets and the use of the analytical tools offered by spreadsheets and statistical packages. It would be wrong, however, to look at this competency as merely an ability to use these tools. A substantial body of knowledge is required to recognise what constitutes reliable and valid evidence and how to present data appropriately.

Scientists make choices about how to represent the data in graphs, charts or, increasingly, in complex simulations or 3D visualisations. Any relationships or patterns must then be read using a knowledge of standard patterns. Whether uncertainty has been minimised by standard statistical techniques must also be considered. All of this draws on a body of procedural knowledge. The scientifically literate individual can also be expected to understand that uncertainty is an inherent feature of all measurement, and that one criterion for expressing confidence in a finding is determining the probability that the finding might have occurred by chance.

It is not sufficient, however, to understand the procedures that have been applied to obtain any data set. The scientifically literate individual needs to be able to judge whether they are appropriate and the ensuing claims are justified (epistemic knowledge). For instance, many sets of data can be interpreted in multiple ways. Argumentation and critique are essential to determining which is the most appropriate conclusion.

Whether it is new theories, novel ways of collecting data or fresh interpretations of old data, argumentation is the means that scientists and technologists use to make their case for new ideas. Disagreement among scientists is normal, not extraordinary. Determining which interpretation is the best requires a knowledge of science (content knowledge). Consensus on key scientific ideas and concepts has been achieved through this process of critique and argumentation (Longino, 1990). Indeed, it is a critical and sceptical disposition towards all empirical evidence that many would see as the hallmark of the professional scientist. The scientifically literate individual understands the function and purpose of argument and critique and why they are essential to the construction of knowledge in science. In addition, they should be able both to construct claims that are justified by data and to identify any flaws in the arguments of others.

## The evolution of the definition of scientific literacy in PISA

In PISA 2000 and 2003, scientific literacy was defined as:

"...the capacity to use scientific knowledge, to identify questions and to draw evidence-based conclusions in order to understand and help make decisions about the natural world and the changes made to it through human activity." (OECD, 2000, 2004)

In 2000 and 2003, the definition embedded knowledge of science and understandings about science within the one term “scientific knowledge”. The 2006 definition separated and elaborated the term “scientific knowledge” by dividing it into two components: “knowledge of science” and “knowledge about science” (OECD, 2006). Both definitions referred to the application of scientific knowledge to understanding and making informed decisions about the natural world. In PISA 2006, the definition was enhanced by the addition of knowledge of the relationship between science and technology – an aspect that was assumed but not elaborated in the 2003 definition.

"For the purposes of PISA, scientific literacy refers to an individual's:

- Scientific knowledge and use of that knowledge to identify questions, acquire new knowledge, explain scientific phenomena and draw evidence-based conclusions about science-related issues.
- Understanding of the characteristic features of science as a form of human knowledge and enquiry.
- Awareness of how science and technology shape our material, intellectual and cultural environments.
- Willingness to engage in science-related issues, and with the ideas of science, as a reflective citizen." (OECD, 2006).

These ideas have evolved further in the PISA 2015 definition of scientific literacy. The major difference is that the notion of “knowledge about science” has been specified more clearly and split into two components – procedural knowledge and epistemic knowledge.

In 2006, the PISA framework was also expanded to include attitudinal aspects of students’ responses to scientific and technological issues within the construct of scientific literacy. In 2006, attitudes were measured in two ways: through the student questionnaire and through items embedded in the student test. Discrepancies were found between the results from the embedded questions and those from the background questionnaire with respect to “interest in science” for all students and gender differences in these issues (OECD, 2009; see also Drechsel, Carstensen and Prenzel, 2011). More important, embedded items extended the length of the test. Hence, in PISA 2015, attitudinal aspects are only measured through the student questionnaire; there are no embedded attitudinal items.

As for the constructs measured within this domain, the first (“interest in science”) and third (“environmental awareness”) remain the same as in 2006. The second (“support for scientific enquiry”) has been changed to a measure of “valuing scientific approaches to enquiry”, which is essentially a change in terminology to better reflect what is measured.

In addition, the contexts in PISA 2015 have been changed from “personal, social and global” in the 2006 assessment to “personal, local/national and global” to make the headings more coherent.

## Organising the Domain of Science

The PISA 2015 definition of scientific literacy consists of four interrelated aspects (see Figures 2.1 and 2.2).

### Figure 2.1 - Aspects of the Scientific Literacy Assessment Framework for PISA 2015

| Contexts     | Personal, local/national and global issues, both current and historical, which demand some understanding of science and technology.                                                                                                                                                                                                                                                                                                |
|--------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Knowledge    | An understanding of the major facts, concepts and explanatory theories that form the basis of scientific knowledge. Such knowledge includes knowledge of both the natural world and technological artefacts (content knowledge), knowledge of how such ideas are produced (procedural knowledge), and an understanding of the underlying rationale for these procedures and the justification for their use (epistemic knowledge). |
| Competencies | The ability to explain phenomena scientifically, evaluate and design scientific enquiry, and interpret data and evidence scientifically.                                                                                                                                                                                                                                                                                           |
| Attitudes    | A set of attitudes towards science indicated by an interest in science and technology, valuing scientific approaches to enquiry where appropriate, and a perception and awareness of environmental issues.                                                                                                                                                                                                                         |

### Figure 2.2 - Inter-relations between the Four Aspects

<!-- image -->

Inter-relations between the four aspects

#### Contexts of Assessment Items

PISA 2015 assesses scientific knowledge in contexts that are relevant to the science curricula of participating countries. Such contexts are not, however, restricted to the common aspects of participants' national curricula. Rather, the assessment requires evidence of the successful use of the three competencies required for scientific literacy in situations set in personal, local/national and global contexts.

Assessment items are not limited to school science contexts. In the PISA 2015 scientific literacy assessment, the items focus on situations relating to the self, family and peer groups (personal), to the community (local and national), and to life across the world (global). Technology-based topics may be used as a common context. Some topics may be set in historical contexts, which are used to assess students' understanding of the processes and practices involved in advancing scientific knowledge.

Figure 2.3 shows how science and technology issues are applied within personal, local/national and global settings. The contexts are chosen in light of their relevance to students' interests and lives. The areas of application are: health and disease, natural resources, environmental quality, hazards, and the frontiers of science and technology. They are the areas in which scientific literacy has particular value for individuals and communities in enhancing and sustaining quality of life, and in developing public policy.

The PISA science assessment is *not* an assessment of contexts. Rather, it assesses competencies and knowledge *in specific contexts* . These contexts are chosen on the basis of the knowledge and understanding that students are likely to have acquired by the age of 15.

Sensitivity to linguistic and cultural differences is a priority in item development and selection, not only for the sake of the validity of the assessment, but also to respect these differences among participating countries.

## Figure 2.3 - Contexts in the PISA 2015 scientific literacy assessment

|                                     | Personal                                                                          | Local/National                                                                                                                        | Global                                                                                             |
|-------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| Health and disease                  | Maintenance of health, accidents, nutrition                                       | Control of disease, social transmission, food choices, community health                                                               | Epidemics, spread of infectious diseases                                                           |
| Natural resources                   | Personal consumption of materials and energy                                      | Maintenance of human populations, quality of life, security, production and distribution of food, energy supply                       | Renewable and non-renewable natural systems, population growth, sustainable use of species         |
| Environmental quality               | Environmentally friendly actions, use and disposal of materials and devices       | Population distribution, disposal of waste, environmental impact                                                                      | Biodiversity, ecological sustainability, control of pollution, production and loss of soil/biomass |
| Hazards                             | Risk assessments of lifestyle choices                                             | Rapid changes (e.g. earthquakes, severe weather), slow and progressive changes (e.g. coastal erosion, sedimentation), risk assessment | Climate change, impact of modern communication                                                     |
| Frontiers of science and technology | Scientific aspects of hobbies, personal technology, music and sporting activities | New materials, devices and processes, genetic modifications, health technology, transport                                             | Extinction of species, exploration of space, origin and structure of the universe                  |

## Scientific competencies

Figures 2.4a, 2.4b and 2.4c provide a detailed description of how students may display the three competencies required for scientific literacy. The set of scientific competencies in Figures 2.4a, 2.4b and 2.4c reflects a view that science is best seen as an ensemble of social and epistemic practices that are common across all sciences (National Research Council, 2012). Hence, all these competencies are framed as actions. They are written in this manner to convey the idea of what the scientifically literate person both understands and is capable of doing. Fluency with these practices is, in part, what distinguishes the expert scientist from the novice. While it would be unreasonable to expect a 15-year-old student to have the expertise of a scientist, a scientifically literate student can be expected to appreciate the role and significance of these practices and try to use them.

## Figure 2.4a - PISA 2015 scientific competencies: Explain phenomena scientifically

### Explain phenomena scientifically

Recognise, offer and evaluate explanations for a range of natural and technological phenomena demonstrating the ability to:

- Recall and apply appropriate scientific knowledge.
- Identify, use and generate explanatory models and representations.
- Make and justify appropriate predictions.
- Offer explanatory hypotheses.
- Explain the potential implications of scientific knowledge for society.

Demonstrating the competency of explaining phenomena scientifically requires students to recall the appropriate content knowledge in a given situation and use it to interpret and explain the phenomenon of interest. Such knowledge can also be used to generate tentative explanatory hypotheses in contexts where there is a lack of knowledge or data. A scientifically literate person is expected to be able to draw on standard scientific models to construct simple representations to explain everyday phenomena, such as why antibiotics do not kill viruses, how a microwave oven works, or why gases are compressible but liquids are not, and use these to make predictions. This competency includes the ability to describe or interpret phenomena and predict possible changes. In addition, it may involve recognising or identifying appropriate descriptions, explanations and predictions.

### Figure 2.4b - PISA 2015 scientific competencies: Evaluate and design scientific enquiry

#### Evaluate and design scientific enquiry

Describe and appraise scientific investigations and propose ways of addressing questions scientifically demonstrating the ability to:

- Identify the question explored in a given scientific study.
- Distinguish questions that could be investigated scientifically.
- Propose a way of exploring a given question scientifically.
- Evaluate ways of exploring a given question scientifically.
- Describe and evaluate how scientists ensure the reliability of data, and the objectivity and generalisability of explanations.

The competency of evaluating and designing scientific enquiry is required to evaluate reports of scientific findings and investigations critically. It relies on the ability to distinguish scientific questions from other forms of enquiry or recognise questions that could be investigated scientifically in a given context. This competency requires a knowledge of the key features of a scientific investigation – for example, what things should be measured, what variables should be changed or controlled, or what action should be taken so that accurate and precise data can be collected. It requires an ability to evaluate the quality of data, which, in turn, depends on recognising that data are not always completely accurate. It also requires the ability to determine whether an investigation is driven by an underlying theoretical premise or, alternatively, whether it seeks to determine patterns.

A scientifically literate person should also be able to recognise the significance of previous research when judging the value of any given scientific enquiry. Such knowledge is needed to situate the work and judge the importance of any possible outcomes. For example, knowing that the search for a malaria vaccine has been an ongoing programme of scientific research for several decades, and given the number of people who are killed by malarial infections, any findings that suggested a vaccine would be achievable would be of substantial significance.

Moreover, students need to understand the importance of developing a sceptical attitude towards all media reports in science. They need to recognise that all research builds on previous work, that the findings of any one study are always subject to uncertainty, and that the study may be biased by the sources of funding. This competency requires students to possess both procedural and epistemic knowledge but may also draw on their content knowledge of science, to varying degrees.

### Figure 2.4c - PISA 2015 scientific competencies: Interpret data and evidence scientifically

#### Interpret data and evidence scientifically

Analyse and evaluate scientific data, claims and arguments in a variety of representations and draw appropriate conclusions, demonstrating the ability to:

- Transform data from one representation to another.
- Analyse and interpret data and draw appropriate conclusions.
- Identify the assumptions, evidence and reasoning in science-related texts.
- Distinguish between arguments that are based on scientific evidence and theory and those based on other considerations.
- Evaluate scientific arguments and evidence from different sources (e.g. newspapers, the Internet, journals).

A scientifically literate person should be able to interpret and make sense of basic forms of scientific data and evidence that are used to make claims and draw conclusions. Displaying this competency may require all three forms of scientific knowledge.

Those who possess this competency should be able to interpret the meaning of scientific evidence and its implications to a specified audience in their own words, using diagrams or other representations as appropriate. This competency requires the use of mathematical tools to analyse or summarise data, and the ability to use standard methods to transform data into different representations.

This competency also includes accessing scientific information and producing and evaluating arguments and conclusions based on scientific evidence (Kuhn, 2010; Osborne, 2010). It may also involve evaluating alternative conclusions using

## Scientific knowledge

### Content knowledge

Given that only a sample of the content domain of science can be assessed in the PISA 2015 scientific literacy assessment, clear criteria are used to guide the selection of the knowledge that is assessed. The criteria are applied to knowledge from the major fields of physics, chemistry, biology, earth and space sciences, and require that the knowledge:

- has relevance to real-life situations
- represents an important scientific concept or major explanatory theory that has enduring utility
- is appropriate to the developmental level of 15-year-olds.

It is thus assumed that students have some knowledge and understanding of the major explanatory ideas and theories of science, including an understanding of the history and scale of the universe, the particle model of matter, and the theory of evolution by natural selection. These examples of major explanatory ideas are provided for illustrative purposes; there has been no attempt to list comprehensively all the ideas and theories that might be considered fundamental for a scientifically literate individual.

### Figure 2.5 - Knowledge of the content of science

#### Physical systems that require knowledge of:

- Structure of matter (e.g. particle model, bonds)
- Properties of matter (e.g. changes of state, thermal and electrical conductivity)
- Chemical changes of matter (e.g. chemical reactions, energy transfer, acids/bases)
- Motion and forces (e.g. velocity, friction) and action at a distance (e.g. magnetic, gravitational and electrostatic forces)
- Energy and its transformation (e.g. conservation, dissipation, chemical reactions)
- Interactions between energy and matter (e.g. light and radio waves, sound and seismic waves)

#### Living systems that require knowledge of:

- Cells (e.g. structures and function, DNA, plant and animal)
- The concept of an organism (e.g. unicellular and multicellular)
- Humans (e.g. health, nutrition, subsystems such as digestion, respiration, circulation, excretion, reproduction and their relationship)
- Populations (e.g. species, evolution, biodiversity, genetic variation)
- Ecosystems (e.g. food chains, matter and energy flow)
- Biosphere (e.g. ecosystem services, sustainability)

#### Earth and space systems that require knowledge of:

- Structures of the Earth systems (e.g. lithosphere, atmosphere, hydrosphere)
- Energy in the Earth systems (e.g. sources, global climate)
- Change in Earth systems (e.g. plate tectonics, geochemical cycles, constructive and destructive forces)
- Earth's history (e.g. fossils, origin and evolution)
- Earth in space (e.g. gravity, solar systems, galaxies)
- The history and scale of the universe and its history (e.g. light year, Big Bang theory)

Figure 2.5 shows the content knowledge categories and examples selected by applying these criteria. Such knowledge is required for understanding the natural world and for making sense of experiences in personal, local/national and global contexts. The framework uses the term “systems” instead of “sciences” in the descriptors of content knowledge. The intention is to convey the idea that citizens have to understand concepts from the physical and life sciences, and earth and space sciences, and how they apply in contexts where the elements of knowledge are interdependent or interdisciplinary. Things viewed as subsystems at one scale may be viewed as whole systems at a smaller scale. For example, the circulatory system can be seen as an entity in itself or as a subsystem of the human body; a molecule can be studied as a stable configuration of atoms but also as a subsystem of a cell or a gas. Thus, applying scientific knowledge and exhibiting scientific competencies requires a determination of which system and which boundaries apply in any particular context.

Table 2.2 shows the desired distribution of items, by content of science.

| System          |   Percentage of total items |
|-----------------|-----------------------------|
| Physical        |                          36 |
| Living          |                          36 |
| Earth and space |                          28 |
| Total           |                         100 |

### Procedural knowledge

A fundamental goal of science is to generate explanatory accounts of the material world. Tentative explanatory accounts are first developed and then tested through empirical enquiry. Empirical enquiry relies on certain well-established concepts, such as the notion of dependent and independent variables, the control of variables, types of measurement, forms of error, methods of minimising error, common patterns observed in data, and methods of presenting data.

It is this knowledge of the concepts and procedures that are essential for scientific enquiry that underpins the collection, analysis and interpretation of scientific data. Such ideas form a body of procedural knowledge that has also been called “concepts of evidence” (Gott, Duggan and Roberts, 2008; Millar et al., 1995). One can think of procedural knowledge as knowledge of the standard procedures scientists use to obtain reliable and valid data. Such knowledge is needed both to undertake scientific enquiry and engage in critical reviews of the evidence that might be used to support particular claims. It is expected, for instance, that students will know that scientific knowledge has differing degrees of certainty associated with it, and so can explain why there is a difference between the confidence associated with measurements of the speed of light (which has been measured many times with ever more accurate instrumentation) and measurements of fish stocks in the North Atlantic or the mountain lion population in California. The examples listed in Figure 2.6 convey the general features of procedural knowledge that may be tested.

#### Figure 2.6 - PISA 2015 procedural knowledge

##### Procedural knowledge

- The concept of variables, including dependent, independent and control variables.
- Concepts of measurement, e.g. quantitative (measurements), qualitative (observations), the use of a scale, categorical and continuous variables.
- Ways of assessing and minimising uncertainty, such as repeating and averaging measurements.
- Mechanisms to ensure the replicability (closeness of agreement between repeated measures of the same quantity) and accuracy of data (the closeness of agreement between a measured quantity and a true value of the measure).
- Common ways of abstracting and representing data using tables, graphs and charts, and using them appropriately.
- The control-of-variables strategy and its role in experimental design or the use of randomised controlled trials to avoid confounded findings and identify possible causal mechanisms.
- The nature of an appropriate design for a given scientific question, e.g. experimental, field-based or pattern-seeking.

### Epistemic knowledge

Epistemic knowledge refers to an understanding of the role of specific constructs and defining features essential to the process of knowledge building in science (Duschl, 2007). Those who have such knowledge can explain, with examples, the distinction between a scientific theory and a hypothesis or a scientific fact and an observation. They know that models, whether representational, abstract or mathematical, are a key feature of science, and that such models are

like maps rather than accurate pictures of the material world. These students can recognise that any particle model of matter is an idealised representation of matter and can explain how the Bohr model is a limited model of what we know about the atom and its constituent parts. They recognise that the concept of a “theory” as used in science is not the same as the notion of a “theory” in everyday language, where it is used as a synonym for a “guess” or a “hunch”. Procedural knowledge is required to explain what is meant by the control-of-variables strategy; epistemic knowledge is required to explain why the use of the control-of-variables strategy or the replication of measurements is central to establishing knowledge in science.

Scientifically literate individuals also understand that scientists draw on data to advance claims to knowledge, and that argument is a commonplace feature of science. In particular, they know that some arguments in science are hypothetico-deductive (e.g. Copernicus’ argument for the heliocentric system), some are inductive (the conservation of energy), and some are an inference to the best explanation (Darwin’s theory of evolution or Wegener’s argument for moving continents). They also understand the role and significance of peer review as the mechanism that the scientific community has established for testing claims to new knowledge. As such, epistemic knowledge provides a rationale for the procedures and practices in which scientists engage, a knowledge of the structures and defining features that guide scientific enquiry, and the foundation for the basis of belief in the claims that science makes about the natural world.

Figure 2.7 represents what are considered to be the major features of epistemic knowledge necessary for scientific literacy.

Figure 2.7 - PISA 2015 epistemic knowledge

## Epistemic knowledge

The constructs and defining features of science. That is:

- The nature of scientific observations, facts, hypotheses, models and theories.
- The purpose and goals of science (to produce explanations of the natural world) as distinguished from technology (to produce an optimal solution to human need), and what constitutes a scientific or technological question and appropriate data.
- The values of science, e.g. a commitment to publication, objectivity and the elimination of bias.
- The nature of reasoning used in science, e.g. deductive, inductive, inference to the best explanation (abductive), analogical, and model-based.

The role of these constructs and features in justifying the knowledge produced by science. That is:

- How scientific claims are supported by data and reasoning in science.
- The function of different forms of empirical enquiry in establishing knowledge, their goal (to test explanatory hypotheses or identify patterns) and their design (observation, controlled experiments, correlational studies).
- How measurement error affects the degree of confidence in scientific knowledge.
- The use and role of physical, system and abstract models and their limits.
- The role of collaboration and critique, and how peer review helps to establish confidence in scientific claims.
- The role of scientific knowledge, along with other forms of knowledge, in identifying and addressing societal and technological issues.

Epistemic knowledge is most likely to be tested pragmatically in a context where a student is required to interpret and answer a question that requires some of this type of knowledge rather than assessing directly whether they understand the features detailed in Figure 2.7. For example, students may be asked to identify whether the conclusions are justified by the data, or what piece of evidence best supports the hypothesis advanced in an item and explain why.

Table 2.3 describes the desired distribution of items by type of knowledge.

Table 2.3 Desired distribution of items, by type of knowledge

| Knowledge   | Percentage of total items   |
|-------------|-----------------------------|
| Content     | 54-66                       |
| Procedural  | 19-31                       |
| Epistemic   | 10-22                       |

The desired balance, by percentage of items, among the three knowledge components – content, procedural and epistemic – is shown in Table 2.4. These weightings are broadly consistent with the previous framework and reflect a consensus view among the experts consulted during the drafting of this framework.

## Table 2.4 Desired distribution of items for knowledge

| Knowledge types            | Physical   | Living   | Earth and space   | Total over systems   |
|----------------------------|------------|----------|-------------------|----------------------|
| Content                    | 20-24      | 20-24    | 14-18             | 54-66                |
| Procedural                 | 7-11       | 7-11     | 5-9               | 19-31                |
| Epistemic                  | 4-8        | 4-8      | 2-6               | 10-22                |
| Total over knowledge types | 36         | 36       | 28                | 100                  |

### Sample items

In this section, three examples of science units are presented. The first is from PISA 2006 and is included to demonstrate the linkage between the 2006 and the 2015 frameworks. Questions from the unit are shown in the original paper-based format and also how they might be transposed and presented on screen. The second example is a new onscreen unit illustrating the 2015 scientific literacy framework. The third example illustrates an interactive, simulated scientific-enquiry environment which allows for assessing students' proficiency in science within a real world setting.

Other examples of science items are available on the PISA website (www.oecd.org/pisa/), including interactive examples (forthcoming, November 2016).

### Science example 1: GREENHOUSE

Science example 1 is entitled GREENHOUSE and deals with the increase in the average temperature of the Earth's atmosphere. The stimulus material consists of a short text introducing the term "Greenhouse effect" and includes graphical information on the average temperature of the Earth's atmosphere and carbon dioxide emissions on Earth over time.

The area of application is Environment Quality within a global setting.

*Read the texts and answer the questions that follow.*

### THE GREENHOUSE EFFECT: FACT OR FICTION?

Living things need energy to survive. The energy that sustains life on the Earth comes from the Sun, which radiates energy into space because it is so hot. A tiny proportion of this energy reaches the Earth.

The Earth's atmosphere acts like a protective blanket over the surface of our planet, preventing the variations in temperature that would exist in an airless world.

Most of the radiated energy coming from the Sun passes through the Earth's atmosphere. The Earth absorbs some of this energy, and some is reflected back from the Earth's surface. Part of this reflected energy is absorbed by the atmosphere.

As a result of this the average temperature above the Earth's surface is higher than it would be if there were no atmosphere. The Earth's atmosphere has the same effect as a greenhouse, hence the term greenhouse effect.

The greenhouse effect is said to have become more pronounced during the twentieth century.

It is a fact that the average temperature of the Earth's atmosphere has increased. In newspapers and periodicals the increased carbon dioxide emission is often stated as the main source of the temperature rise in the twentieth century.

A student named André becomes interested in the possible relationship between the average temperature of the Earth's atmosphere and the carbon dioxide emission on the Earth.

In a library he comes across the following two graphs.

<!-- image --> Carbon dioxide emission graph <!-- image --> Average temperature graph

| Carbon dioxide emission (thousand millions of tonnes per year)   | Average temperature of the Earth's atmosphere (°C)   |
|------------------------------------------------------------------|------------------------------------------------------|
|                                                                  |                                                      |

André concludes from these two graphs that it is certain that the increase in the average temperature of the Earth's atmosphere is due to the increase in the carbon dioxide emission.

## GREENHOUSE – QUESTION 1

What is it about the graphs that supports André's conclusion?

### Figure 2.8 - Framework categorisation for GREENHOUSE question 1

| Framework categories   | 2006 Framework                      | 2015 Framework                      |
|------------------------|-------------------------------------|-------------------------------------|
| Knowledge type         | Knowledge about science             | Epistemic                           |
| Competency             | Explaining phenomena scientifically | Explaining phenomena scientifically |
| Context                | Environmental, global               | Environmental, global               |
| Cognitive demand       | Not applicable                      | Medium                              |

Question 1 demonstrates how the 2015 framework largely maps onto the same categories as the 2006 framework, using the same competency and context categorisations. The 2006 framework included two categorisations of scientific knowledge: knowledge of science (referring to knowledge of the natural world across the major fields of science) and knowledge about science (referring to the means and goals of science). The 2015 framework elaborates on these two aspects, subdividing knowledge about science into procedural and epistemic knowledge. Question 1 requires students not only to understand how the data is represented in the two graphs but also to consider whether this evidence scientifically justifies a given conclusion. This is one of the features of epistemic knowledge in the 2015 framework. The context categorisation is “environmental, global”. A new feature of the 2015 framework is consideration of cognitive demand. This question requires an interpretation of graphs involving a few linked steps; thus, according to the framework, it is categorised as medium cognitive demand.

## GREENHOUSE – QUESTION 2

Another student, Jeanne, disagrees with André's conclusion. She compares the two graphs and says that some parts of the graphs do not support his conclusion.

Give an example of a part of the graphs that does not support André's conclusion. Explain your answer.

### Figure 2.9 - Framework categorisation for GREENHOUSE question 2

| Framework categories   | 2006 Framework                      | 2015 Framework                      |
|------------------------|-------------------------------------|-------------------------------------|
| Knowledge type         | Knowledge about science             | Epistemic                           |
| Competency             | Explaining phenomena scientifically | Explaining phenomena scientifically |
| Context                | Environmental, global               | Environmental, global               |
| Cognitive demand       | Not applicable                      | Medium                              |

Question 2 requires students to study the two graphs in detail. The knowledge, competency, context and cognitive demand are in the same categories as question 1.

## GREENHOUSE – QUESTION 3

André persists in his conclusion that the average temperature rise of the Earth's atmosphere is caused by the increase in the carbon dioxide emission. But Jeanne thinks that his conclusion is premature. She says: "Before accepting this conclusion you must be sure that other factors that could influence the greenhouse effect are constant".

Name one of the factors that Jeanne means.

### Figure 2.10 - Framework categorisation for GREENHOUSE question 3

| Framework categories   | 2006 Framework                      | 2015 Framework                      |
|------------------------|-------------------------------------|-------------------------------------|
| Knowledge type         | Knowledge about science             | Procedural                          |
| Competency             | Explaining phenomena scientifically | Explaining phenomena scientifically |
| Context                | Environmental, global               | Environmental, global               |
| Cognitive demand       | Not applicable                      | Medium                              |

Question 3 requires students to consider control variables in terms of the critical review of evidence used to support claims. This is categorised as “procedural knowledge” in the 2015 framework.

The screenshots below illustrate how the GREENHOUSE question would be presented in an onscreen environment. The text and graphs are essentially unchanged, with students using page turners on the top right of the screen to view graphs and text as required. As the original questions were open responses, the onscreen version also necessitates an open-response format in order to replicate the paper version as closely as possible, ensuring comparability between delivery modes and therefore protecting comparability of data over time.

### Figure 2.11 - GREENHOUSE presented onscreen: Stimulus page 1

#### PISA 2015

**Greenhouse effect**

**Introduction**

**THE GREENHOUSE EFFECT: FACT OR FICTION?**

Living things need energy to survive. The energy that sustains life on the Earth comes from the Sun, which radiates energy into space because it is so hot. A tiny proportion of this energy reaches the Earth.

The Earth's atmosphere acts like a protective blanket over the surface of our planet, preventing the variations in temperature that would exist in an airless world. Most of the radiated energy coming from the Sun passes through the Earth's atmosphere. The Earth absorbs some of this energy, and some is reflected back from the Earth's surface. Part of this reflected energy is absorbed by the atmosphere.

As a result of this the average temperature above the Earth's surface is higher than it would be if there were no atmosphere. The Earth's atmosphere has the same effect as a greenhouse, hence the term greenhouse effect.

The greenhouse effect is said to have become more pronounced during the twentieth century.

It is a fact that the average temperature of the Earth's atmosphere has increased. In newspapers and periodicals the increased carbon dioxide emission is often stated as the main source of the temperature rise in the twentieth century.

### Figure 2.12 - GREENHOUSE presented onscreen: Stimulus page 2

#### PISA 2015

**Greenhouse effect**

**Introduction**

Now click on Next to view the first question.

A student named André becomes interested in the possible relationship between the average temperature of the Earth's atmosphere and the carbon dioxide emission on the Earth.

In a library he comes across the following two graphs.

<!-- image -->

<!-- image -->

André concludes from these two graphs that it is certain that the increase in the average temperature of the Earth's atmosphere is due to the increase in the carbon dioxide emission.

## Figure 2.13 - GREENHOUSE presented onscreen: Question 1

### PISA 2015

#### Greenhouse effect

Question 1/3

Type your answer to the question below.

What is it about the graphs that supports André's conclusion?

### THE GREENHOUSE EFFECT: FACT OR FICTION?

Living things need energy to survive. The energy that sustains life on the Earth comes from the Sun, which radiates energy into space because it is so hot. A tiny proportion of this energy reaches the Earth.

The Earth's atmosphere acts like a protective blanket over the surface of our planet, preventing the variations in temperature that would exist in an airless world. Most of the radiated energy coming from the Sun passes through the Earth's atmosphere. The Earth absorbs some of this energy, and some is reflected back from the Earth's surface. Part of this reflected energy is absorbed by the atmosphere.

As a result of this the average temperature above the Earth's surface is higher than it would be if there were no atmosphere. The Earth's atmosphere has the same effect as a greenhouse, hence the term greenhouse effect.

The greenhouse effect is said to have become more pronounced during the twentieth century.

It is a fact that the average temperature of the Earth's atmosphere has increased. In newspapers and periodicals the increased carbon dioxide emission is often stated as the main source of the temperature rise in the twentieth century.

## Figure 2.14 - GREENHOUSE presented onscreen: Question 2

### PISA 2015

#### Greenhouse effect

Question 2/3

Type your answer to the question below.

Another student, Jeanne, disagrees with André's conclusion. She compares the two graphs and says that some parts of the graphs do not support his conclusion.

Give an example of a part of the graphs that does not support André's conclusion. Explain your answer.

### THE GREENHOUSE EFFECT: FACT OR FICTION?

Living things need energy to survive. The energy that sustains life on the Earth comes from the Sun, which radiates energy into space because it is so hot. A tiny proportion of this energy reaches the Earth.

The Earth's atmosphere acts like a protective blanket over the surface of our planet, preventing the variations in temperature that would exist in an airless world. Most of the radiated energy coming from the Sun passes through the Earth's atmosphere. The Earth absorbs some of this energy, and some is reflected back from the Earth's surface. Part of this reflected energy is absorbed by the atmosphere.

As a result of this the average temperature above the Earth's surface is higher than it would be if there were no atmosphere. The Earth's atmosphere has the same effect as a greenhouse, hence the term greenhouse effect.

The greenhouse effect is said to have become more pronounced during the twentieth century.

It is a fact that the average temperature of the Earth's atmosphere has increased. In newspapers and periodicals the increased carbon dioxide emission is often stated as the main source of the temperature rise in the twentieth century.

## Figure 2.15 - GREENHOUSE presented onscreen: Question 3

### PISA 2015

#### Greenhouse effect

Question 3/3

**Type your answer to the question below.**

André persists in his conclusion that the average temperature rise of the Earth's atmosphere is caused by the increase in the carbon dioxide emission. But Jeanne thinks that his conclusion is premature. She says: "Before accepting this conclusion you must be sure that other factors that could influence the greenhouse effect are constant".

Name one of the factors that Jeanne means.

### THE GREENHOUSE EFFECT: FACT OR FICTION?

Living things need energy to survive. The energy that sustains life on the Earth comes from the Sun, which radiates energy into space because it is so hot. A tiny proportion of this energy reaches the Earth.

The Earth's atmosphere acts like a protective blanket over the surface of our planet, preventing the variations in temperature that would exist in an airless world. Most of the radiated energy coming from the Sun passes through the Earth's atmosphere. The Earth absorbs some of this energy, and some is reflected back from the Earth's surface. Part of this reflected energy is absorbed by the atmosphere.

As a result of this the average temperature above the Earth's surface is higher than it would be if there were no atmosphere. The Earth's atmosphere has the same effect as a greenhouse, hence the term greenhouse effect.

The greenhouse effect is said to have become more pronounced during the twentieth century.

It is a fact that the average temperature of the Earth's atmosphere has increased. In newspapers and periodicals the increased carbon dioxide emission is often stated as the main source of the temperature rise in the twentieth century.

## Science example 2: SMOKING

This new 2015 exemplar unit explores various forms of evidence linked to the harmful effects of smoking and the methods used to help people to stop smoking. New scientific literacy items for 2015 are only developed for computer-based delivery and therefore this exemplar is only shown in an onscreen format.

All onscreen standard question types in the PISA 2015 computer platform have a vertical split screen with the stimuli presented on the right side and the questions and answer mechanisms on the left side.

## Figure 2.16 - SMOKING: Question 1

### PISA 2015 Unit Name: SMOKING

#### Question 1/9

John and Rose are researching cigarette smoking for a school project. Read John's research on the right. Then respond to the question below.

Select two reasons from the list below that suggest why cigarette companies could claim there was no evidence that tar from cigarette smoke caused cancer in humans.

- [ ] Humans are immune to tar
- [ ] Experiments were carried out with mice
- [ ] Chemicals from smoking decreased the effects of tar
- [ ] Humans may react differently from mice
- [ ] Filter-tip cigarettes remove all tar from smoke

### John's research

In the 1950s research studies found that tar from cigarette smoke caused cancer in mice. Tobacco companies claimed there was no evidence that smoking caused cancer in humans. They also began to produce filter-tip cigarettes.

## SMOKING - QUESTION 1

This question requires students to interpret given evidence using their knowledge of scientific concepts. They need to read the information in the stimulus about early research into the potential harmful effects of smoking, and then select two options from the menu to answer the question.

In this question, students have to apply content knowledge using the competency of "explaining phenomena scientifically". The context is categorised as "health and disease" in a local/national setting. The cognitive demand requires the use and application of conceptual knowledge and is therefore categorised as a medium level of demand.

### Figure 2.17 - Framework categorisation for SMOKING question 1

| Framework categories   | 2015 Framework                         |
|------------------------|----------------------------------------|
| Knowledge type         | Content                                |
| Competency             | Explaining phenomena scientifically    |
| Context                | Health and disease, local and national |
| Cognitive demand       | Medium                                 |

## SMOKING - QUESTION 2

This question explores students' understanding of data.

The right side of the screen shows authentic data of cigarette consumption and deaths from lung cancer in men over an extended period of time. Students are asked to select the best descriptor of the data by clicking on one of the radio buttons next to answer statements on the left side of the screen.

### Figure 2.18 - SMOKING: Question 2

PISA 2015 Unit Name: SMOKING

Question 3/9

Rose found a graph while doing research on smoking.

Refer to Rose's research on the right. Then select the best response to the question below.

Which statement best describes the data shown in the graph?

- The graph shows that all men who smoked cigarettes developed lung cancer.
- The graph shows that more men smoked cigarettes in the 1940's than in 2010.
- There is no link between cigarettes smoked and deaths from lung cancer.
- There is a positive link between cigarettes smoked and deaths from lung cancer.

Rose's research

|   Cigarettes smoked per person per year | Lung cancer deaths (per 100,000 people)   |
|-----------------------------------------|-------------------------------------------|
|                                    4000 | 150                                       |
|                                    3000 | 100                                       |
|                                    2000 | 50                                        |
|                                    1000 |                                           |
|                                    1900 |                                           |
|                                    1920 |                                           |
|                                    1940 |                                           |
|                                    1960 |                                           |
|                                    1980 |                                           |

This unit tests content knowledge using the competency of "interpreting data and evidence scientifically".

The context is "health and disease" applied to a local/national setting. As students need to interpret the relationship between two graphs, the cognitive demand is categorised as medium.

## Figure 2.19 - Framework categorisation for SMOKING question 2

| Framework categories   | 2015 Framework                             |
|------------------------|--------------------------------------------|
| Knowledge type         | Content                                    |
| Competency             | Interpret data and evidence scientifically |
| Context                | Health and disease, local and national     |
| Cognitive demand       | Medium                                     |

## Science example 3: ZEER POT

This new 2015 exemplar unit features the use of interactive tasks using simulations of scientific enquiry to explore and assess scientific literacy knowledge and competencies.

This unit focuses on an authentic low-cost cooling container called a Zeer pot, developed for local use in Africa, using readily available local resources. Cost and lack of electricity limit the use of refrigerators in these regions, even though the hot climate requires that people keep food cool so that it can be kept for a longer time before bacterial growth renders it a risk to health.

The first screen shot of this simulation introduces what a Zeer pot looks like and how it works. Students are not expected to have an understanding of how the process of evaporation causes cooling, just that it does.

Using this simulation, students are asked to investigate the conditions that will produce the most effective cooling effects (4°C) for keeping food fresh in the Zeer pot. The simulator keeps certain conditions constant (the air temperature and the humidity), but includes this information to enhance the authentic contextual setting. In the first question, students are asked to investigate the optimum conditions to keep the maximum amount of food fresh in the Zeer pot by altering the thickness of the sand layer and the moisture conditions.

## Figure 2.20 - ZEER POT: Stimulus

### PISA 2015 Unit name: ZEER POT

#### Introduction

A zeer pot refrigeration is an invention to keep food cool without electricity, usually found in African countries.

A small clay pot sits inside a larger clay pot with a clay or fabric lid. The space between the two pots is filled with sand. This creates an insulating layer around the inner pot. The sand is kept damp by adding water at regular intervals. When the water evaporates, the temperature in the inner pot is reduced.

Local people make zeer pots out of clay, a locally available resource.

**Zeer Pot**

Inner clay pot. Food is placed here

Layer of damp sand

Outer clay pot

Cloth or fabric lid

Stand

## Figure 2.21 - ZEER POT: Question 1

### PISA 2015 Unit name: ZEER POT

#### Task 1

You have been asked to investigate the best design of a Zeer pot for a family to keep their food fresh.

Food is best kept at a temperature of 4°C to maximise freshness and minimise bacterial growth.

Use the simulator opposite to work out the maximum amount of food that can be kept fresh (at 4°C) by varying the thickness and moisture condition of the sand layer.

You can run a number of simulations, and repeat or remove any data findings.

Maximum amount of food kept fresh at 4°C is \_\_\_\_\_\_ kg

<!-- image -->

Zeer Pot Diagram

| Thickness of sand layer (cm)   | Amount of food (kg)   | Sand moisture (damp/dry)   | Temperature (°C)   |
|--------------------------------|-----------------------|----------------------------|--------------------|
|                                |                       |                            |                    |

Constant variables:

- Air temp 38°C
- Humidity 20%

When students have set their conditions (which also alter the visual display of the on-screen Zeer pot), they press the record-data button, which then runs the simulation and populates the data chart. They need to run a number of data simulations, and can remove data or repeat any simulations as required. This screen then records their response to the maximum amount of food kept fresh at 4°C. Their approaches to the design and evaluation of this form of scientific enquiry can be assessed in subsequent questions.

The knowledge categorisation for this item is “procedural”, and the competence is “evaluate and design scientific enquiry”. The context categorisation is “natural resources”, although it also has links to “health and disease”. The cognitive demand of this question is categorised as high because students are given a complex situation, and they need to develop a systematic sequence of investigations to answer the question.

### Figure 2.22 - Framework categorisation for ZEERPOT question 1

| Framework categories   | 2015 Framework                         |
|------------------------|----------------------------------------|
| Knowledge type         | Procedural                             |
| Competency             | Evaluate and design scientific enquiry |
| Context                | Natural resources                      |
| Cognitive demand       | High                                   |

### Attitudes

#### Why attitudes matter

Peoples' attitudes towards science play a significant role in their interest, attention and response to science and technology, and to issues that affect them specifically. One goal of science education is to develop attitudes that lead students to engage with scientific issues. Such attitudes also support the subsequent acquisition and application of scientific and technological knowledge for personal, local/national and global benefit, and lead to the development of self-efficacy (Bandura, 1997).

Attitudes form part of the construct of scientific literacy. That is, a person's scientific literacy includes certain attitudes, beliefs, motivational orientations, self-efficacy and values. The construct of attitudes used in PISA draws upon Klopfer's (1976) structure for the affective domain in science education and reviews of attitudinal research (Gardner, 1975; Osborne, Simon and Collins, 2003; Schibeci, 1984). A major distinction made in these reviews is between attitudes towards science and scientific attitudes. While the former is measured by the level of interest displayed in scientific issues and activities, the latter is a measure of a disposition to value empirical evidence as the basis of belief.

### Defining attitudes towards science in PISA 2015

The PISA 2015 assessment evaluates students' attitudes towards science in three areas: interest in science and technology, environmental awareness, and valuing scientific approaches to enquiry (see Figure 2.23), which are considered core to the construct of scientific literacy. These three areas were selected for measurement because a positive attitude towards science, a concern for the environment and an environmentally sustainable way of life, and a disposition to value the scientific approach to enquiry are characteristics of a scientifically literate individual. Thus, the extent to which individual students are, or are not interested in science and recognise its value and implications is considered an important measure of the outcome of compulsory education. Moreover, in 52 of the countries (including all OECD countries) that participated in PISA 2006, students with a higher general interest in science performed better in science (OECD, 2007:143).

Interest in science and technology was selected because of its established relationships with achievement, course selection, career choice and lifelong learning. For instance, there is a considerable body of literature which shows that interest in science is established by age 14 for the majority of students (Ormerod and Duckworth, 1975; Tai et al., 2006). Moreover, students with such an interest are more likely to pursue scientific careers. Policy concerns in many OECD countries about the number of students, particularly girls, who choose to pursue the study of science make the measurement of attitudes towards science an important aspect of the PISA assessment. The results may provide information about a perceived declining interest in the study of science among young people (Bøe et al., 2011). This measure, when correlated with the large body of other information collected by PISA through the student, teacher and school questionnaires, may provide insights into the causes of any decline in interest.

Valuing scientific approaches to enquiry was chosen because scientific approaches to enquiry have been highly successful at generating new knowledge – not only within science itself, but also in the social sciences, and even finance and sports. Moreover, the core value of scientific enquiry and the Enlightenment is the belief in empirical evidence as the basis of rational belief. Recognising the value of the scientific approach to enquiry is, therefore, widely regarded as a fundamental objective of science education that warrants assessing.

Appreciation of, and support for, scientific enquiry implies that students can identify and also value scientific ways of gathering evidence, thinking creatively, reasoning rationally, responding critically and communicating conclusions as they confront life situations related to science and technology. Students should understand how scientific approaches to enquiry function, and why they have been more successful than other methods in most cases. Valuing scientific approaches to enquiry, however, does not mean that an individual has to be positively disposed towards all aspects of science or even use such methods themselves. Thus, the construct is a measure of students' attitudes towards the use of a scientific method to investigate material and social phenomenon and the insights that are derived from such methods.

Environmental awareness is of international concern, as well as being of economic relevance. Attitudes in this area have been the subject of extensive research since the 1970s (see, for example, Bogner and Wiseman, 1999; Eagles and Demare, 1999; Rickinson, 2001; Weaver, 2002). In December 2002, the United Nations approved resolution 57/254 declaring the ten-year period beginning on 1 January 2005 to be the United Nations Decade of Education for Sustainable Development (UNESCO, 2003). The International Implementation Scheme (UNESCO, 2005) identifies the environment as one of the three spheres of sustainability (along with society, including culture, and economy) that should be included in all education programmes for sustainable development.

Given the importance of environmental issues to the continuation of life on Earth and the survival of humanity, young people today need to understand the basic principles of ecology and the need to organise their lives accordingly. This means that developing environmental awareness and a responsible disposition towards the environment is an important element of contemporary science education.

In PISA 2015 these specific attitudes towards science are measured through the student questionnaire. Further detail of these constructs can be found in the Questionnaire framework, Chapter 5.

## Assessing Scientific Literacy

### Cognitive Demand

A key new feature of the PISA 2015 framework is the definition of levels of cognitive demand within the assessment of scientific literacy and across all three competencies of the framework. In assessment frameworks, item difficulty, which is empirically derived, is often confused with cognitive demand. Empirical item difficulty is estimated from the proportion of test-takers who solve the item correctly, and thus assesses the amount of knowledge held by the test-taker population, whereas cognitive demand refers to the type of mental processes required (Davis and Buckendahl, 2011). Care needs to be taken to ensure that the depth of knowledge required, i.e., the cognitive demand test items, is understood explicitly by the item developers and users of the PISA framework. For instance, an item can have high difficulty because the knowledge it is testing is not well known, but the cognitive demand is simply recall. Conversely, an item can be cognitively demanding because it requires the individual to relate and evaluate many items of knowledge – each of which is easily recalled. Thus, not only should the PISA test instrument discriminate in terms of performance between easier and harder test items, the test also needs to provide information on how students across the ability range can deal with problems at different levels of cognitive demand (Brookhart and Nitko, 2011).

The competencies are articulated using a range of terms defining cognitive demand through the use of verbs such as “recognise”, “interpret”, “analyse” and “evaluate”. However, in themselves these verbs do not necessarily indicate a hierarchical order of difficulty that is dependent on the level of knowledge required to answer any item. Various classifications of cognitive demand schemes have been developed and evaluated since Bloom’s Taxonomy was first published (Bloom, 1956). These have been largely based on categorisations of knowledge types and associated cognitive processes that are used to describe educational objectives or assessment tasks.

Bloom’s revised Taxonomy (Anderson and Krathwohl, 2001) identifies four categories of knowledge – factual, conceptual, procedural and meta-cognitive. This categorisation considers these forms of knowledge to be hierarchical and distinct from the six categories of performance used in Bloom’s first taxonomy – remembering, understanding, applying, analysing, evaluating and creating. In Anderson and Krathwohl’s framework, these two dimensions are now seen to be independent of each other, allowing for lower levels of knowledge to be crossed with higher order skills, and vice versa.

A similar framework is offered by Marzano and Kendall’s Taxonomy (2007), which also provides a two-dimensional framework based on the relationship between how mental processes are ordered and the type of knowledge required. The use of mental processes is seen as a consequence of a need to engage with a task with meta-cognitive strategies that define potential approaches to solving problems. The cognitive system then uses either retrieval, comprehension, analysis or knowledge utilisation. Marzano and Kendall divide the knowledge domain into three types of knowledge, information, mental procedures and psychomotor, compared to the four categories in Bloom’s revised Taxonomy. Marzano and Kendall argue that their taxonomy is an improvement upon Bloom’s Taxonomy because it offers a model of how humans actually think rather than simply an organising framework.

A different approach is offered by Ford and Wargo (2012), who offer a framework for scaffolding dialogue as a way of considering cognitive demand. Their framework uses four levels that build on each other: recall, explain, juxtapose and evaluate. Although this framework has not been specifically designed for assessment purposes, it has many similarities to the PISA 2015 definition of scientific literacy and the need to make more explicit references to such demands in the knowledge and competencies.

Another schema can be found in the framework based on Depth of Knowledge developed by Webb (1997) specifically to address the disparity between assessments and the expectations of student learning. For Webb, levels of depth can be determined by taking into account the complexity of both the content and the task required. His schema consists of four major categories: level 1 (recall), level 2 (using skills and/or conceptual knowledge), level 3 (strategic thinking) and level 4 (extended thinking). Each category is populated with a large number of verbs that can be used to describe cognitive processes. Some of these appear at more than one level. This framework offers a more holistic view of learning and assessment tasks, and requires an analysis of both the content and cognitive process demanded by any task. Webb’s Depth of Knowledge (DOK) approach is a simpler but more operational version of the SOLO Taxonomy (Biggs and Collis, 1982) which describes a continuum of student understanding through five distinct stages of pre-structural, unistructural, multistructural, relational and extended abstract understanding.

All the frameworks described briefly above have served to develop the knowledge and competencies in the PISA 2015 Framework. In drawing up such a framework, it is recognised that there are challenges in developing test items based on a cognitive hierarchy. The three main challenges are that:

a) Too much effort is made to fit test items into particular cognitive frameworks, which can lead to poorly developed items.

b) Intended items (with frameworks defining rigorous, cognitively demanding goals) may differ from actual items (which may operationalise the standard in a much less cognitively demanding way).

c) Without a well-defined and understood cognitive framework, item writing and development often focuses on item difficulty and uses a limited range of cognitive processes and knowledge types, which are then only described and interpreted post hoc, rather than building from a theory of increasing competency.

The approach taken in this framework is to use an adapted version of Webb's Depth of Knowledge grid (Webb, 1997) alongside the desired knowledge and competencies. As the competencies are the central feature of the framework, the cognitive framework needs to assess and report on them across the student ability range. Webb's Depth of Knowledge Levels offer a taxonomy for cognitive demand that requires items to identify both the cognitive demand from the verbal cues that are used, e.g. analyse, arrange, compare, and the expectations of the depth of knowledge required.

Figure 2.23 - PISA 2015 Framework for Cognitive Demand

| Depth of Knowledge   | Content knowledge   | Procedural knowledge   | Epistemic knowledge   |
|----------------------|---------------------|------------------------|-----------------------|
| Low                  |                     |                        |                       |
| Medium               |                     |                        |                       |
| High                 |                     |                        |                       |

The grid in Figure 2.23 provides a framework for mapping items against the two dimensions of knowledge and competencies. In addition, each item can also be mapped using a third dimension based on a depth-of-knowledge taxonomy. This provides a means of operationalising cognitive demand as each item can be categorised as making demands that are:

- **Low**
    - Carry out a one-step procedure, for example recall a fact, term, principle or concept, or locate a single point of information from a graph or table.
- **Medium**
    - Use and apply conceptual knowledge to describe or explain phenomena, select appropriate procedures involving two or more steps, organise/display data, interpret or use simple data sets or graphs.
- **High**
    - Analyse complex information or data; synthesise or evaluate evidence; justify; reason, given various sources; develop a plan or sequence of steps to approach a problem.

The distribution of items by depth of knowledge is described in Table 2.5.

Table 2.5 Distribution of items by depth of knowledge

| Depth of knowledge   |   Percentage of items |
|----------------------|-----------------------|
| Low                  |                     8 |
| Medium               |                    30 |
| High                 |                    61 |
| Total                |                   100 |

Items that merely require recall of one piece of information make low cognitive demands, even if the knowledge itself might be quite complex. In contrast, items that require recall of more than one piece of knowledge, and require a comparison and evaluation of the competing merits of their relevance would be seen as having high cognitive demand. The difficulty of any item, therefore, is a combination both of the degree of complexity and range of knowledge it requires, and the cognitive operations that are required to process the item.

Therefore, the factors that determine the demand of items assessing science achievement include:

- The number and degree of complexity of elements of knowledge demanded by the item.
- The level of familiarity and prior knowledge that students may have of the content, procedural and epistemic knowledge involved.
- The cognitive operation required by the item, e.g. recall, analysis, evaluation.
- The extent to which forming a response is dependent on models or abstract scientific ideas.

This four-factor approach allows for a broader measure of scientific literacy across a wider range of student ability. Categorising the cognitive processes required for the competencies that form the basis of scientific literacy together with a consideration of the depth of knowledge required offers a model for assessing the level of demand of individual items. In addition, the relative simplicity of the approach offers a way to minimise the problems encountered in applying such frameworks. The use of this cognitive framework also facilitates the development of an a priori definition of the descriptive parameters of the reporting proficiency scales (see Figure 2.25).

## Test characteristics

Figure 2.24 is a variation of Figure 2.2 that presents the basic components of the PISA framework for the 2015 scientific literacy assessment in a way that can be used to relate the framework with the structure and the content of assessment units. This may be used as a tool both to plan assessment exercises and to study the results of standard assessment exercises. As a starting point to construct assessment units, it shows the need to consider the contexts that will serve as stimulus material, the competencies required to respond to the questions or issues, the knowledge central to the exercise, and the cognitive demand.

### Figure 2.24 - A tool for constructing and analysing assessment units and items

<!-- image -->

Diagram

A test unit is defined by specific stimulus material, which may be a brief written passage, or text accompanying a table, chart, graph or diagram. In units created for PISA 2015, the stimulus material may also include non-static stimulus material, such as animations and interactive simulations. The items are a set of independently scored questions of various types, as illustrated by the examples already discussed. Further examples can be found at the PISA website (www.oecd.org/pisa/) (forthcoming, November 2016).

## Response formats

Three classes of items are used to assess the competencies and scientific knowledge identified in the framework. About one-third of the items are in each of the three classes:

- **simple multiple choice:** items calling for
    - selection of a single response from four options
    - selection of a “hot spot”, an answer that is a selectable element within a graphic or text
- **complex multiple choice:** items calling for
    - responses to a series of related “Yes/No” questions that are treated for scoring as a single item (the typical format in 2006)
    - selection of more than one response from a list
    - completion of a sentence by selecting drop-down choices to fill multiple blanks
    - “drag-and-drop” responses, allowing students to move elements on screen to complete a task of matching, ordering or categorising
- **constructed response:** items calling for written or drawn responses
    - Constructed-response items in scientific literacy typically call for a written response ranging from a phrase to a short paragraph (e.g. two to four sentences of explanation). A small number of constructed-response items call for drawing (e.g. a graph or diagram). In a computer-based assessment, any such item is supported by simple drawing editors that are specific to the response required.

In 2015, some responses are captured by interactive tasks, for example, a student’s choices for manipulating variables in a simulated scientific enquiry. Responses to these interactive tasks are likely scored as complex multiple-choice items. Some kinds of responses to interactive tasks may be sufficiently open-ended that they are treated as constructed response.

## Assessment structure

Computer is the primary mode of delivery for all domains, including scientific literacy, in PISA 2015. All new science literacy items are only available on computer. However a paper-based assessment instrument, consisting only of the trend items, is provided for countries choosing not to test their students by computer. (The PISA 2015 field trial studied the effect on student performance of the change in mode of delivery. For further details see Box 1.2.)

Scientific literacy items are organised into 30-minute sections called “clusters”. Each cluster includes either only new units or only trend units. Overall for 2015, the target number of clusters included in the main survey is:

- six clusters of trend units in 2015 main survey
- six clusters of new units in 2015 main survey.

Each student is assigned one two-hour test form. A test form is composed of four clusters, with each cluster designed

to occupy thirty minutes of testing time. The clusters are placed in multiple computer-based test forms, according to a rotated test design.

Each student spends one hour on scientific literacy, with the remaining time assigned to either one or two of the additional domains of reading, mathematics and collaborative problem solving. For any countries taking the paper-based assessment instrument, intact clusters of 2006 units are formed into a number of test booklets. The paper-based assessment is limited to trend items and does not include any newly developed material. In contrast, the computer-based instrument includes newly developed items as well as trend items. When transposing paper-based trend items to an onscreen format, the presentation, response format and cognitive demand remain comparable.

Item contexts are spread across personal, local/national and global settings roughly in the ratio 1:2:1, as was the case in 2006. A wide selection of areas of application are used for units, subject to satisfying as far as possible the various constraints imposed by the distribution of items shown in Tables 2.1 and 2.4.

## Reporting proficiency in science

To achieve the aims of PISA, scales must be developed to measure student proficiency. A descriptive scale of levels of competence needs to be based on a theory of how the competence develops, not just on a post-hoc interpretation of what items of increasing difficulty seem to be measuring. The 2015 draft framework therefore defined explicitly the parameters of increasing competence and progression, allowing item developers to design items representing this growth in ability (Kane, 2006; Mislevy and Haertel, 2006). Initial draft descriptions of the scales are offered below, though it is recognised that these may need to be updated after the main survey. Although comparability with the 2006 scale descriptors (OECD, 2007) has been maximised in order to enable trend analyses, the new elements of the 2015 framework, such as depth of knowledge, have also been incorporated. The scales have also been extended by the addition of a level "1b" to specifically address and provide a description of students at the lowest level of ability who demonstrate minimal scientific literacy and would previously not have been included in the reporting scales. The initial draft scales for 2015 Framework therefore propose more detailed and more specific descriptors of the levels of scientific literacy, and not an entirely different model as shown in Figure 2.25.

### Figure 2.25 - Initial draft of proficiency scale descriptions for science

|   Level | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|       6 | At Level 6, students are able to use content, procedural and epistemic knowledge to consistently provide explanations, evaluate and design scientific enquiries, and interpret data in a variety of complex life situations that require a high level of cognitive demand. They can draw appropriate inferences from a range of different complex data sources, in a variety of contexts and provide explanations of multi-step causal relationships. They can consistently distinguish scientific and non-scientific questions, explain the purposes of enquiry, and control relevant variables in a given scientific enquiry or any experimental design of their own. They can transform data representations, interpret complex data and demonstrate an ability to make appropriate judgments about the reliability and accuracy of any scientific claims. Level 6 students consistently demonstrate advanced scientific thinking and reasoning requiring the use of models and abstract ideas and use such reasoning in unfamiliar and complex situations. They can develop arguments to critique and evaluate explanations, models, interpretations of data and proposed experimental designs in a range of personal, local and global contexts. |
|       5 | At Level 5, students are able to use content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in a variety of life situations in some but not all cases of high cognitive demand. They draw inferences from complex data sources, in a variety of contexts and can explain some multi-step causal relationships. Generally, they can distinguish scientific and non-scientific questions, explain the purposes of enquiry, and control relevant variables in a given scientific enquiry or any experimental design of their own. They can transform some data representations, interpret complex data and demonstrate an ability to make appropriate judgments about the reliability and accuracy of any scientific claims. Level 5 students show evidence of advanced scientific thinking and reasoning requiring the use of models and abstract ideas and use such reasoning in unfamiliar and complex situations. They can develop arguments to critique and evaluate explanations, models, interpretations of data and proposed experimental designs in some but not all personal, local and global contexts.                                                             |

### Figure 2.25 [continued] - Initial draft of proficiency scale descriptions for science

4 At Level 4, students are able to use content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in a variety of given life situations that require mostly a medium level of cognitive demand. They can draw inferences from different data sources, in a variety of contexts and can explain causal relationships. They can distinguish scientific and non-scientific questions, and control variables in some but not all scientific enquiry or in an experimental design of their own. They can transform and interpret data and have some understanding about the confidence held about any scientific claims. Level 4 students show evidence of linked scientific thinking and reasoning and can apply this to unfamiliar situations. Students can also develop simple arguments to question and critically analyse explanations, models, interpretations of data and proposed experimental designs in some personal, local and global contexts.

3 At Level 3, students are able to use content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in some given life situations that require at most a medium level of cognitive demand. They are able to draw a few inferences from different data sources, in a variety of contexts, and can describe and partially explain simple causal relationships. They can distinguish some scientific and non-scientific questions, and control some variables in a given scientific enquiry or in an experimental design of their own. They can transform and interpret simple data and are able to comment on the confidence of scientific claims. Level 3 students show some evidence of linked scientific thinking and reasoning, usually applied to familiar situations. Students can develop partial arguments to question and critically analyse explanations, models, interpretations of data and proposed experimental designs in some personal, local and global contexts.

2 At Level 2, students are able to use content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in some given familiar life situations that require mostly a low level of cognitive demand. They are able to make a few inferences from different sources of data, in few contexts, and can describe simple causal relationships. They can distinguish some simple scientific and non-scientific questions, and distinguish between independent and dependent variables in a given scientific enquiry or in a simple experimental design of their own. They can transform and describe simple data, identify straightforward errors, and make some valid comments on the trustworthiness of scientific claims. Students can develop partial arguments to question and comment on the merits of competing explanations, interpretations of data and proposed experimental designs in some personal, local and global contexts.

1a At Level 1a, students are able to use a little content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in a few familiar life situations that require a low level of cognitive demand. They are able to use a few simple sources of data, in a few contexts and can describe some very simple causal relationships. They can distinguish some simple scientific and non-scientific questions, and identify the independent variable in a given scientific enquiry or in a simple experimental design of their own. They can partially transform and describe simple data and apply them directly to a few familiar situations. Students can comment on the merits of competing explanations, interpretations of data and proposed experimental designs in some very familiar personal, local and global contexts.

1b At Level 1b, students demonstrate a little evidence to use content, procedural and epistemic knowledge to provide explanations, evaluate and design scientific enquiries and interpret data in a few familiar life situations that require a low level of cognitive demand. They are able to identify straightforward patterns in simple sources of data in a few familiar contexts and can offer attempts at describing simple causal relationships. They can identify the independent variable in a given scientific enquiry or in a simple design of their own. They attempt to transform and describe simple data and apply them directly to a few familiar situations.

The proposed level descriptors are based on the 2015 Framework described in this document and offer a qualitative description of the differences between levels of performance. The factors used to determine the demand of items assessing science achievement that have been incorporated into this outline of the proficiency scales include:

- The number and degree of complexity of elements of knowledge demanded by the item.
- The level of familiarity and prior knowledge that students may have of the content, procedural and epistemic knowledge involved.
- The cognitive operation required by the item, e.g. recall, analysis, evaluation.
- The extent to which forming a response is dependent on models or abstract scientific ideas.

## References

American Association for the Advancement of Science (1989), Science for all Americans: a Project 2061 Report on Literacy Goals in Science, Mathematics and Technology, AAS Publishing, Washington, DC, www.project2061.org/publications/sfaa/online/sfaatoc.htm.

Anderson, L.W. and D.R. Krathwohl (2001), A Taxonomy for Learning, Teaching and Assessing: A Revision of Bloom’s Taxonomy of Educational Objectives, Longman Publishing, London.

Bandura, A. (1997), Self-Efficacy: The Exercise of Control, W.H. Freeman and Company, Macmillan Publishers, New York.

Biggs, J. and K. Collis (1982), Evaluating the Quality of Learning: The SOLO Taxonomy, Academic Press, New York.

Bloom, B.S. (eds.) (1956), Taxonomy of Educational Objectives Book 1: Cognitive Domain, Longmans Publishing, London.

Bøe, M.V. et al. (2011), “Participation in science and technology: Young people’s achievement-related choices in late-modern societies”, Studies in Science Education, Vol. 47/1, pp. 37-72, http://dx.doi.org/10.1080/03057267.2011.549621.

Bogner, F. and M. Wiseman (1999), “Toward measuring adolescent environmental perception”, European Psychologist, Vol. 4/3, http://dx.doi.org/10.1027//1016-9040.4.3.139.

Brookhart, S.M. and A.J. Nitko (2011), “Strategies for constructing assessments of higher order thinking skills”, in G. Schraw and D.R. Robinson (eds.), Assessment of Higher Order Thinking Skills, IAP, Charlotte, NC, pp. 327-359.

Confederacion de Sociedades Científicas de España (2011), Informe ENCIENDE, Enseñanza de las Ciencias en la Didáctica Escolar para edades tempranas en España, Madrid.

Davis, S.L. and C.W. Buckendahl (2011), “Incorporating cognitive demand in credentialing examinations”, in G. Schraw and D.R. Robinson (eds), Assessment of Higher Order Thinking Skills, IAP, Charlotte, NC, pp. 327-359.

Drechsel, B., C. Carstensen and M. Prenzel (2011), “The role of content and context in PISA interest scales: A study of the embedded interest items in the PISA 2006 science assessment”, International Journal of Science Education, Vol. 33/1, pp. 73-95.

Duschl, R. (2007), “Science education in three-part harmony: Balancing conceptual, epistemic and social learning goals”, Review of Research in Education, Vol. 32, pp. 268-291, http://dx.doi.org/10.3102/0091732X07309371.

Eagles, P.F.J. and R. Demare (1999), “Factors influencing children’s environmental attitudes”, The Journal of Environmental Education, Vol. 30/4, www.researchgate.net/profile/Paul\_Eagles/publication/271994465\_Factors\_Influencing\_Children's\_Environmental\_Attitudes/links/553e677b0cf20184050f83a6.pdf.

European Commission (1995), “Teaching and learning: Towards the learning society”, White Paper on Education and Training, Office for Official Publications in European Countries, Luxembourg, http://europa.eu/documents/comm/white\_papers/pdf/com95\_590\_en.pdf.

Fensham, P. (1985), “Science for all: A reflective essay”, Journal of Curriculum Studies, Vol. 17/4, pp. 415-435, http://dx.doi.org/10.1080/002207850170407.

Ford, M.J. and B.M. Wargo (2012), “Dialogic framing of scientific content for conceptual and epistemic understanding”, Science Education, Vol. 96/3, pp. 369-391, http://dx.doi.org/10.1002/sce.20482.

Gardner, P.L. (1975), “Attitudes to Science”, Studies in Science Education, Vol. 2, pp. 1-41.

Gott, R., S. Duggan and R. Roberts (2008),” Concepts of evidence”, University of Durham, www.dur.ac.uk/rosalyn.roberts/Evidence/cofev.htm, (accessed 23 September 2012).

Kane, M. (2006), “Validation”, in R.L. Brennan (eds.), Educational Measurement, 4th ed., Praeger Publishers and the American Council on Education, Westport, CT, pp. 17-64.

Klopfer, L.E. (1971), “Evaluation of learning in science” in B.S. Bloom, J.T. Hastings and G.F. Madaus (eds.), Handbook of Formative and Summative Evaluation of Student Learning, McGraw-Hill Book Company, London.

Klopfer, L.E. (1976), “A structure for the affective domain in relation to science education”, Science Education, Vol. 60/3, pp. 299-312, http://dx.doi.org/10.1002/sce.3730600304.

Kuhn, D. (2010), “Teaching and learning science as argument”, Science Education, Vol. 94/5, pp. 810-824, http://dx.doi.org/10.1002/sce.20395.

Lederman, N.G. (2006), “Nature of science: Past, present and future”, in S. Abell and N.G. Lederman (eds.), Handbook of Research on Science Education, Lawrence Erlbaum, Mawah, NJ, pp. 831-879.

Longino, H.E. (1990), Science as Social Knowledge, Princtetown University Press, Princtetown, NJ.

Marzano, R.J. and J.S. Kendall (2007), The New Taxonomy of Educational Objectives, Corwin Press, Thousand Oaks, CA.

Millar, R. (2006), “Twenty first century science: Insights from the design and implementation of a scientific literacy approach in school science”, *International Journal of Science Education* , Vol. 28/13, pp. 1499-1521, http://dx.doi.org/10.1080/09500690600718344.

Millar, R. and J.F. Osborn (eds.) (1998), *Beyond 2000: Science Education for the Future* , School of Education, King’s College, London, www.nuffieldfoundation.org/sites/default/files/Beyond%202000.pdf.

Millar, R. et al. (1995), “Investigating in the school science laboratory: Conceptual and procedural knowledge and their influence on performance”, *Research Papers in Education* , Vol. 9/2, pp. 207-248, http://dx.doi.org/10.1080/0267152940090205.

Mislevy, R.J. and G.D. Haertel (2006), “Implications of evidence-centered design for educational testing”, *Educational Measurement: Issues and Practice* , Vol. 25/4, pp. 6-20.

National Academy of Science (1995), *National Science Education Standards* , National Academy Press, Washington, DC.

National Research Council (2012), *A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas* , Committee on a Conceptual Framework for New K-12 Science Education Standards, Board on Science Education, Division of Behavioral and Social Sciences and Education, Washington, DC.

National Research Council (2000), *Inquiry and the National Science Education Standards* , National Academy Press, Washington DC.

OECD (2012), “What kinds of careers do boys and girls expect for themselves?”, *PISA in focus* , No 14, OECD Publishing, Paris, http://dx.doi.org/10.1787/5k9d417g2933-en.

OECD (2009), *PISA 2006 Technical Report* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264048096-en.

OECD (2007), *PISA 2006: Science Competencies for Tomorrow’s World: Volume 1: Analysis* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264040014-en.

OECD (2006), *Assessing Scientific, Reading and Mathematical Literacy: A Framework for PISA 2006* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264026407-en.

OECD (2004), *The PISA 2003 Assessment Framework: Mathematics, Reading, Science and Problem Solving Knowledge and Skills* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264101739-en.

OECD (2000), *Measuring Student Knowledge and Skills: The PISA 2000 Assessment of Reading, Mathematical and Scientific Literacy* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264181564-en.

OECD (1999), *Measuring Student Knowledge and Skills: A New Framework for Assessment* , OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264173125-en.

Ormerod, M.B. and D. Duckworth (1975), *Pupils’ Attitudes to Science* , National Foundation for Educational Reasearch, Slough, UK.

Osborne, J.F. (2010), “Arguing to learn in science: The role of collaborative, critical discourse”, *Science* , Vol. 328/5977, pp. 463-466, http://dx.doi.org/10.1126/science.1183944.

Osborne, J.F. and J. Dillon (2008), *Science Education in Europe: Critical Reflections* , Nuffield Foundation, London.

Osborne, J.F., S. Simon and S. Collins (2003), “Attitudes towards science: A review of the literature and its implications”, *International Journal of Science Education* , Vol. 25/9, pp. 1049–1079, http://dx.doi.org/10.1080/0950069032000032199.

Rickinson, M. (2001), “Learners and learning in environmental education: A critical review of the evidence”, *Environmental Education Research* , Vol. 7/3, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.454.4637&amp;rep=rep1&amp;type=pdf.

Rychen, D.S. and L. H. Salganik (eds.) (2003), *Definition and Selection of Key Competencies: Executive Summary* , Hogrefe Publishing, Göttingen, Germany.

Schibeci, R.A. (1984), “Attitudes to science: An update”, *Studies in Science Education* , Vol. 11, pp. 26-59.

Sekretariat der Ständigen Konferenz der Kultusminister der Länder in der Bundesrepublik Deutschland (KMK) (2005), Bildungsstandards im Fach Biologie für den Mittleren Schulabschluss (Jahrgangsstufe 10).

Tai, R.H. et al. (2006), “Planning early for careers in science”, *Science* , Vol. 312, pp. 1143-1145.

Taiwan Ministry of Education (1999), Curriculum outlines for “Nature science and living technology”, Ministry of Education, Taipei, Taiwan.

UNEP (2012), *21 Issues for the 21st Century: Result of the UNEP Foresight Process on Emerging Environmental Issues* , United Nations Environment Programme (UNEP), Nairobi, Kenya, www.unep.org/pdf/Foresight\_Report-21\_Issues\_for\_the\_21st\_Century.pdf.

UNESCO (2003), “UNESCO and the international decade of education for sustainable development (2005-2015)”, *UNESCO International Science, Technology and Environmental Education Newsletter* , Vol. XXVIII, No. 1-2, UNESCO, Paris.

UNESCO (2005), “International implementation scheme” United Nations Decade of Education for Sustainable Development (2005-2014), UNESCO, Paris, [www.bibb.de/dokumente/pdf/a33\_unesco\_international\_implementation\_scheme.pdf](http://www.bibb.de/dokumente/pdf/a33_unesco_international_implementation_scheme.pdf) .

Weaver, A. (2002), “Determinants of environmental attitudes: A five-country comparison”, *International Journal of Sociology* , Vol. 32/1.

Webb, N.L. (1997), “Criteria for alignment of expectations and assessments in mathematics and science education”, *Council of Chief State School Officers and National Institute for Science Education Research Monograph* , National Institution for Science Education, Washington, DC.

Wiliam, D. (2010), “What counts as evidence of educational achievement? The role of constructs in the pursuit of equity in assessment”, *Review of Research in Education* , Vol. 34, pp. 254-284.

Ziman, J. (1979), *Reliable Knowledge: An Exploration of the Grounds for Belief in Science* , Cambridge University Press, Cambridge, UK.

## PISA 2015 Reading Framework

This chapter defines “reading literacy” as assessed in the Programme for International Student Assessment (PISA) in 2015 and the competencies required for reading literacy. It describes the cognitive processes (aspects) involved in reading that are assessed, the types of texts and response formats used in the assessment, and how student performance in reading is measured and reported.

In PISA 2015, reading literacy is assessed as a minor domain, providing an opportunity to make comparisons in student performance over time. This framework uses the same description and illustrations of the PISA reading assessment as included in the 2009 framework, when reading was re-examined and updated for use as the major domain in that cycle. The framework does not, however, cover digital reading (also referred to as electronic reading in 2009). This is because the 2009 report provided separate scales for print reading and digital reading. Since reading is a minor domain in PISA 2015, and since digital reading was not assessed in all participating countries in 2009 or in 2012, there are no separate data on digital reading, nor was digital reading included as part of the overall concept of reading literacy.

For PISA 2015, the computer is the primary mode of delivery for all domains, including reading literacy. However, paper-based assessment instruments are provided for countries that choose not to test their students by computer. The reading literacy component of both the computer-based and paper-based instruments is composed of the same clusters of reading trend items. The number of trend items in the minor domains are increased compared with previous PISA assessments, thereby increasing the construct coverage while reducing the number of students responding to each question. This design is intended to reduce potential bias while stabilising and improving the measurement of trends.

With the move to computer-based delivery for 2015, the 2012 text classification “medium: print and digital” is a potential source of confusion. For 2015, the terminology has been updated to “fixed text” and “dynamic text” to distinguish between delivery mode and the space in which the text is displayed (hereafter referred to as “text display space”), regardless of whether it is printed or on screen. It is important to note, however, that the constructs of the 2009 framework remain unchanged.

## 2015 reading literacy terminology

**Mode:** refers only to the delivery channel. The following distinctions are made:

- **Paper-based:** items delivered on paper
- **Computer-based:** items delivered on computer

**Text display space:** In 2009, a broad classification, “medium”, was used to describe the features of print and digital texts. For 2015, the classification remains, but is renamed “text display space”.

- **Fixed text:** what was previously called “print-medium text”. As this type of text is presented on a screen in PISA 2015, the term “print” no longer applies.
- **Dynamic text:** what was previously called “digital-medium text”. As “print-medium” texts are also presented on a screen in PISA 2015, the term “digital” applies to both text display spaces.

**Digital reading:** The term “digital reading assessment” is retained for historical purposes to refer specifically to the 2009/2012 optional domain.

**Note:** This new terminology is intended to be provisional, for use only in 2015 when items previously delivered on paper and classified as “print” are delivered on a screen. The purpose is to make a clearer distinction between the mode of delivery and the features of the classification previously known as “medium”. In 2018, when reading literacy will once again become the major domain, both the framework and these terms will be revisited and updated.

In 2015, only fixed-text items are used in the assessment, and these are delivered primarily in a computer-based mode. This is shown in Table 3.1 below.

### Table 3.1 Relationship between mode and text display space for 2015

| Mode / Text display space   | Fixed text   | Dynamic text                 |
|-----------------------------|--------------|------------------------------|
| Paper-based mode            | ✓            | ✗                            |
| Computer-based mode         | ✓            | ✓ (but not assessed in 2015) |

Reading literacy was the major domain assessed in 2000, for the first PISA cycle and in 2009, for the fourth PISA cycle. For the sixth PISA cycle (PISA 2015), reading is a minor domain and its framework has not changed from the PISA 2009 cycle (OECD, 2010). There were two major modifications to the PISA 2009 version of the reading framework: the incorporation of an assessment of digital reading and the elaboration of the constructs of reading engagement and metacognition. However, reading is a minor domain in PISA 2015. The reading of digital texts is not included and no data on engagement or metacognition in reading are collected.

The PISA framework for assessing the reading literacy of students towards the end of compulsory education, therefore, must focus on skills that include finding, selecting, interpreting and evaluating information from a full range of texts, including those encountered both inside and outside the classroom.

## DEFINING READING LITERACY

Definitions of reading and reading literacy have changed over time in parallel with changes in society, economy and culture. The concept of learning, particularly the concept of lifelong learning, has expanded the perception of reading literacy. Literacy is no longer considered to be an ability acquired only in childhood during the early years of schooling. Instead, it is viewed as an expanding set of knowledge, skills and strategies that individuals build on throughout life in various contexts, through interaction with their peers and the wider community.

Cognitive-based theories of reading literacy emphasise the interactive nature of reading and the constructive nature of comprehension, in the print medium (Binkley and Linnakylä, 1997; Bruner, 1990; Dole et al., 1991) and to an even greater extent in the digital medium (Fastrez, 2001; Legros and Crinon, 2002; Leu, 2007; Reinking, 1994). The reader generates meaning in response to text by using previous knowledge and a range of text and situational cues that are often socially and culturally derived. While constructing meaning, the reader uses various processes, skills and strategies to foster, monitor and maintain understanding. These processes and strategies are expected to vary with context and purpose as readers interact with a variety of continuous and non-continuous texts in the print medium and (typically) with multiple texts in the digital medium.

The PISA 2015 definition of reading literacy, the same as used in PISA 2009, as shown in Box 3.1:

### Box 3.1 The 2015 definition of reading literacy

Reading literacy is understanding, using, reflecting on and engaging with written texts, in order to achieve one’s goals, develop one’s knowledge and potential, and participate in society.

## Reading literacy...

The term “reading literacy” is preferred to “reading” because it is likely to convey to a non-expert audience more precisely what the survey is measuring. “Reading” is often understood as simply decoding, or even reading aloud, whereas the intention of this survey is to measure something broader and deeper. Reading literacy includes a wide range of cognitive competencies, from basic decoding, to knowledge of words, grammar and larger linguistic and textual structures and features, to knowledge about the world.

In this study, “reading literacy” is intended to express the active, purposeful and functional application of reading in a range of situations and for various purposes. According to Holloway (1999), reading skills are essential to the academic achievement of middle- and high school students. PISA assesses a wide range of students. Some will go on to university; some will pursue further studies in preparation for joining the labour force; some will enter the workforce directly after completing compulsory education. Achievement in reading literacy is not only a foundation for achievement in other subject areas within the education system, but also a prerequisite for successful participation in most areas of adult life (Cunningham and Stanovich, 1998; Smith et al., 2000). Indeed, regardless of their academic or labour-force aspirations, students’ reading literacy is important for their active participation in their community and economic and personal life.

Reading literacy skills matter not just for individuals, but for economies as a whole. Policy makers and others are coming to recognise that in modern societies, human capital – the sum of what the individuals in an economy know and can do – may be the most important form of capital. Economists have for many years developed models showing generally that a country’s education levels are a predictor of its economic growth potential (Coulombe et al., 2004).

### ...is understanding, using, reflecting on...

The word “understanding” is readily connected with “reading comprehension”, a well-accepted element of reading. The word “using” refers to the notions of application and function – doing something with what we read. “Reflecting on” is added to “understanding” and “using” to emphasise the notion that reading is interactive: readers draw on their own thoughts and experiences when engaging with a text. Of course, every act of reading requires some reflection, drawing on information from outside the text. Even at the earliest stages, readers draw on symbolic knowledge to decode a text

and require a knowledge of vocabulary to construct meaning. As readers develop their stores of information, experience and beliefs, they constantly, often unconsciously, test what they read against outside knowledge, thereby continually reviewing and revising their sense of the text.

### ...and engaging with...

A reading literate person not only has the skills and knowledge to read well, but also values and uses reading for a variety of purposes. It is therefore a goal of education to cultivate not only proficiency but also engagement in reading. Engagement in this context implies the motivation to read and comprises a cluster of affective and behavioural characteristics that include an interest in and enjoyment of reading, a sense of control over what one reads, involvement in the social dimension of reading, and diverse and frequent reading practices.

### ...written texts...

The term “written texts” is meant to include all those coherent texts in which language is used in its graphic form, whether printed and digital. Instead of the word “information”, which is used in some other definitions of reading, the term “texts” was chosen because of its association with written language and because it more readily connotes literary as well as information-focused reading.

These texts do not include aural language artefacts, such as voice recordings; nor do they include film, TV, animated visuals or pictures without words. They do include visual displays, such as diagrams, pictures, maps, tables, graphs and comic strips that include some written language (for example, captions). These visual texts can exist either independently or they can be embedded in larger texts. Digital texts are distinguished from printed texts in a number of respects, including physical readability; the amount of text visible to the reader at any one time; the way different parts of a text and different texts are connected with one another through hypertext links; and, given these text characteristics, the way that readers typically engage with digital texts. To a much greater extent than with printed or hand-written texts, readers need to construct their own pathways to complete any reading activity associated with a digital text.

### ...in order to achieve one’s goals, develop one’s knowledge and potential, and participate in society.

This phrase is meant to capture the full scope of situations in which reading literacy plays a role, from private to public, from school to work, from formal education to lifelong learning and active citizenship. “To achieve one’s goals and to develop one’s knowledge and potential” spells out the idea that reading literacy enables the fulfilment of individual aspirations – both defined ones, such as graduating or getting a job, and those less defined and less immediate that enrich and extend personal life and lifelong education. The word “participate” is used because it implies that reading literacy allows people to contribute to society as well as to meet their own needs. “Participating” includes social, cultural and political engagement.

## ORGANISING THE DOMAIN OF READING

This section describes how the domain is represented, a vital issue because the organisation and representation of the domain determines the test design and, ultimately, the evidence about student proficiencies that can be collected and reported.

Reading is a multidimensional domain. While many elements are part of the construct, not all can be taken into account in building the PISA assessment. Only those considered most important were selected.

The PISA reading literacy assessment is built on three major task characteristics to ensure a broad coverage of the domain:

- **situation** , which refers to the range of broad contexts or purposes for which reading takes place
- **text** , which refers to the range of material that is read
- **aspect** , which refers to the cognitive approach that determines how readers engage with a text.

In PISA, features of the text and aspect variables (but not of the situation variable) are also manipulated to influence the difficulty of a task.

Reading is a complex activity. The elements of reading do not exist independently of one another in neat compartments. The assignment of texts and tasks to framework categories does not imply that the categories are strictly partitioned or that the materials exist in atomised cells determined by a theoretical structure. The framework scheme is provided to ensure coverage, to guide the development of the assessment and to set parameters for reporting, based on what are considered the marked features of each task.

Examples of reading items are available in the *PISA 2012 Assessment and Analytical Framework* (OECD, 2013) and on the PISA website ( [www.oecd.org/pisa/](http://www.oecd.org/pisa/) ).

### Situation

The PISA situation variables were adapted from the Common European Framework of Reference (CEFR) developed for the Council of Europe (Council of Europe, 1996). The four situation variables – personal, public, educational and occupational – are described in the following paragraphs.

The *personal* situation relates to texts that are intended to satisfy an individual’s personal interests, both practical and intellectual. This category also includes texts that are intended to maintain or develop personal connections with other people. It includes personal letters, fiction, biography, and informational texts that are intended to be read to satisfy curiosity, as a part of leisure or recreational activities. In the digital medium it includes personal e-mails, instant messages and diary-style blogs.

The *public* category describes the reading of texts that relate to activities and concerns of the larger society. The category includes official documents and information about public events. In general, the texts associated with this category assume a more or less anonymous contact with others; they also therefore include forum-style blogs, news websites and public notices that are encountered both on line and in print.

The content of *educational* texts is usually designed specifically for the purpose of instruction. Printed text books and interactive learning software are typical examples of material generated for this kind of reading. Educational reading normally involves acquiring information as part of a larger learning task. The materials are often not chosen by the reader, but instead assigned by an instructor. The model tasks are those usually identified as “reading to learn” (Sticht, 1975; Stiggins, 1982).

Many 15-year-olds will move from school into the labour force within one to two years. A typical *occupational* reading task is one that involves the accomplishment of some immediate task. It might include searching for a job, either in a print newspaper’s classified advertisement section, or on line; or following workplace directions. The model tasks of this type are often referred to as “reading to do” (Sticht, 1975; Stiggins, 1982).

*Situations* is used in PISA reading literacy to define texts and their associated tasks, and refers to the contexts and uses for which the author constructed the text. The manner in which the situation variable is specified is therefore about supposed audience and purpose, and is not simply based on the place where the reading activity is carried out. Many texts used in classrooms are not specifically designed for classroom use. For example, a piece of literary text may typically be read by a 15-year-old in a mother-tongue language or literature class, yet the text was written (presumably) for readers’ personal enjoyment and appreciation. Given its original purpose, such a text is classified as *personal* in PISA. As Hubbard (1989) has shown, some kinds of reading usually associated with out-of-school settings for children, such as rules for clubs and records of games, often take place unofficially at school as well. These texts are classified as *public* in PISA. Conversely, textbooks are read both in schools and in homes, and the process and purpose probably differ little from one setting to another. Such texts are classified as *educational* in PISA.

The four categories overlap. In practice, for example, a text may be intended both to delight and to instruct (personal and educational); or to provide professional advice that is also general information (occupational and public). While content is not a variable that is specifically manipulated in this study, by sampling texts across a variety of situations the intent is to maximise the diversity of content that is included in the PISA reading literacy survey.

Table 3.2 shows the desired distribution of items by situation for reading tasks.

| Situation    |   Percentage of total items |
|--------------|-----------------------------|
| Personal     |                          30 |
| Educational  |                          25 |
| Occupational |                          15 |
| Public       |                          30 |
| Total        |                         100 |

## Text

Reading requires material for the reader to read. In an assessment, that material – a text (or a set of texts) related to a particular task – must be coherent within itself. That is, the text must be able to stand alone without requiring additional material to make sense to the proficient reader. While it is obvious that there are many different kinds of texts and that any assessment should include a broad range, it is not so obvious that there is an ideal categorisation of kinds of texts.

### PISA 2009 and PISA 2012

In PISA 2009 and PISA 2012, the addition of digital reading to the framework made this issue still more complex. There were four main text classifications, because of the print and digital reading assessments proposed in these surveys:

- Medium: print and digital
- Environment: authored, message-based and mixed (only applicable to digital medium)
- Text format: continuous, non-continuous, mixed and multiple
- Text type: description, narration, exposition, argumentation, instruction and transaction

### PISA 2015

As explained above, in PISA 2015 only the items used previously for the “print reading assessment” are delivered on computer or paper, and there are only two text classifications:

- Text format
- Text type

**Text display space** is a third classification of text with two categories, fixed texts and dynamic text. It is not used in PISA 2015 but will be integrated into the PISA 2018 survey.

In PISA 2015, the term ‘text display space’ is used to describe the features of the space – fixed or dynamic – and not the mode in which the text is presented.

**Fixed texts** usually appear on paper in forms such as single sheets, brochures, magazines and books, but tend to appear more and more on a screen as PDFs and on e-readers. This development results in further blurring the distinction between what was labelled “print reading” and “digital reading” in the PISA 2009 framework. As PISA 2015 uses only what was labelled “print reading” in 2009 there are no conceptual change in this aspect for PISA 2015. The physical status of the fixed text encourages (though it may not compel) the reader to approach the content of the text in a particular sequence. In essence, such texts have a fixed or static existence. In real life and in the assessment context, the extent or amount of the text is immediately visible to the reader.

When moving the fixed-text “print” reading trend items from paper to computer-based delivery in the 2015 assessment, care needed to be taken to use navigation tools typical of dynamic texts sparingly and only the most obvious among them. Effects of presenting the original paper-based items on the computer were examined during the mode-effect study in the field trial.

**Dynamic texts** only appear on a screen. Dynamic text is synonymous with hypertext: a text or texts with navigation tools and features that make possible and indeed even require non-sequential reading. Each reader constructs a “customised” text from the information encountered at the links he or she follows. In essence, such texts have an unfixed, dynamic existence. In dynamic texts, typically only a fraction of the available text can be seen at any one time, and often the extent of text available is unknown. No dynamic texts are included in PISA 2015.

The **environment** classification was a new variable for the PISA 2009 reading framework. Since it applies only to dynamic texts it is not discussed in the 2015 PISA framework.

### Text format

An important classification of texts is the distinction between continuous and non-continuous texts.

Texts in **continuous** and **non-continuous** format appear in both the print and digital media. **Mixed** and **multiple** format texts are also prevalent in both media, particularly so in the digital medium. Each of these four formats is elaborated as follow:

**Continuous** texts are formed by sentences organised into paragraphs. These may fit into even larger structures, such as sections, chapters, and books (e.g. newspaper reports, essays, novels, short stories, reviews and letters including on e-book readers).

Non-continuous are most frequently organised in matrix format, composed of a number of lists (Kirsch and Mosenthal, 1990) (e.g. lists, tables, graphs, diagrams, advertisements, schedules, catalogues, indexes and forms). They thus require a different approach to reading than continuous texts do.

Many texts are single, coherent artefacts consisting of a set of elements in both a continuous and non-continuous format. In well-constructed mixed texts, the constituents (e.g. a prose explanation, along with a graph or table) are mutually supportive, with coherence and cohesion links throughout. Mixed text in the print medium is a common format in magazines, reference books and reports. In the digital medium, authored web pages are typically mixed texts, with combinations of lists, paragraphs of prose, and often graphics. Message-based texts, such as online forms, e-mail messages and forums, also combine texts that are continuous and non-continuous in format.

Multiple texts are defined as those that have been generated independently, and make sense independently; they are juxtaposed for a particular occasion or may be loosely linked together for the purposes of the assessment. The relationship between the texts may not be obvious; they may be complementary or may contradict one another. For example, a set of websites from different companies providing travel advice may or may not provide similar directions to tourists. Multiple texts may have a single “pure” format (for example, continuous), or may include both continuous and non-continuous texts.

Table 3.3 shows the desired distribution of items by text format.

| Text format    |   Percentage of total items |
|----------------|-----------------------------|
| Continuous     |                          60 |
| Non-continuous |                          30 |
| Mixed          |                           5 |
| Multiple       |                           5 |
| Total          |                         100 |

**Text type**

A different categorisation of text is by text type: description, narration, exposition, argumentation, instruction and transaction.

Texts, as they are found in the world, typically resist categorisation; they are usually not written with rules in mind, and tend to cut across categories. In order to ensure that the reading instrument represents different types of reading, PISA categorises texts based on their predominant characteristics.

The following classification of texts used in PISA is adapted from the work of Werlich (1976).

Description is the type of text in which the information refers to properties of objects in space. The typical questions that descriptive texts answer are “what” questions (e.g. a depiction of a particular place in a travelogue or diary, a catalogue, a geographical map, an online flight schedule or a description of a feature, function or process in a technical manual).

Narration is the type of text in which the information refers to properties of objects in time. Narration typically answers questions relating to “when”, or “in what sequence”. “Why characters in stories behave as they do” is another question that narration typically answers (e.g. a novel, a short story, a play, a biography, a comic strip, fictional texts and a newspaper report of an event).

Exposition is the type of text in which the information is presented as composite concepts or mental constructs, or those elements into which concepts or mental constructs can be analysed. The text provides an explanation of how the different elements interrelate in a meaningful whole, and often answers questions about “how” (e.g. a scholarly essay, a diagram showing a model of memory, a graph of population trends, a concept map and an entry in an online encyclopaedia).

Argumentation is the type of text that presents the relationship among concepts or propositions. Argument texts often answer “why” questions. An important sub-classification of argument texts is persuasive and opinionative texts, referring to opinions and points of view. Examples of text in the text type category argumentation are a letter to the editor, a poster advertisement, the posts in an online forum and a web-based review of a book or film.

## Instruction

Instruction is the type of text that provides directions on what to do. The text presents directions for certain behaviours in order to complete a task (e.g., a recipe, a series of diagrams showing a procedure for giving first aid, and guidelines for operating digital software).

## Transaction

Transaction is the kind of text that aims to achieve a specific purpose outlined in the text, such as requesting that something is done, organising a meeting or making a social engagement with a friend. Before the spread of digital communication, this kind of text was a significant component of some kinds of letters and, as an oral exchange, the principal purpose of many phone calls. This text type was not included in Werlich's (1976) categorisation. It was used for the first time in the PISA 2009 framework because of its prevalence in the digital medium (e.g., everyday e-mail and text message exchanges between colleagues or friends that request and confirm arrangements).

## Aspect

Whereas navigation tools and features are the visible or physical features that allow readers to negotiate their way into, around and between texts, aspects are the mental strategies, approaches or purposes that readers use to negotiate their way into, around and between texts.

Five aspects guide the development of the reading literacy assessment tasks:

- retrieving information
- forming a broad understanding
- developing an interpretation
- reflecting on and evaluating the content of a text
- reflecting on and evaluating the form of a text.

As it is not possible to include sufficient items in the PISA assessment to report on each of the five aspects as a separate subscale, these five aspects are organised into three broad aspect categories for reporting on reading literacy:

- access and retrieve
- integrate and interpret
- reflect and evaluate.

Retrieving information tasks, which focus the reader on separate pieces of information within the text, are assigned to the access and retrieve scale.

Forming a broad understanding and developing an interpretation tasks focus the reader on relationships within a text. Tasks that focus on the whole text require readers to form a broad understanding; tasks that focus on relationships between parts of the text require developing an interpretation. The two are grouped together under integrate and interpret.

Tasks addressing the last two aspects, reflecting on and evaluating the content of a text and reflecting on and evaluating the form of a text, are grouped together into a single reflect and evaluate aspect category. Both require the reader to draw primarily on knowledge outside the text and relate it to what is being read. Reflecting on and evaluating content tasks are concerned with the notional substance of a text; reflecting on and evaluating form tasks are concerned with its structure or formal features.

### Figure 3.1 - Relationship between the reading framework and the aspect subscales

READING LITERACY

Figure 3.1 shows the relationship between the five aspects targeted in the test development and the three broad reporting aspects.

An elaboration of the three broad aspect categories is given below.

### Access and retrieve

Accessing and retrieving involves going to the information space provided and navigating in that space to locate and retrieve one or more distinct pieces of information. Access and retrieve tasks can range from locating the details required by an employer from a job advertisement, to finding a telephone number with several prefix codes, to finding a particular fact to support or disprove a claim someone has made.

While *retrieving* describes the process of selecting the required information, *accessing* describes the process of getting to the place, the information space, where the required information is located. Some items may require retrieving information only, especially in fixed texts where the information is immediately visible and where the reader only has to select what is appropriate in a clearly specified information space. On the other hand, some items in the dynamic space require little more than accessing (for example, clicking to select an item in a list of search results). However, only the former processes are involved in the *access and retrieve* tasks in PISA 2015 as the digital reading assessment is not offered. Such *access and retrieve* items in the fixed-text display space might require readers to use navigation features, such as headings or captions, to find their way to the appropriate section of the text before locating the relevant information. The process of accessing and retrieving information involves skills associated with selecting, collecting and retrieving information.

### Integrate and interpret

Integrating and interpreting involves processing what is read to make internal sense of a text.

*Integrating* focuses on demonstrating an understanding of the coherence of the text. *Integrating* involves connecting various pieces of information to make meaning, whether it be identifying similarities and differences, making comparisons of degree, or understanding cause-and-effect relationships.

*Interpreting* refers to the process of making meaning from something that is not stated. When interpreting, a reader is identifying the underlying assumptions or implications of part or all of the text.

Both *integrating* and *interpreting* are required to *form a broad understanding* . A reader must consider the text as a whole or in a broad perspective. Students may demonstrate initial understanding by identifying the main topic or message or by identifying the general purpose or use of the text.

Both *integrating* and *interpreting* are also involved in *developing an interpretation* , which requires readers to extend their initial broad impressions so that they develop a deeper, more specific or more complete understanding of what they have read. *Integrating* tasks include identifying and listing supporting evidence, and comparing and contrasting information in which the requirement is to draw together two or more pieces of information from the text. In order to process either explicit or implicit information from one or more sources in such tasks, the reader must often infer an intended relationship or category. *Interpreting* tasks may involve drawing an inference from a local context, for example, interpreting the meaning of a word or phrase that gives a particular nuance to the text. This process of comprehension is also assessed in tasks that require the student to make inferences about the author's intention, and to identify the evidence used to infer that intention.

The relationship between the processes of integration and interpretation may therefore be seen as intimate and interactive. Integrating involves first inferring a relationship within the text (a kind of interpretation), and then bringing pieces of information together, therefore allowing an interpretation to be made that forms a new integrated whole.

### Reflect and evaluate

Reflecting and evaluating involves drawing upon knowledge, ideas or attitudes beyond the text in order to relate the information provided within the text to one’s own conceptual and experiential frames of reference.

*Reflect* items may be thought of as those that require readers to consult their own experience or knowledge to compare, contrast or hypothesise. *Evaluate* items are those that ask readers to make a judgement drawing on standards beyond the text.

Reflecting on and evaluating the content of a text requires the reader to connect information in a text to knowledge from outside sources. Readers must also assess the claims made in the text against their own knowledge of the world.

## Reflecting on and Evaluating the Form of a Text

Often readers are asked to articulate and defend their own points of view. To do so, readers must be able to develop an understanding of what is said and intended in a text. They must then test that mental representation against what they know and believe on the basis of either prior information, or information found in other texts. Readers must call on supporting evidence from within the text and contrast it with other sources of information, using both general and specific knowledge as well as the ability to reason abstractly.

Reflecting on and evaluating the form of a text requires readers to stand apart from the text, to consider it objectively and to evaluate its quality and appropriateness. Implicit knowledge of text structure, the style typical of different kinds of texts and register play an important role in these tasks. Evaluating how successful an author is in portraying some characteristic or persuading a reader depends not only on substantive knowledge but also on the ability to detect subtleties in language.

Some examples of assessment tasks characteristic of reflecting on and evaluating the form of a text include determining the usefulness of a particular text for a specified purpose and evaluating an author's use of particular textual features in accomplishing a particular goal. The student may also be called upon to describe or comment on the author's use of style and to identify the author's purpose and attitude. To some extent, every critical judgement requires the reader to consult his or her own experience; some kinds of reflection, on the other hand, do not require evaluation (for example, comparing personal experience with something described in a text). Thus evaluation might be seen as a subset of reflection.

### Inter-relation and Interdependence of the Three Aspects

The three broad aspects defined for PISA reading literacy are not conceived of as entirely separate and independent, but rather as interrelated and interdependent. Indeed, from a cognitive-processing perspective, they can be considered semi-hierarchical: it is not possible to interpret or integrate information without having first retrieved it; and it is not possible to reflect on or evaluate information without having made some sort of interpretation. In PISA, however, the framework description of reading aspects distinguishes approaches to reading that are demanded for different contexts and purposes; these are then reflected in assessment tasks that emphasise one or other aspect. Table 3.4 shows the desired distribution of items by aspect.

#### Table 3.4 Desired distribution of reading items, by aspect

| Aspect                  |   Percentage of total items |
|-------------------------|-----------------------------|
| Access and retrieve     |                          25 |
| Integrate and interpret |                          50 |
| Reflect and evaluate    |                          25 |
| Total                   |                         100 |

## Assessing Reading Literacy

The previous section outlined the conceptual framework for reading literacy. The concepts in the framework must, in turn, be represented in tasks and questions in order to collect evidence of students' proficiency in reading literacy.

The distribution of tasks across the major framework variables of situation, text and aspect was discussed in the previous section. In this section some of the other major issues in constructing and operationalising the assessment are considered: factors affecting item difficulty, and how difficulty can be manipulated; the choice of response formats; and some issues around coding and scoring. Considerations of moving the fixed-text "print-medium" trend items to computer-based delivery in 2015 are also discussed further in this section.

### Factors Affecting Item Difficulty

The difficulty of any reading literacy task depends on an interaction among several variables. Drawing on Kirsch and Mosenthal's work (see, for example, Kirsch, 2001; Kirsch and Mosenthal, 1990), we can manipulate the difficulty of items by applying knowledge of the following aspect and text format variables.

In access and retrieve tasks, difficulty depends on the number of pieces of information that the reader needs to locate, the amount of inference required, the amount and prominence of competing information, and the length and complexity of the text.

In integrate and interpret tasks, difficulty is affected by the type of interpretation required (for example, making a comparison is easier than finding a contrast); the number of pieces of information to be considered; the degree and prominence of competing information in the text; and by the nature of the text: the less familiar and the more abstract the content, and the longer and more complex the text, the more difficult the task is likely to be.

In reflect and evaluate tasks, difficulty is affected by the type of reflection or evaluation required (from least to most difficult, the types of reflection are: connecting; explaining and comparing; hypothesising and evaluating); the nature of the knowledge that the reader needs to bring to the text (a task is more difficult if the reader needs to draw on narrow, specialised knowledge rather than broad and common knowledge); the relative abstraction and length of the text; and by the depth of understanding of the text required to complete the task.

In tasks relating to continuous texts, difficulty is influenced by the length of the text, the explicitness and transparency of its structure, how clearly the parts are related to the general theme, and whether there are text features, such as paragraphs or headings, and discourse markers, such as sequencing words.

In tasks relating to non-continuous texts, difficulty is influenced by the amount of information in the text; the list structure (simple lists are easier to negotiate than more complex lists); whether the components are ordered and explicitly organised, for example with labels or special formatting; and whether the information required is in the body of the text or in a separate part, such as a footnote.

## Response formats

The form in which the evidence is collected – the response format – varies according to what is considered appropriate given the kind of evidence that is being collected, and also according to the pragmatic constraints of a large-scale assessment. As in any large-scale assessments the range of feasible item formats is limited, with multiple-choice (simple and complex) and constructed response items (where students write their own answer) being the most manageable formats.

Students in different countries are more or less familiar with various response formats. Including items in a variety of formats is likely to provide some balance between more and less familiar formats for all students, regardless of nationality.

To ensure proper coverage of the ability ranges in different countries, to ensure fairness given the inter-country and gender differences observed, and to ensure a valid assessment of the reflect and evaluate aspect, both multiple choice and open constructed response items continue to be used in PISA reading literacy assessments regardless of the change in delivery mode. Any major change in the distribution of item types in print reading might also impact on the measurement of trends.

Table 3.5 shows target coding requirements for PISA reading tasks. The distribution is shown in relation to the three aspects of reading literacy assessment. Items that require expert judgement consist of open constructed responses. Items that do not require coder judgement consist of simple multiple-choice, complex-multiple choice and closed constructed-response items. The closed constructed response items are those that require the student to generate a response, but require minimal judgement on the part of a coder. For example, a task in which a student is asked to copy a single word from the text, where only one word is acceptable, would be classified as a closed constructed response item. Such items impose a minor cost burden in operational terms and therefore from a pragmatic perspective, these closed constructed response items can be grouped with multiple choice items.

### Table 3.5 Approximate distribution of tasks, by coding requirement for PISA 2015

| Aspect                  |   % of tasks requiring expert judgement in coding |   % of tasks not requiring expert judgement in coding |   % of test |
|-------------------------|---------------------------------------------------|-------------------------------------------------------|-------------|
| Access and retrieve     |                                                11 |                                                    14 |          25 |
| Integrate and interpret |                                                14 |                                                    36 |          50 |
| Reflect and evaluate    |                                                18 |                                                     7 |          25 |
| Total                   |                                                43 |                                                    57 |         100 |

Table 3.5 indicates that while there is some distribution of items that require coder judgement and those that do not across the aspects, they are not distributed evenly. The reflection and evaluation aspect tasks are assessed through a larger percentage of constructed response items, which require expert coder judgement.

Given that the delivery of the 2015 assessment is computer-based, it may be possible to use computer coding for some responses not requiring expert judgement without affecting the construct or attributes of the items.

## Coding and scoring

Codes are applied to test items, either by a more or less automated process of capturing the alternative chosen by the student for a multiple-choice answer, or by a human judge (expert coder) selecting a code that best captures the kind of response given by a student to an item that requires a constructed response. The code is then converted to a score for the item.

## Transition from paper-based to computer-based delivery

The main mode of delivery for the previous PISA assessments was paper. In moving to computer-based delivery in 2015, care must be taken to maintain comparability between the assessments. Some of the factors considered when transposing items from paper to computer mode are discussed below.

- **Item types:** The computer provides a range of opportunities for designers of test items, including new item formats (e.g., drag-and-drop, hotspots). Since the purpose of the 2015 assessment is to study trends, there is less opportunity to exploit innovative item types. The majority of response formats remains unchanged in 2015, although some drop-down or hotspot items may be used to enable computer coding of items that were previously scored by experts, but only where no expert judgement is required and the item construct is not affected.
- **Stimulus presentation:** A feature of fixed texts defined in the construct is that “the extent or amount of the text is immediately visible to the reader”. Clearly, it is impossible, both on paper and on a screen, to have long texts displayed on a single page or screen. To allow for this and still satisfy the construct of fixed texts, pagination is used for texts rather than scrolling. Texts that cover more than one page are presented in their entirety before the student sees the first question.
- **IT skills:** Just as paper-based assessments rely on a set of fundamental skills for working with printed materials, so computer-based assessments rely on a set of fundamental skills for using computers. These include knowledge of basic hardware (e.g., keyboard and mouse) and basic conventions (e.g., arrows to move forward and specific buttons to press to execute commands). The intention is to keep such skills to a minimal core level.

There is research evidence that a computer-based testing environment can influence students’ performance in reading. Some early studies indicated that reading speed was slower in a computer-based environment (Dillon, 1994) and less accurate (Muter et al., 1982), although these studies were conducted on proofreading tasks, not in an assessment situation.

There is a large body of more recent literature on paper- and computer-based tests’ equivalency (see e.g., Macedo-Rouet et al., 2009; Paek, 2005); however these still reveal conflicting findings. A meta-analysis of studies looking at K-12 students’ mathematics and reading achievement (Wang et al, 2008) indicated that, overall, administration mode has no statistically significant effect on scores.

A mode-effects study was conducted as part of the OECD Programme for the International Assessment of Adult Competencies (PIAAC) field trial. In this study, adults were randomly assigned to either a computer-based or paper-based assessment of literacy and numeracy skills. The majority of the items used in the paper-delivery mode was adapted for computer delivery and used in this study. Analyses of these data revealed that almost all of the item parameters were stable across the two modes, thus showing that responses could be measured along the same literacy and numeracy scales. This study, along with the results, was written up as part of the *Technical Report of the Survey of Adult Skills* (OECD, 2014). Given this evidence, it was hypothesised that 2009 reading items could be transposed onto a screen without affecting trend data. (The PISA 2015 field trial studied the effect on student performance of the change in mode of delivery. For further details see Box 1.2.)

## Reporting proficiency in reading

PISA reports results in terms of proficiency scales that are interpretable for the purposes of policy. In PISA 2015, reading is a minor domain, and fewer reading items are administered to participating students. A single reading literacy scale is reported based upon the overall combined scale for reading.

To capture the progression of complexity and difficulty in PISA 2015, this reading literacy scale is based on the PISA 2009 combined print reading literacy scale and is divided into seven levels. Figure 3.2 describes these seven levels of reading proficiency. Level 6 is the highest described level of proficiency (Level 5 was the highest level before PISA 2009). The bottom level of measured proficiency is Level 1b (for the PISA 2009 and all subsequent PISA reading assessments, Level 1 was re-labelled as Level 1a and a new level was added, Level 1b, that describes students who would previously have been rated as “below Level 1”). These different levels of proficiency allow countries to know more about the kinds of tasks students with very high and very low reading proficiency are capable of performing. Levels 2, 3, 4 and 5 remain the same in PISA 2015 as in PISA 2000.

## Summary description of the seven levels of reading proficiency in PISA 2015

| Level   |   Lower score limit | Characteristics of tasks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|---------|---------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 6       |                 698 | Tasks at this level typically require the reader to make multiple inferences, comparisons and contrasts that are both detailed and precise. They require demonstration of a full and detailed understanding of one or more texts and may involve integrating information from more than one text. Tasks may require the reader to deal with unfamiliar ideas, in the presence of prominent competing information, and to generate abstract categories for interpretations. Reflect and evaluate tasks may require the reader to hypothesise about or critically evaluate a complex text on an unfamiliar topic, taking into account multiple criteria or perspectives, and applying sophisticated understandings from beyond the text. A salient condition for access and retrieve tasks at this level is precision of analysis and fine attention to detail that is inconspicuous in the texts.                                                                                                                                                                                 |
| 5       |                 626 | Tasks at this level that involve retrieving information require the reader to locate and organise several pieces of deeply embedded information, inferring which information in the text is relevant. Reflective tasks require critical evaluation or hypothesis, drawing on specialised knowledge. Both interpretative and reflective tasks require a full and detailed understanding of a text whose content or form is unfamiliar. For all aspects of reading, tasks at this level typically involve dealing with concepts that are contrary to expectations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| 4       |                 553 | Tasks at this level that involve retrieving information require the reader to locate and organise several pieces of embedded information. Some tasks at this level require interpreting the meaning of nuances of language in a section of text by taking into account the text as a whole. Other interpretative tasks require understanding and applying categories in an unfamiliar context. Reflective tasks at this level require readers to use formal or public knowledge to hypothesise about or critically evaluate a text. Readers must demonstrate an accurate understanding of long or complex texts whose content or form may be unfamiliar.                                                                                                                                                                                                                                                                                                                                                                                                                         |
| 3       |                 480 | Tasks at this level require the reader to locate, and in some cases recognise the relationship between, several pieces of information that must meet multiple conditions. Interpretative tasks at this level require the reader to integrate several parts of a text in order to identify a main idea, understand a relationship or construe the meaning of a word or phrase. They need to take into account many features in comparing, contrasting or categorising. Often the required information is not prominent or there is much competing information; or there are other text obstacles, such as ideas that are contrary to expectation or negatively worded. Reflective tasks at this level may require connections, comparisons and explanations, or they may require the reader to evaluate a feature of the text. Some reflective tasks require readers to demonstrate a fine understanding of the text in relation to familiar, everyday knowledge. Other tasks do not require detailed text comprehension but require the reader to draw on less common knowledge. |
| 2       |                 407 | Some tasks at this level require the reader to locate one or more pieces of information, which may need to be inferred and may need to meet several conditions. Others require recognising the main idea in a text, understanding relationships, or construing meaning within a limited part of the text when the information is not prominent and the reader must make low level inferences. Tasks at this level may involve comparisons or contrasts based on a single feature in the text. Typical reflective tasks at this level require readers to make a comparison or several connections between the text and outside knowledge, by drawing on personal experience and attitudes.                                                                                                                                                                                                                                                                                                                                                                                        |
| 1a      |                 335 | Tasks at this level require the reader to locate one or more independent pieces of explicitly stated information; to recognise the main theme or author’s purpose in a text about a familiar topic, or to make a simple connection between information in the text and common, everyday knowledge. Typically the required information in the text is prominent and there is little, if any, competing information. The reader is explicitly directed to consider relevant factors in the task and in the text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 1b      |                 262 | Tasks at this level require the reader to locate a single piece of explicitly stated information in a prominent position in a short, syntactically simple text with a familiar context and text type, such as a narrative or a simple list. The text typically provides support to the reader, such as repetition of information, pictures or familiar symbols. There is minimal competing information. In tasks requiring interpretation the reader may need to make simple connections between adjacent pieces of information.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |

## Note

1. This does not preclude the use of several texts in a single task, but each of the texts should be coherent in itself.

## References

- Binkley, M. and P. Linnakylä (1997), “Teaching reading in the United States and Finland”, in M. Binkley, K. Rust and T. Williams (eds.), Reading Literacy in an International Perspective, US Department of Education, Washington, DC.
- Bruner, J. (1990), Acts of meaning, Harvard University Press, Cambridge, MA.
- Coulombe, S., J.F. Tremblay and S. Marchand (2004), “Literacy scores, human capital, and growth across fourteen OECD countries”, International Adult Literacy Survey, Statistics Canada, Ottawa.
- Council of Europe (1996), Modern Languages: Learning, Teaching, Assessment: A Common European Framework of Reference, CC LANG, Vol. 95/5, Rev. IV, Council of Europe, Strasbourg.
- Cunningham, A.E. and K.E. Stanovich (1998), “Early reading acquisition and its relation to reading experience and ability 10 years later”, Developmental Psychology, Vol. 33, pp. 934-945.
- Dillon, A. (1994), Designing Usable Electronic Text: Ergonomic Aspects of Human Information Usage, Taylor and Francis, London.
- Dole, J.G. et al. (1991), “Moving from the old to the new: Research on reading comprehension instruction”, Review of Educational Research, Vol. 16/2, pp. 239-264.
- Fastrez, P. (2001), “Characteristic(s) of hypermedia and how they relate to knowledge”, Education Media International, Vol. 38/2-3, pp. 101-110.
- Halpern, D.F. (1989), Thought and Knowledge: An Introduction to Critical Thinking, Lawrence Erlbaum Associates, Hillsdale, NJ.
- Holloway, J.H. (1999), “Improving the reading skills of adolescents”, Educational Leadership, Vol. 57/2, pp. 80-82.
- Hubbard, R. (1989), “Notes from the underground: Unofficial literacy in one sixth grade”, Anthropology and Education Quarterly, Vol. 20, pp. 291-307.
- Kirsch, I. (2001), The International Adult Literacy Survey: Understanding What Was Measured, Educational Testing Service, Princeton, NJ.
- Kirsch, I. and P.B. Mosenthal (1990), “Exploring document literacy: Variables underlying the performance of young adults”, Reading Research Quarterly, Vol. 25/1, pp. 5-30.
- Legros, D. and J. Crinon (eds.) (2002), Psychologie des Apprentissages et Multimédia, Armand Colin, Paris.
- Leu, D. (2007), “Expanding the Reading Literacy Framework of PISA 2009 to include online reading comprehension”, unpublished document.
- Macedo-Rouet, M. et al. (2009), “Students’ performance and satisfaction with web vs. paper-based practice quizzes and lecture notes”, Computers and Education, Vol. 53, pp. 375-384.
- Muter, P. et al. (1982), “Extended reading of continuous text on television screens”, Human Factors, Vol. 24, pp. 501-508.
- OECD (2014), Technical Report of the Survey of Adult Skills (PIAAC), pre-publication, OECD, Paris, www.oecd.org/site/piaac/Technical%20Report\_17OCT13.pdf.
- OCDE (2013), PISA 2012 Assessment and Analytical Framework: Mathematics, Reading, Science, Problem Solving and Financial Literacy, PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264190511-en.
- OECD (2010), PISA 2009 Assessment Framework: Key Competencies in Reading, Mathematics and Science, PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264062658-en.
- Paek, P. (2005), Recent Trends in Comparability Studies: Pearson Educational Measurement, Pearson Educational Measurement, http://images.pearsonassessments.com/images/tmrs/tmrs\_rg/TrendsCompStudies.pdf (accessed 21 November 2007).
- Pew Internet and American Life Project (2005), “Internet: The mainstreaming of online life”, Trends 2005, Pew Research Center, Washington, DC.
- Reinking, D. (1994), “Electronic literacy” (Perspectives Series No.1-PS-N-07), The National Reading Research Center, Athens, GA, and College Park, MD.
- Shetzer, H. and M. Warschauer (2000), “An electronic literacy approach to network-based language teaching”, in M. Warschauer and R. Kem (eds.), Network-based Language Teaching: Concepts and Practice, Cambridge University Press, New York, pp. 171-185.

Smith, M.C. et al. (2000), “What will be the demands of literacy in the workplace in the next millennium?”, Reading Research Quarterly, Vol. 35/3, pp. 378-383.

Sticht, T.G. (ed.) (1975), Reading for Working: A Functional Literacy Anthology, Human Resources Research Organization, Alexandria, VA.

Stiggins, R.J. (1982), “An analysis of the dimensions of job-related reading”, Reading World, Vol. 82, pp. 237-247.

Sweets, R. and A. Meates (2004), ICT and Low Achievers: What Does PISA Tell us?, Hungarian Ministry of Education, Budapest, and OECD, Paris.

Wang, S. et al. (2007), “A meta-analysis of testing mode effects in Grade K–12 mathematics tests”, Educational and Psychological Measurement, Vol. 67, pp. 219-238.

Warschauer, M. (1999), Electronic Literacies: Language Culture and Power in Online Education, Lawrence Erlbaum Associates, Mahwah, NJ.

Werlich, E. (1976), A Text Grammar of English, Quelle and Meyer, Heidelberg.

## PISA 2015 Mathematics Framework

This chapter defines "mathematical literacy" as assessed in the Programme for International Student Assessment (PISA) in 2015 and the competencies required for mathematical literacy. It explains the processes, content knowledge and contexts reflected in the assessment's mathematics problems, and how student performance in mathematics is measured and reported.

In PISA 2015, mathematics is assessed as a minor domain, providing an opportunity to make comparisons of student performance over time. This framework continues the description and illustration of the PISA mathematics assessment as set out in the 2012 framework, when mathematics was re-examined and updated for use as the major domain in that cycle.

For PISA 2015, the computer is the primary mode of delivery for all domains, including mathematical literacy. However, paper-based assessment instruments are provided for countries that choose not to test their students by computer. The mathematical literacy component for both the computer-based and paper-based instruments are composed of the same clusters of mathematics trend items. The number of trend items in minor domains are increased, compared with previous PISA assessments, therefore increasing the construct coverage while reducing the number of students responding to each question. This design is intended to reduce potential bias while stabilising and improving the measurement of trends.

In PISA 2012, as the computer-based assessment of mathematics (CBAM) was an optional domain and was not taken by all countries, it is not part of the mathematical literacy trend. Therefore, CBAM items developed for PISA 2012 are not included in the 2015 assessment where mathematical literacy is a minor domain, despite the change in delivery mode.

The framework has been updated to reflect the change in delivery mode, and includes a discussion of the considerations of transposing paper items to a screen and examples of what the results look like. The definition and constructs of mathematical literacy however, remain unchanged and consistent with those used in 2012.

The PISA 2015 mathematics framework is organised into several major sections. The first section, “Defining Mathematical Literacy,” explains the theoretical underpinnings of the PISA mathematics assessment, including the formal definition of the mathematical literacy construct. The second section, “Organising the Domain of Mathematics,” describes three aspects: a) the mathematical processes and the fundamental mathematical capabilities (in previous frameworks the “competencies”) underlying those processes; b) the way mathematical content knowledge is organised in the PISA 2015 framework, and the content knowledge that is relevant to an assessment of 15-year-old students; and c) the contexts in which students face mathematical challenges. The third section, “Assessing Mathematical Literacy”, outlines the approach taken to apply the elements of the framework previously described, including the structure of the survey, the transfer to a computer-based assessment and reporting proficiency. The 2012 framework was written under the guidance of the 2012 Mathematics Expert Group (MEG), a body appointed by the main PISA contractors with the approval of the PISA Governing Board (PGB). The ten MEG members included mathematicians, mathematics educators, and experts in assessment, technology, and education research from a range of countries. In addition, to secure more extensive input and review, a draft of the PISA 2012 mathematics framework was circulated for feedback to over 170 mathematics experts from over 40 countries. Achieve and the Australian Council for Educational Research (ACER), the two organisations contracted by the Organisation for Economic Co-operation and Development (OECD) to manage framework development, also conducted various research efforts to inform and support development work. Framework development and the PISA programme generally have been supported and informed by the ongoing work of participating countries (e.g. the research described in OECD, 2010). This PISA 2015 Framework is an update written under the guidance of the 2015 Mathematics Expert Group (MEG), a body appointed by the Core 1 contractor with the approval of the PISA Governing Board (PGB).

## DEFINING MATHEMATICAL LITERACY

An understanding of mathematics is central to a young person’s preparedness for life in modern society. A growing proportion of problems and situations encountered in daily life, including in professional contexts, require some level of understanding of mathematics, mathematical reasoning and mathematical tools, before they can be fully understood and addressed. Mathematics is a critical tool for young people as they confront issues and challenges in personal, occupational, societal, and scientific aspects of their lives. It is thus important to have an understanding of the degree to which young people emerging from school are adequately prepared to apply mathematics to understanding important issues and solving meaningful problems. An assessment at age 15 provides an early indication of how individuals may respond in later life to the diverse array of situations they will encounter that involve mathematics.

The construct of mathematical literacy used in this report is intended to describe the capacities of individuals to reason mathematically and use mathematical concepts, procedures, facts and tools to describe, explain and predict phenomena. This conception of mathematical literacy supports the importance of students developing a strong understanding of concepts of pure mathematics and the benefits of being engaged in explorations in the abstract world of mathematics. The construct of mathematical literacy, as defined for PISA, strongly emphasises the need to develop students’ capacity to use mathematics in context, and it is important that they have rich experiences in their mathematics classrooms to accomplish this. For the purposes of PISA 2012, mathematical literacy was defined as shown in Box 4.1.

### Box 4.1 The 2015 definition of mathematical literacy

Mathematical literacy is an individual's capacity to formulate, employ and interpret mathematics in a variety of contexts. It includes reasoning mathematically and using mathematical concepts, procedures, facts and tools to describe, explain and predict phenomena. It assists individuals to recognise the role that mathematics plays in the world and to make the well-founded judgements and decisions needed by constructive, engaged and reflective citizens.

This definition is also used in the PISA 2015 assessment.

The focus of the language in the definition of mathematical literacy is on active engagement in mathematics, and is intended to encompass reasoning mathematically and using mathematical concepts, procedures, facts and tools in describing, explaining and predicting phenomena. In particular, the verbs “formulate”, “employ”, and “interpret” point to the three processes in which students as active problem solvers will engage.

The language of the definition is also intended to integrate the notion of mathematical modelling, which has historically been a cornerstone of the PISA framework for mathematics (e.g. OECD, 2004), into the PISA 2015 definition of mathematical literacy. As individuals use mathematics and mathematical tools to solve problems in contexts, their work progresses through a series of stages (individually developed later in the document).

The modelling cycle is a central aspect of the PISA conception of students as active problem solvers; however, it is often not necessary to engage in every stage of the modelling cycle, especially in the context of an assessment (Niss et al., 2007). The problem solver frequently carries out some steps of the modelling cycle but not all of them (e.g. when using graphs), or goes around the cycle several times to modify earlier decisions and assumptions.

The definition also acknowledges that mathematical literacy helps individuals to recognise the role that mathematics plays in the world and in helping them make the kinds of well-founded judgements and decisions required of constructive, engaged and reflective citizens.

Mathematical tools mentioned in the definition refer to a variety of physical and digital equipment, software and calculation devices. The 2015 computer-based survey includes an online calculator as part of the computer-based test material provided for some questions.

### ORGANISING THE DOMAIN OF MATHEMATICS

The PISA mathematics framework defines the domain of mathematics for the PISA survey and describes an approach to assessing the mathematical literacy of 15-year-olds. That is, PISA assesses the extent to which 15-year-old students can handle mathematics adeptly when confronted with situations and problems – the majority of which are presented in real-world contexts.

For purposes of the assessment, the PISA 2015 definition of mathematical literacy can be analysed in terms of three interrelated aspects:

- The mathematical processes that describe what individuals do to connect the context of the problem with mathematics and thus solve the problem, and the capabilities that underlie those processes.
- The mathematical content that is targeted for use in the assessment items.
- The contexts in which the assessment items are located.

The following sections elaborate these aspects. In highlighting these aspects of the domain, the PISA 2012 mathematics framework, which is also used in PISA 2015, helps to ensure that assessment items developed for the survey reflect a range of processes, content and contexts, so that, considered as a whole, the set of assessment items effectively operationalises what this framework defines as mathematical literacy. To illustrate the aspects of mathematic literacy, examples are available in the PISA 2012 Assessment and Analytical Framework (OECD, 2013) and on the PISA website (www.oecd.org/pisa/).

## Mathematical processes and the underlying mathematical capabilities

### Mathematical processes

The definition of mathematical literacy refers to an individual's capacity to formulate, employ and interpret mathematics. These three words – formulate, employ and interpret – provide a useful and meaningful structure for organising the mathematical processes that describe what individuals do to connect the context of a problem with the mathematics and thus solve the problem. Items in the 2015 PISA mathematics survey are assigned to one of three mathematical processes:

- formulating situations mathematically
- employing mathematical concepts, facts, procedures and reasoning
- interpreting, applying and evaluating mathematical outcomes.

It is important for both policy makers and those engaged more closely in the day-to-day education of students to know how effectively students are able to engage in each of these processes. The formulating process indicates how effectively students are able to recognise and identify opportunities to use mathematics in problem situations and then provide the necessary mathematical structure needed to formulate that contextualised problem into a mathematical form. The employing process indicates how well students are able to perform computations and manipulations and apply the concepts and facts that they know to arrive at a mathematical solution to a problem formulated mathematically. The interpreting process indicates how effectively students are able to reflect upon mathematical solutions or conclusions, interpret them in the context of a real-world problem, and determine whether the results or conclusions are reasonable. Students' facility in applying mathematics to problems and situations is dependent on skills inherent in all three of these processes, and an understanding of their effectiveness in each category can help inform both policy-level discussions and decisions being made closer to the classroom level.

## Formulating situations mathematically

The word *formulate* in the definition of mathematical literacy refers to individuals being able to recognise and identify opportunities to use mathematics and then provide mathematical structure to a problem presented in some contextualised form. In the process of formulating situations mathematically, individuals determine where they can extract the essential mathematics to analyse, set up and solve the problem. They translate from a real-world setting to the domain of mathematics and provide the real-world problem with mathematical structure, representations and specificity. They reason about and make sense of constraints and assumptions in the problem. Specifically, this process of formulating situations mathematically includes activities such as the following:

- identifying the mathematical aspects of a problem situated in a real-world context and identifying the significant variables
- recognising mathematical structure (including regularities, relationships and patterns) in problems or situations
- simplifying a situation or problem in order to make it amenable to mathematical analysis
- identifying constraints and assumptions behind any mathematical modelling and simplifications gleaned from the context
- representing a situation mathematically, using appropriate variables, symbols, diagrams and standard models
- representing a problem in a different way, including organising it according to mathematical concepts and making appropriate assumptions
- understanding and explaining the relationships between the context-specific language of a problem and the symbolic and formal language needed to represent it mathematically
- translating a problem into mathematical language or a representation
- recognising aspects of a problem that correspond with known problems or mathematical concepts, facts or procedures
- using technology (such as a spreadsheet or the list facility on a graphing calculator) to portray a mathematical relationship inherent in a contextualised problem.

## Employing mathematical concepts, facts, procedures and reasoning

The word *employ* in the definition of mathematical literacy refers to individuals being able to apply mathematical concepts, facts, procedures and reasoning to solve mathematically formulated problems to obtain mathematical conclusions. In the process of employing mathematical concepts, facts, procedures and reasoning to solve problems, individuals perform the mathematical procedures needed to derive results and find a mathematical solution (e.g. performing arithmetic computations, solving equations, making logical deductions from mathematical assumptions, performing symbolic manipulations, extracting mathematical information from tables and graphs, representing and manipulating shapes in space, and analysing data). They work on a model of the problem situation, establish regularities, identify connections between mathematical entities, and create mathematical arguments. Specifically, this process of employing mathematical concepts, facts, procedures and reasoning includes activities such as:

- devising and implementing strategies for finding mathematical solutions
- using mathematical tools¹, including technology, to help find exact or approximate solutions
- applying mathematical facts, rules, algorithms and structures when finding solutions
- manipulating numbers, graphical and statistical data and information, algebraic expressions and equations, and geometric representations
- making mathematical diagrams, graphs and constructions, and extracting mathematical information from them
- using and switching between different representations in the process of finding solutions
- making generalisations based on the results of applying mathematical procedures to find solutions
- reflecting on mathematical arguments and explaining and justifying mathematical results.

## Interpreting, applying and evaluating mathematical outcomes

The word *interpret* used in the definition of mathematical literacy focuses on the abilities of individuals to reflect upon mathematical solutions, results, or conclusions and interpret them in the context of real-life problems. This involves translating mathematical solutions or reasoning back into the context of a problem and determining whether the results are reasonable and make sense in the context of the problem. This mathematical process category encompasses both the “interpret” and “evaluate” arrows noted in the previously defined model of *mathematical literacy* in practice (see Figure 4.1).

## Individuals Engaged in This Process

Individuals engaged in this process may be called upon to construct and communicate explanations and arguments in the context of the problem, reflecting on both the modelling process and its results. Specifically, this process of interpreting, applying and evaluating mathematical outcomes includes activities such as:

- Interpreting a mathematical result back into the real-world context
- Evaluating the reasonableness of a mathematical solution in the context of a real-world problem
- Understanding how the real world impacts the outcomes and calculations of a mathematical procedure or model in order to make contextual judgements about how the results should be adjusted or applied
- Explaining why a mathematical result or conclusion does, or does not, make sense given the context of a problem
- Understanding the extent and limits of mathematical concepts and mathematical solutions
- Critiquing and identifying the limits of the model used to solve a problem.

### Desired Distribution of Items by Mathematical Process

The goal in constructing the assessment is to achieve a balance that provides approximately equal weighting between the two processes that involve making a connection between the real world and the mathematical world and the process that calls for students to be able to work on a mathematically formulated problem. Table 4.1 shows the desired distribution of items by process.

#### Table 4.1 Desired Distribution of Mathematics Items, by Process Category

| Process Category                                                 |   Percentage of Items |
|------------------------------------------------------------------|-----------------------|
| Formulating situations mathematically                            |                    25 |
| Employing mathematical concepts, facts, procedures and reasoning |                    50 |
| Interpreting, applying and evaluating mathematical outcomes      |                    25 |
| Total                                                            |                   100 |

### Fundamental Mathematical Capabilities Underlying the Mathematical Processes

A decade of experience in developing PISA items and analysing the ways in which students respond to items has revealed that there is a set of fundamental mathematical capabilities that underpins each of these reported processes and mathematical literacy in practice. The work of Mogens Niss and his Danish colleagues (Niss, 2003; Niss and Jensen, 2002; Niss and Højgaard, 2011) identified eight capabilities – referred to as “competencies” by Niss and in the PISA 2003 framework (OECD, 2004) – that are instrumental to mathematical behaviour.

The PISA 2015 framework uses a modified formulation of this set of capabilities, which condenses the number from eight to seven based on an investigation of the operation of the competencies through previously administered PISA items (Turner et al., 2013). These cognitive capabilities are available to or learnable by individuals in order to understand and engage with the world in a mathematical way, or to solve problems. As the level of mathematical literacy possessed by an individual increases, that individual is able to draw to an increasing degree on the fundamental mathematical capabilities (Turner and Adams, 2012). Thus, increasing activation of fundamental mathematical capabilities is associated with increasing item difficulty. This observation has been used as the basis of the descriptions of different proficiency levels of mathematical literacy reported in previous PISA surveys and discussed later in this framework.

The seven fundamental mathematical capabilities used in this framework are as follows:

- **Communication:** Mathematical literacy involves communication. The individual perceives the existence of some challenge and is stimulated to recognise and understand a problem situation. Reading, decoding and interpreting statements, questions, tasks or objects enables the individual to form a mental model of the situation, which is an important step in understanding, clarifying and formulating a problem. During the solution process, intermediate results may need to be summarised and presented. Later on, once a solution has been found, the problem solver may need to present the solution, and perhaps an explanation or justification, to others.
- **Mathematising:** Mathematical literacy can involve transforming a problem defined in the real world to a strictly mathematical form (which can include structuring, conceptualising, making assumptions, and/or formulating a model), or interpreting or evaluating a mathematical outcome or a mathematical model in relation to the original problem. The term mathematising is used to describe the fundamental mathematical activities involved.

## Representation

Mathematical literacy frequently involves representations of mathematical objects and situations. This can entail selecting, interpreting, translating between, and using a variety of representations to capture a situation, interact with a problem, or to present one's work. The representations referred to include graphs, tables, diagrams, pictures, equations, formulae and concrete materials.

## Reasoning and Argument

This capability involves logically rooted thought processes that explore and link problem elements so as to make inferences from them, check a justification that is given, or provide a justification of statements or solutions to problems.

## Devising Strategies for Solving Problems

Mathematical literacy frequently requires devising strategies for solving problems mathematically. This involves a set of critical control processes that guide an individual to effectively recognise, formulate and solve problems. This skill is characterised as selecting or devising a plan or strategy to use mathematics to solve problems arising from a task or context, as well as guiding its implementation. This mathematical capability can be demanded at any of the stages of the problem-solving process.

## Using Symbolic, Formal and Technical Language and Operations

Mathematical literacy requires using symbolic, formal and technical language and operations. This involves understanding, interpreting, manipulating, and making use of symbolic expressions within a mathematical context (including arithmetic expressions and operations) governed by mathematical conventions and rules. It also involves understanding and utilising formal constructs based on definitions, rules and formal systems and also using algorithms with these entities. The symbols, rules and systems used vary according to what particular mathematical content knowledge is needed for a specific task to formulate, solve or interpret the mathematics.

## Using Mathematical Tools

Mathematical tools include physical tools, such as measuring instruments, as well as calculators and computer-based tools that are becoming more widely available. In addition to knowing how to use these tools to assist them in completing mathematical tasks, students need to know about the limitations of such tools. Mathematical tools can also have an important role in communicating results.

## Figure 4.2 - Relationship between mathematical processes (top row) and fundamental mathematical capabilities (left-most column)

|                                          | Formulating situations mathematically                                                                                                  | Employing mathematical concepts, facts, procedures and reasoning                                                                                                                                                                                         | Interpreting, applying and evaluating mathematical outcomes                                                                                                           |
|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Communicating                            | Read, decode, and make sense of statements, questions, tasks, objects or images, in order to form a mental model of the situation      | Articulate a solution, show the work involved in reaching a solution and/or summarise and present intermediate mathematical results                                                                                                                      | Construct and communicate explanations and arguments in the context of the problem                                                                                    |
| Mathematising                            | Identify the underlying mathematical variables and structures in the real world problem, and make assumptions so that they can be used | Use an understanding of the context to guide or expedite the mathematical solving process, e.g. working to a context- appropriate level of accuracy                                                                                                      | Understand the extent and limits of a mathematical solution that are a consequence of the mathematical model employed                                                 |
| Representation                           | Create a mathematical representation of real-world information                                                                         | Make sense of, relate and use a variety of representations when interacting with a problem                                                                                                                                                               | Interpret mathematical outcomes in a variety of formats in relation to a situation or use; compare or evaluate two or more representations in relation to a situation |
| Reasoning and Argument                   | Explain, defend or provide a justification for the identified or devised representation of a real-world situation                      | Explain, defend or provide a justification for the processes and procedures used to determine a mathematical result or solution Connect pieces of information to arrive at a mathematical solution, make generalisations or create a multi-step argument | Reflect on mathematical solutions and create explanations and arguments that support, refute or qualify a mathematical solution to a contextualised problem           |
| Devising Strategies for Solving Problems | Select or devise a plan or strategy to mathematically reframe contextualised problems                                                  | Activate effective and sustained control mechanisms across a multi-step procedure leading to a mathematical solution, conclusion or generalisation                                                                                                       | Devise and implement a strategy in order to interpret, evaluate and validate a mathematical solution to a contextualised problem                                      |

### Relationship between mathematical processes and fundamental mathematical capabilities

| Fundamental Mathematical Capabilities                        | Formulating situations mathematically                                                                                             | Employing mathematical concepts, facts, procedures and reasoning                                                                                                | Interpreting, applying and evaluating mathematical outcomes                                                                                                                                                                                         |
|--------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Using symbolic, formal and technical language and operations | Use appropriate variables, symbols, diagrams and standard models to represent a real-world problem using symbolic/formal language | Understand and utilise formal constructs based on definitions, rules and formal systems as well as employing algorithms                                         | Understand the relationship between the context of the problem and representation of the mathematical solution. Use this understanding to help interpret the solution in context and gauge the feasibility and possible limitations of the solution |
| Using mathematical tools                                     | Use mathematical tools in order to recognise mathematical structures or to portray mathematical relationships                     | Know about and be able to make appropriate use of various tools that may assist in implementing processes and procedures for determining mathematical solutions | Use mathematical tools to ascertain the reasonableness of a mathematical solution and any limits and constraints on that solution, given the context of the problem                                                                                 |

These capabilities are evident to varying degrees in each of the three mathematical processes. The ways in which these capabilities manifest themselves within the three processes are described in Figure 4.1.

A good guide to the empirical difficulty of items can be obtained by considering which aspects of the fundamental mathematical capabilities are required for planning and executing a solution (Turner, 2012; Turner and Adams, 2012; Turner et al., 2013). The easiest items will require the activation of few capabilities and in a relatively straightforward way. The hardest items require complex activation of several capabilities. Predicting difficulty requires consideration of both the number of capabilities and the complexity of activation required.

### Mathematical content knowledge

An understanding of mathematical content – and the ability to apply that knowledge to the solution of meaningful contextualised problems – is important for citizens in the modern world. That is, to solve problems and interpret situations in personal, occupational, societal and scientific contexts, there is a need to draw upon certain mathematical knowledge and understandings.

Mathematical structures have been developed over time as a means to understand and interpret natural and social phenomena. In schools, the mathematics curriculum is typically organised around content strands (e.g. number, algebra and geometry) and detailed topic lists that reflect historically well-established branches of mathematics and that help in defining a structured curriculum. However, outside the mathematics classroom, a challenge or situation that arises is usually not accompanied by a set of rules and prescriptions that shows how the challenge can be met. Rather, it typically requires some creative thought in seeing the possibilities of bringing mathematics to bear on the situation and in formulating it mathematically. Often a situation can be addressed in different ways drawing on different mathematical concepts, procedures, facts or tools.

Since the goal of PISA is to assess mathematical literacy, an organisational structure for mathematical content knowledge is proposed based on the mathematical phenomena that underlie broad classes of problems and which have motivated the development of specific mathematical concepts and procedures. Because national mathematics curricula are typically designed to equip students with knowledge and skills that address these same underlying mathematical phenomena, the outcome is that the range of content arising from organising content this way is closely aligned with that typically found in national mathematics curricula. This framework lists some content topics appropriate for assessing the mathematical literacy of 15-year-old students, based on analyses of national standards from eleven countries.

To organise the domain of mathematics for purposes of assessing mathematical literacy, it is important to select a structure that grows out of historical developments in mathematics, that encompasses sufficient variety and depth to reveal the essentials of mathematics, and that also represents, or includes, the conventional mathematical strands in an acceptable way. Thus, a set of content categories that reflects the range of underlying mathematical phenomena was selected for the PISA 2015 framework, consistent with the categories used for previous PISA surveys.

The following list of content categories, therefore, is used in PISA 2015 to meet the requirements of historical development, coverage of the domain of mathematics and the underlying phenomena which motivate its development, and reflection of the major strands of school curricula. These four categories characterise the range of mathematical content that is central to the discipline and illustrate the broad areas of content used in the test items for PISA 2015:

- Change and relationships
- Space and shape
- Quantity
- Uncertainty and data

With these four categories, the mathematical domain can be organised in a way that ensures a spread of items across the domain and focuses on important mathematical phenomena, but at the same time, avoids a too fine division that would work against a focus on rich and challenging mathematical problems based on real situations. While categorisation by content category is important for item development and selection, and for reporting of assessment results, it is important to note that some specific content topics may materialise in more than one content category. Connections between aspects of content that span these four content categories contribute to the coherence of mathematics as a discipline and are apparent in some of the assessment items selected for the PISA 2015 assessment.

The broad mathematical content categories and the more specific content topics appropriate for 15-year-old students described later in this section reflect the level and breadth of content that is eligible for inclusion on the PISA 2015 survey. Narrative descriptions of each content category and the relevance of each to solving meaningful problems are provided first, followed by more specific definitions of the kinds of content that are appropriate for inclusion in an assessment of mathematical literacy of 15-year-old students. These specific topics reflect commonalities found in the expectations set by a range of countries and education jurisdictions. The standards examined to identify these content topics are viewed as evidence not only of what is taught in mathematics classrooms in these countries but also as indicators of what countries view as important knowledge and skills for preparing students of this age to become constructive, engaged and reflective citizens.

Descriptions of the mathematical content knowledge that characterise each of the four categories – change and relationships, space and shape, quantity, and uncertainty and data – are provided below.

## Change and relationships

The natural and designed worlds display a multitude of temporary and permanent relationships among objects and circumstances, where changes occur within systems of inter-related objects or in circumstances where the elements influence one another. In many cases these changes occur over time, and in other cases changes in one object or quantity are related to changes in another. Some of these situations involve discrete change; others change continuously. Some relationships are of a permanent, or invariant, nature. Being more literate about change and relationships involves understanding fundamental types of change and recognising when they occur in order to use suitable mathematical models to describe and predict change. Mathematically this means modelling the change and the relationships with appropriate functions and equations, as well as creating, interpreting, and translating among symbolic and graphical representations of relationships.

Change and relationships is evident in such diverse settings as growth of organisms, music, and the cycle of seasons, weather patterns, employment levels and economic conditions. Aspects of the traditional mathematical content of functions and algebra, including algebraic expressions, equations and inequalities, tabular and graphical representations, are central in describing, modelling and interpreting change phenomena. Representations of data and relationships described using statistics also are often used to portray and interpret change and relationships, and a firm grounding in the basics of number and units is also essential to defining and interpreting change and relationships. Some interesting relationships arise from geometric measurement, such as the way that changes in perimeter of a family of shapes might relate to changes in area, or the relationships among lengths of the sides of triangles.

## Space and shape

Space and shape encompasses a wide range of phenomena that are encountered everywhere in our visual and physical world: patterns, properties of objects, positions and orientations, representations of objects, decoding and encoding of visual information, navigation and dynamic interaction with real shapes as well as with representations. Geometry serves as an essential foundation for space and shape, but the category extends beyond traditional geometry in content, meaning and method, drawing on elements of other mathematical areas such as spatial visualisation, measurement and algebra.

For instance, shapes can change, and a point can move along a locus, thus requiring function concepts. Measurement formulas are central in this area. The manipulation and interpretation of shapes in settings that call for tools ranging from dynamic geometry software to Global Positioning System (GPS) software are included in this content category.

PISA assumes that the understanding of a set of core concepts and skills is important to mathematical literacy relative to space and shape. Mathematical literacy in the area of space and shape involves a range of activities such as understanding perspective (for example in paintings), creating and reading maps, transforming shapes with and without technology, interpreting views of three-dimensional scenes from various perspectives and constructing representations of shapes.

## Quantity

The notion of Quantity may be the most pervasive and essential mathematical aspect of engaging with, and functioning in, our world. It incorporates the quantification of attributes of objects, relationships, situations and entities in the world, understanding various representations of those quantifications, and judging interpretations and arguments based on quantity. To engage with the quantification of the world involves understanding measurements, counts, magnitudes, units, indicators, relative size, and numerical trends and patterns. Aspects of quantitative reasoning – such as number sense, multiple representations of numbers, elegance in computation, mental calculation, estimation and assessment of reasonableness of results – are the essence of mathematical literacy relative to quantity.

Quantification is a primary method for describing and measuring a vast set of attributes of aspects of the world. It allows for the modelling of situations, for the examination of change and relationships, for the description and manipulation of space and shape, for organising and interpreting data, and for the measurement and assessment of uncertainty. Thus mathematical literacy in the area of quantity applies knowledge of number and number operations in a wide variety of settings.

## Uncertainty and data

In science, technology and everyday life, uncertainty is a given. Uncertainty is therefore a phenomenon at the heart of the mathematical analysis of many problem situations, and the theory of probability and statistics as well as techniques of data representation and description have been established to deal with it. The uncertainty and data content category includes recognising the place of variation in processes, having a sense of the quantification of that variation, acknowledging uncertainty and error in measurement, and knowing about chance. It also includes forming, interpreting and evaluating conclusions drawn in situations where uncertainty is central. The presentation and interpretation of data are key concepts in this category (Moore, 1997).

There is uncertainty in scientific predictions, poll results, weather forecasts and economic models. There is variation in manufacturing processes, test scores and survey findings, and chance is fundamental to many recreational activities enjoyed by individuals. The traditional curricular areas of probability and statistics provide formal means of describing, modelling and interpreting a certain class of uncertainty phenomena, and for making inferences. In addition, knowledge of number and of aspects of algebra, such as graphs and symbolic representation, contribute to facility in engaging in problems in this content category. The focus on the interpretation and presentation of data is an important aspect of the uncertainty and data category.

## Desired distribution of items by content category

The trend items selected for PISA 2015 are distributed across the four content categories, as shown in Table 4.2. The goal in constructing the survey is a balanced distribution of items with respect to content category, since all of these domains are important for constructive, engaged and reflective citizens.

| Content category         |   Percentage of items |
|--------------------------|-----------------------|
| Change and relationships |                    25 |
| Space and shape          |                    25 |
| Quantity                 |                    25 |
| Uncertainty and data     |                    25 |
| Total                    |                   100 |

## Content topics for guiding the assessment of mathematical literacy

To effectively understand and solve contextualised problems involving change and relationships, space and shape, quantity and uncertainty and data requires drawing upon a variety of mathematical concepts, procedures, facts, and tools at an appropriate level of depth and sophistication. As an assessment of mathematical literacy, PISA strives to assess the levels and types of mathematics that are appropriate for 15-year-old students on a trajectory to become constructive, engaged and reflective citizens able to make well-founded judgements and decisions. It is also the case that PISA, while not designed or intended to be a curriculum-driven assessment, strives to reflect the mathematics that students have likely had the opportunity to learn by the time they are 15 years old.

The content included in PISA 2015 is the same as that developed in PISA 2012. The four content categories of change and relationships, space and shape, quantity and uncertainty and data serve as the foundation for identifying this range of content, yet there is not a one-to-one mapping of content topics to these categories. The following content is intended to reflect the centrality of many of these concepts to all four content categories and reinforce the coherence of mathematics as a discipline. It intends to be illustrative of the content topics included in PISA 2015, rather than an exhaustive listing:

- **Functions:** the concept of function, emphasising but not limited to linear functions, their properties, and a variety of descriptions and representations of them. Commonly used representations are verbal, symbolic, tabular and graphical.
- **Algebraic expressions:** verbal interpretation of and manipulation with algebraic expressions, involving numbers, symbols, arithmetic operations, powers and simple roots.
- **Equations and inequalities:** linear and related equations and inequalities, simple second-degree equations, and analytic and non-analytic solution methods.
- **Co-ordinate systems:** representation and description of data, position and relationships.
- **Relationships within and among geometrical objects in two and three dimensions:** static relationships such as algebraic connections among elements of figures (e.g. the Pythagorean theorem as defining the relationship between the lengths of the sides of a right triangle), relative position, similarity and congruence, and dynamic relationships involving transformation and motion of objects, as well as correspondences between two- and three-dimensional objects.
- **Measurement:** quantification of features of and among shapes and objects, such as angle measures, distance, length, perimeter, circumference, area and volume.
- **Numbers and units:** concepts, representations of numbers and number systems, including properties of integer and rational numbers, relevant aspects of irrational numbers, as well as quantities and units referring to phenomena such as time, money, weight, temperature, distance, area and volume, and derived quantities and their numerical description.
- **Arithmetic operations:** the nature and properties of these operations and related notational conventions.
- **Percents, ratios and proportions:** numerical description of relative magnitude and the application of proportions and proportional reasoning to solve problems.
- **Counting principles:** simple combinations and permutations.
- **Estimation:** purpose-driven approximation of quantities and numerical expressions, including significant digits and rounding.
- **Data collection, representation and interpretation:** nature, genesis and collection of various types of data, and the different ways to represent and interpret them.
- **Data variability and its description:** concepts such as variability, distribution and central tendency of data sets, and ways to describe and interpret these in quantitative terms.
- **Samples and sampling:** concepts of sampling and sampling from data populations, including simple inferences based on properties of samples.
- **Chance and probability:** notion of random events, random variation and its representation, chance and frequency of events, and basic aspects of the concept of probability.

### Contexts

The choice of appropriate mathematical strategies and representations is often dependent on the context in which a mathematics problem arises. Context is widely regarded as an aspect of problem solving that imposes additional demands on the problem solver (see Watson and Callingham, 2003, for findings about statistics). For the PISA survey, it is important that a wide variety of contexts is used. This offers the possibility of connecting with the broadest possible range of individual interests and with the range of situations in which individuals operate in the 21st century.

For purposes of the PISA 2015 mathematics framework, four context categories have been defined and are used to classify assessment items developed for the PISA survey:

- **Personal** – Problems classified in the personal context category focus on activities of one’s self, one’s family or one’s peer group. The kinds of contexts that may be considered personal include (but are not limited to) those involving food preparation, shopping, games, personal health, personal transportation, sports, travel, personal scheduling and personal finance.
- **Occupational** – Problems classified in the occupational context category are centred on the world of work. Items categorised as occupational may involve (but are not limited to) such things as measuring, costing and ordering materials for building, payroll/accounting, quality control, scheduling/inventory, design/architecture and job-related decision making. Occupational contexts may relate to any level of the workforce, from unskilled work to the highest levels of professional work, although items in the PISA survey must be accessible to 15-year-old students.
- **Societal** – Problems classified in the societal context category focus on one’s community (whether local, national or global). They may involve (but are not limited to) such things as voting systems, public transport, government, public policies, demographics, advertising, national statistics and economics. Although individuals are involved in all of these things in a personal way, in the societal context category the focus of problems is on the community perspective.
- **Scientific** – Problems classified in the scientific category relate to the application of mathematics to the natural world and issues and topics related to science and technology. Particular contexts might include (but are not limited to) such areas as weather or climate, ecology, medicine, space science, genetics, measurement and the world of mathematics itself. Items that are intramathematical, where all the elements involved belong in the world of mathematics, fall within the scientific context.

PISA assessment items are arranged in units that share stimulus material. It is therefore usually the case that all items in the same unit belong to the same context category. Exceptions do arise; for example stimulus material may be examined from a personal point of view in one item and a societal point of view in another. When an item involves only mathematical constructs without reference to the contextual elements of the unit within which it is located, it is allocated to the context category of the unit. In the unusual case of a unit involving only mathematical constructs and being without reference to any context outside of mathematics, the unit is assigned to the scientific context category.

Using these context categories provides the basis for selecting a mix of item contexts and ensures that the assessment reflects a broad range of uses of mathematics, ranging from everyday personal uses to the scientific demands of global problems. Moreover, it is important that each context category be populated with assessment items having a broad range of item difficulties. Given that the major purpose of these context categories is to challenge students in a broad range of problem contexts, each category should contribute substantially to the measurement of mathematical literacy. It should not be the case that the difficulty level of assessment items representing one context category is systematically higher or lower than the difficulty level of assessment items in another category.

In identifying contexts that may be relevant, it is critical to keep in mind that a purpose of the assessment is to gauge the use of mathematical content knowledge, processes and capabilities that students have acquired by the age of 15. Contexts for assessment items, therefore, are selected in light of relevance to students’ interests and lives, and the demands that will be placed upon them as they enter society as constructive, engaged and reflective citizens. National project managers from countries participating in the PISA survey are involved in judging the degree of such relevance.

### Desired distribution of items by context category

The trend items selected for the PISA 2015 mathematics survey represent a spread across these context categories, as described in Table 4.3. With this balanced distribution, no single context type is allowed to dominate, providing students with items that span a broad range of individual interests and a range of situations that they might expect to encounter in their lives.

| Content category   |   Percentage of items |
|--------------------|-----------------------|
| Personal           |                    25 |
| Occupational       |                    25 |
| Societal           |                    25 |
| Scientific         |                    25 |
| Total              |                   100 |

## Assessing Mathematical Literacy

This section outlines the approach taken to apply the elements of the framework described in previous sections to PISA 2015. This includes the structure of the mathematics component of the PISA survey, arrangements for transferring the paper-based trend items to a computer-based delivery, and reporting mathematical proficiency.

### Structure of the survey instrument

In 2012, when mathematical literacy was the major domain, the paper-based instrument contained a total of 270 minutes of mathematics material. The material was arranged in nine clusters of items, with each cluster representing 30 minutes of testing time. The item clusters were placed in test booklets according to a rotated design; they also contained linked materials.

Mathematical literacy is a minor domain in 2015 and students are asked to complete fewer clusters. However, the item clusters are similarly constructed and rotated. Six mathematics clusters from previous cycles, including one “easy” and one “hard”, are used in one of three designs, depending on whether countries take the Collaborative Problem Solving option or not, or whether they take the test on paper. Using six clusters rather than three as was customary for the minor domains in previous cycles results in a larger number of trend items, therefore the construct coverage is increased. However, the number of students responding to each question is lower. This design is intended to reduce potential bias, thus stabilising and improving the measurement of trends. The field trial was used to perform a mode-effect study and to establish equivalence between the computer- and paper-based forms.

### Response formats

Three types of response format are used to assess mathematical literacy in PISA 2015: open constructed-response, closed constructed-response and selected-response (simple and complex multiple-choice) items. Open constructed-response items require a somewhat extended written response from a student. Such items also may ask the student to show the steps taken or to explain how the answer was reached. These items require trained experts to manually code student responses.

Closed constructed-response items provide a more structured setting for presenting problem solutions, and they produce a student response that can be easily judged to be either correct or incorrect. Often student responses to questions of this type can be keyed into data-capture software, and coded automatically, but some must be manually coded by trained experts. The most frequent closed constructed-responses are single numbers.

Selected-response items require students to choose one or more responses from a number of response options. Responses to these questions can usually be automatically processed. About equal numbers of each of these response formats is used to construct the survey instruments.

### Item scoring

Although most of the items are dichotomously scored (that is, responses are awarded either credit or no credit), the open constructed-response items can sometimes involve partial credit scoring, which allows responses to be assigned credit according to differing degrees of “correctness” of responses. For each such item, a detailed coding guide that allows for full credit, partial credit or no credit is provided to persons trained in the coding of student responses across the range of participating countries to ensure coding of responses is done in a consistent and reliable way. To maximise the comparability between the paper-based and computer-based assessments, careful attention is given to the scoring guides in order to ensure that the important elements are included.

### Computer-based assessment of mathematics

The main mode of delivery for the PISA 2012 assessment was paper-based. In moving to computer-based delivery for 2015, care is taken to maximise comparability between the two assessments. The following section describes some of the features intrinsic to a computer-based assessment. Although these features provide the opportunities outlined below, to ensure comparability the PISA 2015 survey consists solely of items from the 2012 paper-based assessment. The features described here, however, will be used in future PISA assessments when their introduction can be controlled to ensure comparability with prior assessments.

Increasingly, mathematics tasks at work involve some kind of electronic technology, so that mathematical literacy and computer use are melded together (Hoyles et al., 2002). For employees at all levels of the workplace, there is now an interdependency between mathematical literacy and the use of computer technology. Solving PISA items on a computer rather than on paper moves PISA into the reality and the demands of the 21st century.

There is a great deal of research evidence into paper- and computer-based test performance, but findings are mixed. Some research suggests that a computer-based testing environment can influence students' performance. Richardson et al. (2002) reported that students found computer-based problem-solving tasks engaging and motivating, often despite the unfamiliarity of the problem types and the challenging nature of the items. They were sometimes distracted by attractive graphics, and sometime used poor heuristics when attempting tasks.

In one of the largest comparisons of paper-based and computer-based testing, Sandene et al. (2008) found that eighth-grade students’ mean score was four points higher on a computer-based mathematics test than on an equivalent paper-based test. Bennett et al. (2008) concluded from his research that computer familiarity affects performance on computer-based mathematics tests, while others have found that the range of functions available through computer-based tests can affect performance. For example, Mason (2001) found that students’ performance was negatively affected in computer-based tests compared to paper-based tests when there was no opportunity on the computer version to review and check responses. Bennett (2003) found that screen size affected scores on verbal-reasoning tests, possibly because smaller computer screens require scrolling.

By contrast, Wang et al (2008) conducted a meta-analysis of studies pertaining to K-12 students’ mathematics achievements which indicated that administration mode has no statistically significant effect on scores. Moreover, recent mode studies that were part of the Programme for the International Assessment of Adult Competencies (PIAAC) suggested that equality can be achieved (OECD, 2014). In this study, adults were randomly assigned to either a computer-based or paper-based assessment of literacy and numeracy skills. The majority of the items used in the paper delivery mode were adapted for computer delivery and used in this study. Analyses of these data revealed that almost all of the item parameters were stable across the two modes, thus demonstrating the ability to place respondents on the same literacy and numeracy scale. Given this, it is hypothesised that mathematics items used in PISA 2012 can be transposed onto a screen without affecting trend data. (The PISA 2015 field trial studied the effect on student performance of the change in mode of delivery. For further details see Box 1.2.)

Just as paper-based assessments rely on a set of fundamental skills for working with printed materials, computer-based assessments rely on a set of fundamental information and communications technology (ICT) skills for using computers. These include knowledge of basic hardware (e.g. keyboard and mouse) and basic conventions (e.g. arrows to move forward and specific buttons to press to execute commands). The intention is to keep such skills to a minimal, core level in the computer-based assessment.

## Reporting proficiency in mathematics

The outcomes of the PISA mathematics survey are reported in a number of ways. Estimates of overall mathematical proficiency are obtained for sampled students in each participating country, and a number of proficiency levels are defined. Descriptions of the degree of mathematical literacy typical of students in each level are also developed. For PISA 2003, scales based on the four broad content categories were developed. In Figure 4.3, descriptions of the six proficiency levels reported for the overall PISA mathematics scale in 2012 are presented. These form the basis of the PISA 2015 mathematics scale. The finalised 2012 scale is used to report the PISA 2015 outcomes. As mathematical literacy is a minor domain in 2015, only the overall proficiency scale is reported.

Fundamental mathematical capabilities play a central role in defining what it means to be at different levels of the scales for mathematical literacy overall and for each of the reported processes. For example, in the proficiency scale description for Level 4 (see Figure 4.3), the second sentence highlights aspects of mathematising and representation that are evident at this level. The final sentence highlights the characteristic communication, reasoning and argument of Level 4, providing a contrast with the short communications and lack of argument of Level 3 and the additional reflection of Level 5. In an earlier section of this framework and in Figure 4.2, each of the mathematical processes was described in terms of the fundamental mathematical capabilities that individuals might activate when engaging in that process.

### Figure 4.3 - Summary description of the six levels of mathematics proficiency in PISA 2015

|   Level | What students can typically do                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|       6 | At Level 6, students can conceptualise, generalise and utilise information based on their investigations and modelling of complex problem situations, and can use their knowledge in relatively non-standard contexts. They can link different information sources and representations and flexibly translate among them. Students at this level are capable of advanced mathematical thinking and reasoning. These students can apply this insight and understanding, along with a mastery of symbolic and formal mathematical operations and relationships, to develop new approaches and strategies for attacking novel situations. Students at this level can reflect on their actions, and can formulate and precisely communicate their actions and reflections regarding their findings, interpretations, arguments, and the appropriateness of these to the original situation. |
|       5 | At Level 5, students can develop and work with models for complex situations, identifying constraints and specifying assumptions. They can select, compare and evaluate appropriate problem-solving strategies for dealing with complex problems related to these models. Students at this level can work strategically using broad, well-developed thinking and reasoning skills, appropriate linked representations, symbolic and formal characterisations, and insight pertaining to these situations. They begin to reflect on their work and can formulate and communicate their interpretations and reasoning.                                                                                                                                                                                                                                                                    |
|       4 | At Level 4, students can work effectively with explicit models for complex concrete situations that may involve constraints or call for making assumptions. They can select and integrate different representations, including symbolic, linking them directly to aspects of real-world situations. Students at this level can utilise their limited range of skills and can reason with some insight, in straightforward contexts. They can construct and communicate explanations and arguments based on their interpretations, arguments and actions.                                                                                                                                                                                                                                                                                                                                |
|       3 | At Level 3, students can execute clearly described procedures, including those that require sequential decisions. Their interpretations are sufficiently sound to be a base for building a simple model or for selecting and applying simple problem-solving strategies. Students at this level can interpret and use representations based on different information sources and reason directly from them. They typically show some ability to handle percentages, fractions and decimal numbers, and to work with proportional relationships. Their solutions reflect that they have engaged in basic interpretation and reasoning.                                                                                                                                                                                                                                                   |
|       2 | At Level 2, students can interpret and recognise situations in contexts that require no more than direct inference. They can extract relevant information from a single source and make use of a single representational mode. Students at this level can employ basic algorithms, formulae, procedures or conventions to solve problems involving whole numbers. They are capable of making literal interpretations of the results.                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|       1 | At Level 1, students can answer questions involving familiar contexts where all relevant information is present and the questions are clearly defined. They are able to identify information and to carry out routine procedures according to direct instructions in explicit situations. They can perform actions that are almost always obvious and follow immediately from the given stimuli.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |

## Note

1. In some countries, “mathematical tools” can also refer to established mathematical procedures, such as algorithms. For the purposes of the PISA framework, “mathematical tools” refers only to the physical and digital tools described in this section.

## References

Bennett, R.E. (2003), *Online Assessment and the Comparability of Score Meaning* , Research Memorandum, Educational Testing Service, Princeton, NJ.

Bennett, R.E. et al. (2008), “Does it matter if i take my mathematics test on computer? A second empirical study of mode effects in NAEP”, *Journal of Technology, Learning, and Assessment* , Vol. 6/9.

Hoyle, C. et al. (2002), *Mathematical Skills in the Workplace: Final Report to the Science Technology and Mathematics Council* , Institute of Education, University of London, London, http://eprints.ioe.ac.uk/1565/.

Mason, B., M. Patry and D. Berstein (2001), “An examination of the equivalence between non-adaptive computer based and traditional testing”, *Journal of Education Computing Research* , Vol. 1/24, pp. 29-39.

Moore, D. (1997), “New pedagogy and new content: The case of statistics”, *International Statistical Review* , Vol. 2/65, pp. 123-137.

Niss, M. (2003), “Mathematical competencies and the learning of mathematics: The Danish KoM Project”, in A. Gagatsis and S. Papastavridis (eds.), *3rd Mediterranean Conference on Mathematics Education* , Hellenic Mathematical Society, Athens, pp. 116-124.

Niss, M., W. Blum and P. Galbraith (2007), “Introduction”, in W. Blum, P.L. Galbraith, H.W. Henn and M. Niss (eds.), *Modelling and Applications in Mathematics Education* (The 14th ICMI Study), Springer, New York, pp. 3-32.

Niss, M. and T.H. Jensen (2002), “Kompetencer og matematiklæring: Ideer og inspiration til udvikling af matematikundervisning i Danmark, uddannelsesstyrelsens temahæfteserie”, No. 18, Ministry of Education, Copenhagen, http://pub.uvm.dk/2002/kom/.

Niss, M. and T. Højgaard (eds.) (2011), “Competencies and mathematical learning: Ideas and inspiration for the development of mathematics teaching and learning in Denmark”, Ministry of Education, Report No. 485, Roskilde University, Roskilde, https://pure.au.dk/portal/files/41669781/thj11\_mn\_kom\_in\_english.pdf.

OECD (2014), *Technical Report of the Survey of Adult Skills (PIAAC)* , pre-publication, OECD, Paris, www.oecd.org/site/piaac/-Technical%20Report\_17OCT13.pdf.

OECD (2013), *PISA 2012 Assessment and Analytical Framework: Mathematics, Reading, Science, Problem Solving and Financial Literacy* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264190511-en.

OECD (2010), *Pathways to Success: How Knowledge and Skills at Age 15 Shape Future Lives in Canada* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264081925-en.

OECD (2004), *The PISA 2003 Assessment Framework: Mathematics, Reading, Science and Problem Solving Knowledge and Skills* , PISA, OECD Publishing, Paris, http://dx.doi.org/10.1787/9789264101739-en.

Richardson, M. et al. (2002), “Challenging minds? Students’ perceptions of computer-based world class tests of problem solving”, *Computers in Human Behavior* , Vol. 18/6, pp. 633-649.

Sandene, B. et al. (2005), *Online Assessment in Mathematics and Writing: Reports from the NAEP Technology-Based Assessment Project, Research and Development Series* (NCES 2005–457). US Department of Education, National Center for Education Statistics, US Government Printing Office, Washington, DC.

Turner, R. and R.J. Adams (2012), “Some drivers of test item difficulty in mathematics: an analysis of the competency rubric”, paper presented at the annual meeting of the American Educational Research Association (AERA), 13-17 April, Vancouver, http://research.acer.edu.au/pisa/77.

Turner, R. et al. (2013), “Using mathematical competencies to predict item difficulty in PISA”, in M. Prenzel et al. (eds), *Research on PISA: Research Outcomes of the PISA Research Conference 2009* , Springer, New York, pp. 23-27.

Watson, J.M. and R. Callingham (2003), “Statistical literacy: A complex hierarchical construct”, *Statistics Education Research Journal* , Vol. 2/2, pp. 3-46.

Wang, S. et al. (2007), “A meta-analysis of testing mode effects in Grade K–12 mathematics tests”, *Educational and Psychological Measurement* , Vol. 67, pp. 219-238.

## PISA 2015 Financial Literacy Framework

This chapter describes the rationale behind measuring 15-year-olds' financial literacy in the Programme for International Student Assessment (PISA) and defines the term. It explains the content, processes and contexts that are reflected in the financial literacy problems used in the assessment, and describes how student proficiency in financial literacy is measured and reported.

## POLICY INTEREST IN FINANCIAL LITERACY

In recent years, developed and emerging countries and economies have become increasingly concerned about the level of financial literacy among their citizens. This has stemmed, in particular, from shrinking public and private support systems, shifting demographic profiles, including the ageing of the population, and wide-ranging developments in the financial marketplace. A lack of financial literacy contributes to ill-informed financial decisions, and these decisions could, in turn, have tremendous adverse effects on both personal and, ultimately, global finance (OECD/INFE, 2009; OECD, 2009a; see also Gerardi et al., 2010, for empirical analysis of financial literacy and mortgage delinquency). As a result, financial literacy is now acknowledged as an important element of economic and financial stability and development. This is reflected in the G20 endorsement of the OECD/INFE (International Network on Financial Education) High-level Principles on National Strategies for Financial Education (G20, 2012; OECD/INFE, 2012), the OECD/INFE policy handbook on national strategies for financial education and core competencies on financial literacy for youth, and its statement supporting the widespread use of instruments to measure financial literacy, including the PISA financial literacy assessment (G20, 2013; OECD INFE, 2015b; OECD INFE, 2015c).

## Demographic and cultural shifts

In most countries, longevity is increasing, and in many the birth rate is falling. At the same time, women's participation in the labour force and the proportion of people entering higher education are both increasing, and grown-up children are less likely to continue to live in close proximity to older family members than in previous generations. The likely outcome of these shifts will be a greater need for financial security in retirement and professional care in old age, resulting in additional government expenditure (Colombo et al., 2011). Working-age adults may be expected to shoulder the tax burden to finance this expenditure while at the same time also saving for their own retirement, potentially repaying their own student loans, and managing increasingly varied working-life trajectories, which may include periods of inactivity, self-employment and/or retraining.

## Risk shift and increased individual responsibility

There has been a widespread transfer of risk from both governments and employers to individuals, meaning that now many people face the financial risks associated with longevity, investment, credit, out-of-pocket healthcare and long-term care. The number of financial decisions that individuals have to make, and the significance of these decisions, is increasing as a consequence of changes in the market and the economy. For instance, longer life expectancy means individuals need to ensure that they accumulate savings to cover much longer periods of retirement than previous generations, despite the steadily rising age of retirement in many countries. Traditional pay-as-you-go (PAYG) public pension schemes are supplemented by privately funded schemes in which the individual may be responsible for making investment decisions, including the contribution rate, the investment allocation and the type of pay-out product. Moreover, defined-contribution pension plans are quickly replacing defined-benefit pension plans for new entrants, shifting onto workers the risks of uncertain investment performance and of longer life expectancy.

Even when individuals use the services of financial intermediaries and advisors, they need to be financially literate in order to understand what is being offered or advised, and to manage the products they choose. They should also be aware that some advisors may face a conflict of interest. Depending on the national legal framework for financial advice, individuals may be fully responsible for the financial product they decide to purchase, facing all the direct consequences of their choice.

Surveys show that a majority of workers are unaware of the risks they now have to face, and have neither sufficient financial knowledge nor the skills to manage such risks adequately, even if they are aware of them (OECD, 2008; Money and Pensions Panel, 2013; Barrett et al., 2013).

## Greater supply of a wide range of financial products and services

In addition, in all countries, growing numbers of consumers have access to a wide range of financial products and services from a variety of providers, delivered through various channels.¹ Greater financial inclusion in emerging economies, as well as worldwide developments in technology and deregulation have resulted in widening access to all kinds of financial products, from current accounts and remittances products to revolving credit and equity portfolios. The products available are also becoming more complex, and individuals are required to compare these products in a number of ways, such as the fees charged, interest rates paid or received, length of contract and exposure to risk. Individuals must also identify appropriate providers and delivery channels from the vast array of possibilities, including community groups, traditional financial institutions, online banks and mobile phone companies.

## Increased demand for financial products and services

Economic and technological developments have brought greater global connectedness and massive changes in both the methods and frequency of communications and financial transactions, as well as in social interactions and consumer behaviour. Such changes have made it more important that individuals are able to interact with financial providers and their intermediaries. In particular, consumers often need access to financial services (including banks and other providers, such as post offices) in order to make and receive electronic payments, like income, remittances and online transactions, and even to conduct face-to-face transactions when cash or cheques are no longer favoured. Together, these trends have transferred the responsibility of major financial decisions to individuals, enlarged the options for the majority of the population (including new financial consumers) and increased the level of complexity they face. Against this backdrop, individuals are expected to be sufficiently financially literate to take the necessary steps to protect themselves and their relatives and ensure their financial well-being.

## Expected benefits of financial education and improved levels of financial literacy

Existing empirical evidence shows that young people and adults in both developed and emerging economies who have been exposed to good-quality financial education are subsequently more likely than others to plan ahead, save and engage in other responsible financial behaviours (Bernheim et al., 2001; Cole et al., 2011; Lusardi, 2009; Atkinson et al. 2015; Bruhn et al. 2013; Miller et al. 2014). This evidence suggests a possible causal link between financial education and outcomes, and indicates that improved levels of financial literacy can lead to positive behaviour change.

Other research indicates a number of potential benefits of being financially literate. There is mounting evidence that in developed countries those with higher financial literacy are better able to manage their money, participate in the stock market and perform better on their portfolio choice, and that they are more likely to choose mutual funds with lower fees (Hastings and Tejeda-Ashton, 2008; Hilgert et al., 2003; Lusardi and Mitchell, 2008, 2011; Stango and Zinman, 2009; van Rooij et al., 2011; Yoong, 2011). In emerging economies, financial literacy is shown to be correlated with holding basic financial products, like bank accounts, and buying insurance (OECD/INFE, 2013; Xu and Zia, 2012). Similarly, 15-year-old students with bank accounts have higher levels of financial literacy than those without, on average across the OECD countries participating in the 2012 PISA exercise (OECD, 2014c). Moreover, adults who have greater financial knowledge are more likely to accumulate more wealth (Lusardi and Mitchell, 2011).

Higher levels of financial literacy have been found to be related not only to asset building but also to debt and debt management, with more financially literate individuals opting for less costly mortgages and avoiding high interest payments and additional fees (Gerardi et al., 2010; Lusardi and Tufano, 2009a, 2009b; Moore et al., 2003).

In addition to the benefits identified for individuals, large-scale financial literacy can be expected to improve economic and financial stability for a number of reasons (OECD, 2005). Financially literate consumers can make more informed decisions and demand higher-quality services, which can, in turn, encourage competition and innovation in the market. As individuals can protect themselves to a greater extent against income or expenditure shocks and are less likely to default on credit commitments, macro-level shocks are likely to have a lower impact on financially literate populations. Financially literate consumers are also less likely to react to market conditions in unpredictable ways, less likely to make unfounded complaints and more likely to take appropriate steps to manage the risks transferred to them. All of these factors can lead to a more efficient financial services sector. They can also ultimately help to reduce government aid (and taxation) aimed at assisting those who have taken unwise financial decisions – or no decision at all.

## Box 5.1 OECD activities in relation to financial education

In 2002, the OECD initiated a far-reaching financial education project to address governments' emerging concerns about the potential consequences of low levels of financial literacy. This project is serviced by the OECD Committee on Financial Markets and the Insurance and Private Pensions Committee in coordination with other relevant bodies, including the Education Policy Committee, on issues related to schools. The project takes a holistic approach to financial-consumer issues that highlights how, alongside improved financial access, adequate consumer protection and regulatory frameworks, financial education has a complementary role to play in promoting the outcome of financial literacy.

One of the first milestones of the financial education project was the adoption of the *Recommendation on Principles and Good Practices for Financial Education and Awareness* by the OECD Council (OECD, 2005a). Alongside these recommendations, the publication, *Improving Financial Literacy: Analysis of Issues and Policies* , details the reasons for focusing on financial education, and provides a first international overview of financial education work being undertaken in various countries (OECD, 2005b). The book also includes principles and good practices for policymakers and other stakeholders seeking to improve levels of financial literacy in their country. It is complemented by a global clearinghouse on financial education, the OECD International Gateway for Financial Education ( [www.financial-education.org/home.html](http://www.financial-education.org/home.html) ), which gathers data, resources, research and news on financial education issues and programmes from around the world.

Recognising the increasingly global nature of financial literacy and education issues, in 2008 the OECD created the International Network on Financial Education (INFE) to benefit from and encompass the experience and expertise of developed and emerging economies. More than 240 public institutions from more than 110 countries and economies are members of the INFE (2015 figures). Members meet twice a year to discuss the latest developments in their country, share their expertise, and collect evidence, as well as to develop analytical and comparative studies, methodologies, good practice, policy instruments and practical guidance on key priority areas.

### Financial education for youth and in schools

The 2005 OECD Recommendation advised that “financial education should start at school. People should be educated about financial matters as early as possible in their lives” (OECD, 2005a). Two main reasons underpin the OECD recommendation: the importance of focusing on youth in order to provide them with key life skills before they start to become active financial consumers; and the relative efficiency of providing financial education in schools rather than attempting remedial actions in adulthood.

At the time the OECD Recommendation was published, there was a lack of guidance on ways to implement financial education initiatives for youth and in schools. The OECD/INFE subsequently created a dedicated expert subgroup to develop policy and practical tools. The resulting publication was welcomed by G20 leaders in September 2013 (OECD, 2014b). The publication includes guidelines for financial education in schools and guidance on financial education learning frameworks, which were also supported by the Ministers of Finance of the Asia-Pacific Economic Cooperation in August 2012.

Young people are increasingly seen as an important target group for financial education. A survey of individual financial literacy schemes supported by the European Commission (Habschick et al., 2007) found that most were directed at children and young people; and stock-taking exercises launched by the OECD/INFE demonstrated that many OECD and non-OECD countries have developed or are developing programmes in schools to varying extents (OECD 2014b; Messy and Monticone, 2016a forthcoming, 2016b).

Note: The Joint Ministerial Statement from the 2012 APEC Finance Ministerial Meeting is available at http://mddb.apec.org .

### Focus on youth

People form habits and behaviours from a young age, learning from their parents and others around them. This shows how important it is to intervene early to help shape beneficial behaviours and attitudes (Whitebread and Bingham, 2013). Young people need to understand basic financial principles and practices from an early age in order to operate within the complex financial landscape they are likely to find themselves, often before reaching adulthood. Younger generations are

not only likely to face ever-increasing complexity in financial products, services and markets, but, as noted above, they are more likely to have to bear more financial risks in adulthood than their parents. In particular, they are likely to bear more responsibility for planning their own retirement savings and investments, and covering their healthcare needs; and they will have to deal with more sophisticated and diverse financial products.

Young people may learn beneficial behaviours from their friends and family, such as prioritising their expenditure or putting money aside “for a rainy day”; but the recent changes in the financial marketplace and social welfare systems make it unlikely that they can gain adequate knowledge or information about these systems unless they work in related fields.² The majority of young people will have to apply their skills to search for information and solve problems, and know when to make informed use of professional financial advice. Efforts to improve financial knowledge in the workplace or in other settings can be severely limited by a lack of early exposure to financial education and by a lack of awareness of the benefits of continuing financial education. It is therefore important to provide early opportunities for establishing the foundations of financial literacy.

In addition to preparing young people for their adult life, financial education for youth and in schools can also address the immediate financial issues facing young people. Children are often consumers of financial services from a young age. The results of the PISA 2012 financial literacy assessment revealed that, on average across the 13 participating OECD countries and economies, almost 60% of 15-year-old students have a bank account (OECD, 2014c). Moreover, it is not uncommon for them to have accounts with access to online payment facilities or to use mobile phones (with various payment options) even before they become teenagers. Clearly, they would benefit from improved financial literacy skills. Before leaving school, they may also need to make decisions about issues such as scooter or car insurance, savings products and overdrafts.

In many countries, adolescents (around the age of 15 to 18) and their parents face one of their most important financial decisions: that is, whether or not to invest in tertiary education. The gap in wages between university-educated workers and those who had not attended university has widened in many economies (OECD, 2014a). At the same time, the education costs borne by students and their families have increased, often resulting in large student loans to repay, and potentially leading towards a reliance on credit (Bradley, 2012; OECD, 2014b; Ratcliffe and McKernan, 2013; Smithers, 2010).

## Efficiency of providing financial education in schools

Research suggests that there is a link between financial literacy and a family’s economic and educational background: those who are more financially literate disproportionately come from highly educated families that hold a wide range of financial products (Lusardi et al., 2010). Results of the 2012 PISA financial literacy assessment show that, on average across OECD countries and economies, 14% of the variation in student performance in financial literacy within each country and economy is associated with the student’s socio-economic status, and that students with at least one parent who has tertiary-level education score higher, on average, than other students (OECD, 2014c). In order to provide equal opportunities to all students, it is important to offer financial education to those who would not otherwise have access to it. Schools are well-positioned to advance financial literacy among all demographic groups and reduce gaps and inequalities in financial literacy, including across generations.

Recognising both the importance of financial literacy for youth and the unique potential to create more skilled and knowledgeable future generations, an increasing number of countries have begun to develop financial education programmes for children and young people. These are either dedicated to youth generally or to (some part of) the school population, and include programmes at the national, regional and local levels as well as pilot exercises.

### The need for data

Policy makers, educators and researchers need high-quality data on levels of financial literacy in order to inform financial education strategies and the implementation of financial education programmes in schools by identifying priorities and measuring change across time.

Several countries have undertaken national surveys of financial literacy across their adult population; and the OECD has developed a questionnaire designed to capture levels of financial literacy among adults at an international level, which was first piloted in 2010 and is now being used for a second international comparative study (Atkinson and Messy, 2012; OECD/INFE, 2011; OECD/INFE, 2015a). However, until financial literacy was included in the 2012 PISA assessment, there were few efforts to collect data on the levels of financial literacy among young people under the age of 18, and none that could be compared across countries.

## Measuring financial literacy in PISA

A robust measure of financial literacy among young people provides information at the national level that can indicate whether the current approach to financial education is effective. In particular, it can help to identify issues that need addressing through schools or extracurricular activities or programmes that will enable young people to be properly and equitably equipped to make financial decisions in adulthood. Such a measure can also be used as a baseline against which the success of school and other programmes can be assessed and reviewed in the future.

An international study provides additional benefits to policy makers and other stakeholders. Comparing levels of financial literacy across countries makes it possible to see which countries have the highest levels of financial literacy and begin to identify particularly effective national strategies and good practices. It also makes it possible to recognise common challenges and explore the possibility of finding international solutions to the issues faced.

Thus, collecting robust and internationally comparable financial literacy data in the student population provides policy makers, educators, curriculum and resource developers, researchers and others with:

- international evidence on how young people are distributed across the financial literacy proficiency scale, which can be used to develop more targeted programmes and policies
- an opportunity to compare financial education strategies across countries and explore good practice
- comparable data over time to track trends in financial literacy and potentially assess the association between financial literacy and the availability of financial education in schools.

In addition, developing a financial literacy assessment framework that is applicable across countries provides national authorities with detailed guidance about the scope and operational definition of financial literacy without having to fund national studies. As noted in the article, “Financial Literacy and Education Research Priorities”, there has been a gap in the research on financial literacy related to the lack of consistency in defining and measuring programme success. There is a need for researchers to develop a clear understanding of what it means to be ‘financially educated’ (Schuchardt et al., 2009).

### Measuring financial literacy in PISA

PISA 2012 was the first large-scale international study to assess the financial literacy of young people. PISA assesses the readiness of students for their life beyond compulsory schooling – and, in particular, their capacity to use knowledge and skills – by collecting and analysing cognitive and other information from 15-year-olds in countries and economies.

PISA financial literacy data provides a rich set of comparative data that policy makers and other stakeholders can use to make evidence-based decisions. International comparative data on financial literacy can answer questions such as, “How well are young people prepared for the new financial systems that are becoming more global and more complex?” and “In which countries/economies do students show high levels of financial literacy?”.

As with the core PISA domains of reading, mathematics and science, the main focus of the financial literacy assessment in PISA is on measuring the proficiency of 15-year-old students in demonstrating and applying knowledge and skills. And like the other PISA domains, financial literacy is assessed using an instrument designed to provide data that are valid, reliable and comparable.

The PISA financial literacy assessment framework developed in 2012 (OECD, 2013a) provided the first step in constructing an assessment that satisfies these three broad criteria. The main benefit of constructing an assessment framework is improved measurement, as it provides an articulated plan for developing the individual items and designing the instrument that will be used to assess the domain. A further benefit is that it provides a common language for discussion of the domain, thereby improving understanding of what is being measured. It also promotes an analysis of the kinds of knowledge and skills associated with competency in the domain, thus providing the groundwork for building a described proficiency scale or scales that can be used to interpret the results.

The development of the PISA frameworks can be described as a sequence of the following six steps:

- Develop a definition for the domain and a description of the assumptions that underlie that definition.
- Identify a set of key characteristics that should be taken into account when constructing assessment tasks for international use.
- Operationalise the set of key characteristics that will be used in test construction, with definitions based on existing literature and experience in conducting other large-scale assessments.

- Evaluate how to organise the set of tasks constructed in order to report to policy makers and researchers on achievement in each assessment domain for 15-year-old students in participating countries.
- Validate the variables and assess the contribution each makes to understanding task difficulty across the various participating countries.
- Prepare a described proficiency scale for the results.

The 2015 framework maintains the definition of the domain used in 2012 while updating the operationalisation of the domain to ensure that it is in line with recent developments in financial markets and the latest research findings.

### DEFINING FINANCIAL LITERACY

In developing a working definition of financial literacy that can be used as the basis for designing an international financial literacy assessment, the Financial Literacy Expert Group (FEG) looked both to existing PISA domain definitions of literacies and to articulations of the nature of financial education.

PISA conceives of literacy as students' capacity to apply knowledge and skills in key subject areas and to analyse, reason and communicate effectively as they pose, solve and interpret problems in a variety of situations. PISA is forward-looking, focusing on young people's ability to use their knowledge and skills to meet real-life challenges, rather than merely on the extent to which they have mastered specific curricular content (OECD, 2010a).

In its *Recommendation on Principles and Good Practices for Financial Education and Awareness* , the OECD defined financial education as “the process by which financial consumers/investors improve their understanding of financial products, concepts and risks and, through information, instruction and/or objective advice, develop the skills and confidence to become more aware of financial risks and opportunities, to make informed choices, to know where to go for help, and to take other effective actions to improve their financial well-being” (OECD, 2005a).

The FEG agreed that “understanding”, “skills” and the notion of applying understanding and skills (“effective actions”) were key elements of this definition. It was recognised, however, that the definition of financial education describes a process – education – rather than an outcome. What was required for the assessment framework was a definition encapsulating the outcome of that process in terms of competency or literacy.

The definition of financial literacy for PISA is as shown in Box 5.2.

#### Box 5.2 The 2015 definition of scientific literacy

Financial literacy is knowledge and understanding of financial concepts and risks, and the skills, motivation and confidence to apply such knowledge and understanding in order to make effective decisions across a range of financial contexts, to improve the financial well-being of individuals and society, and to enable participation in economic life.

This definition, like other PISA domain definitions, has two parts. The first part refers to the kind of thinking and behaviour that characterises the domain. The second part refers to the purposes for developing the particular literacy.

In the following paragraphs, each part of the definition of financial literacy is considered in turn to help clarify its meaning in relation to the assessment.

##### Financial literacy...

Literacy is viewed as an expanding set of knowledge, skills and strategies, which individuals build on throughout life, rather than as a fixed quantity, a line to be crossed, with illiteracy on one side and literacy on the other. Literacy involves more than the reproduction of accumulated knowledge, although measuring prior financial knowledge is an important element in the assessment. It also involves a mobilisation of cognitive and practical skills, and other resources, such as attitudes, motivation and values. The PISA assessment of financial literacy draws on a range of knowledge and skills associated with the capacity to deal with the financial demands of everyday life and uncertain futures within contemporary society.

##### ...is knowledge and understanding of financial concepts and risks...

Financial literacy is thus contingent on some knowledge and understanding of fundamental elements of the financial world, including key financial concepts as well as the purpose and basic features of financial products. This also includes risks that may threaten financial well-being as well as insurance policies and pensions. It can be assumed that 15-year-olds

...are beginning to acquire this knowledge and gain experience of the financial environment that they and their families inhabit and the main risks they face. All of them are likely to have been shopping to buy household goods or personal items; some will have taken part in family discussions about money and whether what is wanted is actually needed or affordable; and a sizeable proportion of students will have already begun to earn and save money. Some students already have experience of financial products and commitments through a bank account or a mobile phone contract. A grasp of concepts such as interest, inflation, and value for money are soon going to be, if they are not already, important for their financial well-being.

...and the skills,...

These skills include generic cognitive processes, such as accessing information, comparing and contrasting, extrapolating and evaluating, applied in a financial context. They include basic skills in mathematical literacy, such as the ability to calculate a percentage, undertake basic mathematical operations or convert from one currency to another, and language skills, such as the capacity to read and interpret advertising and contractual texts.

...motivation and confidence...

Financial literacy involves not only the knowledge, understanding and skills to deal with financial issues, but also non-cognitive attributes: the motivation to seek information and advice in order to engage in financial activities, the confidence to do so, and the ability to manage emotional and psychological factors that influence financial decision making. These attributes are considered as a goal of financial education, as well as being instrumental in building financial knowledge and skills.

...to apply such knowledge and understanding in order to make effective decisions...

PISA focuses on the ability to activate and apply knowledge and understanding in real-life situations rather than on the ability to reproduce knowledge. In assessing financial literacy, this translates into a measure of young people’s ability to transfer and apply what they have learned about personal finance into effective decision making. The term “effective decisions” refers to informed and responsible decisions that satisfy a given need.

...across a range of financial contexts...

Effective financial decisions apply to a range of financial contexts that relate to young people’s present daily life and experience, but also to steps they are likely to take in the near future as adults. For example, young people may currently make relatively simple decisions such as how they will use their pocket money or, at most, which mobile phone contract they will choose; but they may soon be faced with major decisions about education and work options with long-term financial consequences.

...to improve the financial well-being of individuals and society...

Financial literacy in PISA is primarily conceived of as literacy around personal or household finance, distinguished from economic literacy, which includes concepts such as the theories of demand and supply, market structures and so on. Financial literacy is concerned with the way individuals understand, manage and plan their own and their households’ – which often means their families’ – financial affairs. It is recognised, however, that good financial understanding, management and planning on the part of individuals has some collective impact on the wider society, in contributing to national and even global stability, productivity and development.

...and to enable participation in economic life.

Like the other PISA literacy definitions, the definition of financial literacy implies the importance of the individual’s role as a thoughtful and engaged member of society. Individuals with a high level of financial literacy are better equipped to make decisions that are of benefit to themselves, and also to constructively support and critique the economic world in which they live.

## ORGANISING THE DOMAIN OF FINANCIAL LITERACY

How the domain is represented and organised determines the assessment design, including item development and, ultimately, the evidence about student proficiencies that can be collected and reported. Many elements are part of the concept of financial literacy, not all of which can be taken into account in an assessment like PISA. It is necessary to select the elements that will best ensure construction of an assessment comprising tasks with an appropriate range of difficulty and a broad coverage of the domain.

A review of approaches and rationales adopted in previous large-scale studies, and particularly in PISA, shows that most consider the relevant content, processes and contexts for assessment as they specify what they wish to assess. Content, processes and contexts can be thought of as three different perspectives on the area to be assessed.

- **Content** comprises the knowledge and understanding that are essential in the area of literacy in question.
- **Processes** describes the mental strategies or approaches that are called upon to negotiate the material.
- **Contexts** refers to the situations in which the domain knowledge, skills and understandings are applied, ranging from the personal to the global.

To construct the assessment, the different categories within each perspective are identified and weighted, and then a set of tasks is developed to reflect these categories. The three perspectives are also helpful in thinking about how achievement in the area is to be reported.

The following section examines each of the three perspectives and the framework categories into which they are divided. Examples of items drawn from the PISA 2012 field trial to illustrate these three different perspectives are available in the PISA 2012 Assessment and Analytical Framework (OECD, 2013b) and on the PISA website (www.oecd.org/pisa/). While they are representative of those used in the main survey, these particular items are not used in the assessment instrument; only secure, unpublished items are used to protect the integrity of the data that is collected to measure student proficiency.

## Content

The content of financial literacy is conceived of as the areas of knowledge and understanding that must be drawn upon in order to perform a particular task. A review of the content of existing financial literacy learning frameworks from Australia, Brazil, England, Japan, Malaysia, the Netherlands, New Zealand, Northern Ireland, Scotland, South Africa and the United States indicates that there is some consensus on financial literacy content areas (OECD, 2014b). The review shows that the content of financial education in schools was similar, albeit with some cultural differences, and that it was possible to identify a series of topics commonly included in these frameworks. These topics form the four content areas of the PISA financial literacy assessment: *money and transactions* , *planning and managing finances* , *risk and reward* , and *financial landscape* . Further work undertaken by the OECD/INFE to develop a core-competencies framework on financial literacy for young people provides additional guidance on how these content areas are mapped to desired outcomes (OECD/INFE, 2015c).

### Money and transactions

This content area includes awareness of the different forms and purposes of money and managing monetary transactions, which may include spending or making payments, taking into account value for money, and using bank cards, cheques, bank accounts and currencies. It also covers practices such as taking care of cash and other valuables, calculating value for money, and filing documents and receipts.

Tasks in this content area can ask students to show that they are:

- Aware of the different forms and purposes of money. Students can:
    - Recognise bank notes and coins.
    - Understand that money can be exchanged for goods and services.
    - Identify different ways to pay for items purchased in person or at a distance (from a catalogue or on line, for example).
    - Recognise that there are various ways of receiving money from other people and transferring money between people or organisations, such as cash, cheques, card payments in person or on line, or electronic transfers on line or via SMS.
    - Understand that money can be borrowed or lent, and the purpose of interest (taking into account that the payment and receipt of interest is forbidden in some religions).
- Confident and capable of handling and monitoring transactions. Students can:
    - Use cash, cards and other payment methods to purchase items.
    - Use cash machines to withdraw cash or to get an account balance.
    - Calculate the correct change.
    - Work out which of two consumer items of different sizes would give better value for money, taking into account the individual’s specific needs and circumstances.
    - Check transactions listed on a bank statement and note any irregularities.

## Planning and managing finances

Income, expenditure and wealth need planning and managing over both the short term and long term. This content area reflects the process of managing, planning and monitoring income and expenses and understanding ways of enhancing wealth and financial well-being. It includes content related to credit use as well as savings and wealth creation.

Tasks in this content area can ask students to show that they know about and can:

- Monitor and control income and expenses. Students can:
    - Identify various types of income (e.g. allowances, salary, commission, benefits,) and ways of discussing income (such as hourly wage and gross or net annual income).
    - Draw up a budget to plan regular spending and saving and live within it.
- Use income and other available resources in the short and long term to enhance financial well-being. Students can:
    - Understand how to manipulate various elements of a budget, such as identifying priorities if income does not meet planned expenses, or finding ways to increase savings, such as reducing expenses or increasing income.
    - Assess the impact of different spending plans and be able to set spending priorities in the short and long term.
    - Plan ahead to pay future expenses: for example, working out how much money needs to be saved each month to make a particular purchase or pay a bill.
    - Understand the purposes of accessing credit and the ways in which expenditure can be smoothed over time through borrowing or saving.
    - Understand the idea of building wealth, the impact of compound interest on savings, and the pros and cons of investment products.
    - Understand the benefits of saving for long-term goals or anticipated changes in circumstance, such as living independently.
    - Understand how government taxes and benefits affect personal and household finances.

### Risk and reward

Risk and reward is a key area of financial literacy. It incorporates the ability to identify ways of balancing and covering risks and managing finances in uncertainty with an understanding of the potential for financial gains or losses across a range of financial contexts. There are two types of risk of particular importance in this domain. The first relates to financial losses that an individual cannot bear, such as those caused by catastrophic or repeated costs. The second is the risk inherent in financial products, such as credit agreements with variable interest rates, or investment products. This content area therefore includes knowledge of the types of products that may help people to protect themselves from the consequences of negative outcomes, such as insurance and savings, as well as the ability to assess the level of risk and reward related to different products, purchases, behaviours or external factors.

Tasks in this content area can ask students to show that they:

- Recognise that certain financial products, including insurance, and processes, such as saving, can be used to manage and offset various risks, depending on different needs and circumstances. Students know how to assess whether certain insurance policies may be of benefit.
- Understand the benefits of contingency planning, diversification and the dangers of default on payment of bills and credit agreements. Students can apply this knowledge to decisions about:
    - limiting the risk to personal capital
    - various types of investment and savings vehicles, including formal financial products and insurance products, where relevant
    - various forms of credit, including informal and formal credit, unsecured and secured, rotating and fixed term, and those with fixed or variable interest rates
- Know about and can manage risks and rewards associated with life events, the economy and other external factors, such as the potential impact of:
    - theft or loss of personal items, job loss, birth or adoption of a child, deteriorating health or mobility
    - fluctuations in interest rates and exchange rates
    - other market changes

- Know about the risks and rewards associated with substitutes for financial products, particularly:
    - saving in cash, or buying property, livestock or gold as a store of wealth
    - taking credit or borrowing money from informal lenders
- Know that there may be unidentified risks and rewards associated with new financial products (examples may include innovative digital finance or “crowd funding”, but, by definition, such a list will change over time).

## Financial landscape

This content area relates to the character and features of the financial world. It covers awareness of the role of regulation and consumer protection, knowing the rights and responsibilities of consumers in the financial marketplace and within the general financial environment, and the main implications of financial contracts. Information resources are also topics relevant to this content area. In its broadest sense, financial landscape also incorporates an understanding of the consequences of changes in economic conditions and public policies, such as changes in interest rates, inflation, taxation or welfare benefits for individuals, households and society.

Tasks in this content area can ask students to show that they:

- Are aware of the role of regulation and consumer protection.
- Know about rights and responsibilities. Students can:
    - Understand that buyers and sellers have rights, such as being able to apply for redress.
    - Understand that buyers and sellers have responsibilities, such as giving accurate information when applying for financial products (consumers and investors), disclosing all material facts (providers); and being aware of the implications of one of the parties not doing so (consumers and investors).
    - Recognise the importance of the legal documentation provided when purchasing financial products or services and the importance of understanding the content.
- Know and understand the financial environment. Students:
    - Can identify which providers are trustworthy, and which products and services are protected through regulation or consumer-protection laws.
    - Can identify whom to ask for advice when choosing financial products, and where to go for help or guidance in relation to financial matters.
    - Are aware of existing financial crimes, such as identity theft and scams, knowledge of how to take appropriate precautions to protect personal data and avoid other scams, and knowledge of their rights and responsibilities in the event that they are a victim.
    - Are aware of the potential for new forms of financial crime and awareness of the risks.
- Know and understand the impact of their own financial decisions on themselves and others. Students:
    - Understand that individuals have choices in spending and saving, and each action can have consequences for the individual and for society.
    - Recognise how personal financial habits, actions and decisions have an impact at an individual, community, national and international level.
- Understand the influence of economic and external factors. Students:
    - Are aware of the economic climate and understand the impact of policy changes, such as reforms related to the funding of post-school training or compulsory savings for retirement.
    - Understand how the ability to build wealth or access credit depends on economic factors, such as interest rates, inflation and credit scores.
    - Understand that a range of external factors, such as advertising and peer pressure, can affect individuals’ financial choices and outcomes.

## Processes

The process categories relate to cognitive processes. They are used to describe students’ ability to recognise and apply concepts relevant to the domain, and to understand, analyse, reason about, evaluate and suggest solutions. PISA defines four process categories for financial literacy: identify financial information, analyse information in a financial context,

## Identify financial information

This process is engaged when the individual searches and accesses sources of financial information, and identifies or recognises its relevance. In PISA 2015 the information is in the form of texts, such as contracts, advertisements, charts, tables, forms and instructions displayed on a screen. A typical task might ask students to identify the features of a purchase invoice, or recognise the balance on a bank statement. A more difficult task might involve searching through a contract that uses complex legal language to locate information that explains the consequences of defaulting on loan repayments. This process category is also reflected in tasks that involve recognising financial terminology, such as identifying “inflation” as the term used to describe increasing prices over time.

## Analyse information in a financial context

This process covers a wide range of cognitive activities undertaken in financial contexts, including interpreting, comparing and contrasting, synthesising, and extrapolating from information that is provided. Essentially, it involves recognising something that is not explicit: identifying the underlying assumptions or implications of an issue in a financial context. For example, a task may involve comparing the terms offered by different mobile phone contracts, or working out whether an advertisement for a loan is likely to include unstated conditions. An example in this process category is provided below, in the unit SHARES.

## Evaluate financial issues

In this process, the focus is on recognising or constructing financial justifications and explanations, drawing on financial knowledge and understanding applied in specified contexts. It involves such cognitive activities as explaining, assessing and generalising. Critical thinking is brought into play in this process, when students must draw on knowledge, logic and plausible reasoning to make sense of and form a view about a finance-related problem. The information that is required to deal with such a problem may be partly provided in the stimulus of the task, but students will need to connect such information with their own prior financial knowledge and understanding.

In the PISA context, any information that is required to understand the problem is intended to be within the expected range of experiences of a 15-year-old – either direct experiences or those that can be readily imagined and understood. For example, it is assumed that 15-year-olds are likely to be able to identify with the experience of wanting something that is not essential, such as a music player or games console. A task based on this scenario could ask about the factors that might be considered in determining the relative financial merits of making a purchase or deferring it, given specified financial circumstances.

## Apply financial knowledge and understanding

The fourth process picks up a term from the definition of financial literacy: “to apply such [financial] knowledge and understanding”. It focuses on taking effective action in a financial setting by using knowledge of financial products and contexts, and understanding of financial concepts. This process is reflected in tasks that involve performing calculations and solving problems, often taking into account multiple conditions. An example of this kind of task is calculating the interest on a loan over two years. This process is also reflected in tasks that require recognition of the relevance of prior knowledge in a specific context. For example, a task might require the student to work out whether purchasing power will decline or increase over time when prices are changing at a given rate. In this case, knowledge about inflation needs to be applied.

## Contexts

Decisions about financial issues are often dependent on the contexts or situations in which they are presented. By situating tasks in a variety of contexts, the assessment offers the possibility of connecting with the broadest possible range of individual interests across a variety of situations in which individuals need to function in the 21st century.

Certain situations will be more familiar to 15-year-olds than others. In PISA, assessment tasks are framed in situations of general life, which may include but are not confined to school contexts. The focus may be on the individual, family or peer group, on the wider community, or even on a global scale.

As a starting point, the Financial Literacy Expert Group (FEG) looked at the contexts used in the Programme for the International Assessment of Adult Competencies (PIAAC) literacy framework: *education and work, home and family, leisure and recreation* , and *community and citizenship* (PIAAC Literacy Expert Group, 2009). For the purposes of the financial literacy domain, the heading *leisure and recreation* was replaced by *individual* to reflect the fact that many of the financial interactions that young people have are related to themselves as individual consumers. Such interactions may include leisure and recreation, but are not limited to these. It was further decided to replace *community and citizenship* with *societal* . While *community and citizenship* captures the idea of a perspective wider than the personal, it was felt that the term *community* was not wide enough.

*Societal* , by contrast, implicitly encompasses national and global situations as well as the more local, thus better fitting the potential reach of financial literacy. The contexts identified for the PISA financial literacy assessment are, then, *education and work, home and family, individual* and *societal* .

## Education and work

The context of *education and work* is of great importance to young people. Virtually all 15-year-olds will be starting to think about financial matters related to both education and work, whether they are spending existing earnings, considering future education options or planning their working life.

The educational context is obviously relevant to PISA students, since they are, by definition, a sample of the school-based population; indeed, many of them will continue in education or training for some time. However, many 15-year-old students are also already engaged in some form of paid work outside school hours making the work context equally valid. In addition, many will move from education into some form of employment, including self-employment, before reaching their 20s.

Typical tasks within this context could include understanding payslips, planning to save for tertiary education, investigating the benefits and risks of taking out a student loan, and participating in workplace savings schemes.

## Home and family

*Home and family* includes financial issues relating to the costs involved in running a household. Family is the most likely household circumstance for 15-year-olds; however, this category also encompasses households that are not based on family relationships, such as the kind of shared accommodation that young people often use shortly after leaving the family home. Tasks within this context may include buying household items or family groceries, keeping records of family spending, and making plans for family events. Decisions about budgeting and prioritising spending may also be framed within this context.

## Individual

The context of the *individual* is important within personal finance since there are many decisions that a person takes entirely for personal benefit or gratification, and many risks and responsibilities that must be borne by individuals. These decisions span essential personal needs as well as leisure and recreation. They include choosing personal products and services, such as clothing, toiletries or haircuts, buying consumer goods, such as electronic or sports equipment, and more long-term commitments, such as season tickets or a gym membership. They also cover the process of making personal decisions and the importance of ensuring individual financial security, such as keeping personal information safe and being cautious about unfamiliar products.

Although the decisions made by an individual may be influenced by the family and society (and may ultimately affect society), when it comes to opening a bank account, buying shares or getting a loan, it is typically the individual who has the legal responsibility and ownership. The context *individual* therefore includes contractual issues around events such as opening a bank account, purchasing consumer goods, paying for recreational activities, and dealing with relevant financial services that are often associated with larger consumption items, such as credit and insurance.

## Societal

The environment young people are living in is characterised by change, complexity and interdependence. Globalisation is creating new forms of interdependence where actions are subject to economic influences and consequences that

stretch well beyond the individual and the local community. While the core of the financial literacy domain is focused on personal finances, the societal context recognises that individual financial well-being cannot be entirely separated from the rest of society. Personal financial well-being affects and is affected by the local community, the nation and even global activities. Financial literacy within this context includes such matters as being informed about consumer rights and responsibilities, understanding the purpose of taxes and local government charges, being aware of business interests, and taking into account the role of consumer purchasing power. It also extends to considering financial choices, such as donating to non-profit organisations and charities.

## Non-cognitive factors

The PISA working definition of financial literacy includes the non-cognitive terms motivation and confidence, attitudes which, according to some, have an influence on money-management behaviour (Johnson and Staten, 2010). PISA conceives of both financial attitudes and behaviour as aspects of financial literacy in their own right. Attitudes and behaviour are also of interest in terms of their interactions with the cognitive elements of financial literacy. Information collected about the financial attitudes and behaviour of 15-year-olds could also constitute useful baseline data for any longitudinal study of the financial literacy of adults, including their financial behaviours.

The Financial Literacy Expert Group identified four non-cognitive factors to include in the framework: access to information and education, access to money and financial products, attitudes towards and confidence about financial matters, and spending and saving behaviour.

### Access to information and education

There are various sources of financial information and education that may be available to students, including informal discussion with friends, parents or other family members, information from the financial sector, as well as formal school education. The literature in this area often refers to the process of “financial socialisation”, which can be seen as the process of acquiring financial literacy. Parents have a major role in the financial socialisation of children but, as discussed above, they may not have been exposed to all the financial contexts and decisions that their children face (Gudmondson and Danes, 2011; Otto, 2013). Copying and discussing financial behaviours with friends can be another important source of socialisation, but this also may vary in terms of quality and reliability, with recent research from the UK indicating that money is rarely talked about honestly (Money Advice Service, 2014). In addition, the amount and quality of formal education and training about money and personal finance received by students varies within and across countries (OECD, 2014b, 2014c).

Data about students’ access to financial information and education can be collected through both the student questionnaire and the questionnaire for school principals. In the student questionnaire, students can be asked about their typical sources of information in order to analyse the extent to which each source is correlated with financial literacy. This is intended to provide a description of students’ main sources of financial socialisation, rather than assessing whether they understand the importance of using appropriate sources of information or advice, which is covered in the cognitive assessment. Students can also be asked about the types of tasks that they face and the financial concepts they are exposed to during curricular classes. The school questionnaire can ask principals about the availability and quality of financial education in their schools. Evidence about the extent to which there is a link between levels of financial literacy and financial education inside and outside schools is likely to be particularly useful in shaping education programmes for improving financial literacy.

### Access to money and financial products

The results of the 2012 PISA financial literacy exercise showed that, in the Flemish Community of Belgium, Estonia, New Zealand and Slovenia, students with a bank account scored higher in financial literacy than students with similar socio-economic status who did not hold a bank account (OECD, 2014c). While this does not indicate a causal relationship, it is plausible to assume that real-life experiences of financial products may influence young people’s financial literacy and vice versa. Personal experience may come, for example, from using financial products, such as payment cards, from dealing with the banking system, or from occasional working activities outside of school hours.

Students who have had more personal experience in dealing with financial matters from earning money or receiving an allowance might also be expected to perform better on the cognitive assessment than those without such experience. However, a recent review suggests that the key factor may not be experience, but the extent to which parents are involved in the spending decisions made by young people, with higher financial literacy associated with more involved parents (Drever et al., 2015). The 2015 framework recognises the importance of knowing whether students have access to money and financial products.

## Attitudes towards and confidence about financial matters

The PISA definition of financial literacy highlights the important role of attitudes. Individual preferences can determine financial behaviour and affect the ways in which financial knowledge is used. PISA 2012 showed that students' perseverance and openness to problem solving were strongly associated with their financial literacy scores (OECD, 2014c). In addition, the extent to which students believe that they are in control of their future, and their preference for current consumption may influence their financial decisions, their independence, and their propensity to learn how to make plans for their own financial security (Golsteyn et al., 2013; Lee and Mortimer, 2009; Meier and Sprenger, 2013).

Confidence in one's own ability to make a financial decision may also be a key driver in explaining who will work through complex financial problems or make choices across several possible products. At the same time, however, confidence may turn into overconfidence, leading to a tendency to mistakes and overly risky decisions. The 2015 framework recognises the importance of a student's perception of his or her own financial knowledge and skills.

## Spending and saving behaviour

While items in the cognitive assessment test students' ability to make particular spending and savings decisions, it is also useful to have some measure of what their actual (reported) behaviour is: that is, how students save and spend in practice. The PISA financial literacy assessment provides the opportunity to look at the potential relationship between 15-year-olds' spending and saving behaviour and their results on the cognitive financial literacy assessment.

## ASSESSING FINANCIAL LITERACY

### The structure of the assessment

In 2012, the PISA financial literacy assessment was developed as a one-hour, paper-based exercise to be completed in addition to one hour of material from other cognitive domains. The financial literacy assessment was composed of 40 items divided into two clusters, chosen from 75 tasks that were used in the field trial. The selection of items was made based on their psychometric properties, such as ensuring that each item distinguished between high- and low-scoring students.

In 2015, items are transferred to a computer-based platform. Additional items were developed for this form of delivery in order to replace items that had been released in the report of the 2012 results. The 2015 financial literacy assessment was developed as a one-hour exercise, comprising 43 items divided into two clusters.

As with other PISA assessment domains, computer-based financial literacy items are grouped in units composed of one or two items based around a common stimulus. The selection includes financially-focused stimulus material in diverse formats, including prose, diagrams, tables, charts and illustrations. All financial literacy assessments include a broad sample of items covering a range of difficulty that allows for measuring and describing the strengths and weaknesses of students and key subgroups of students.

### Response formats and coding

Some PISA items require short descriptive responses; others require more direct responses of one or two sentences or a calculation, while some can be answered by checking a box. Decisions about the form in which the data are collected – the response formats of the items – are based on what is considered appropriate given the kind of evidence that is being collected, and also on technical and pragmatic considerations. In the financial literacy assessment as in other PISA assessments, two broad types of items are used: constructed-response items and selected-response items.

Constructed-response items require students to generate their own answers. The format of the answer may be a single word or figure, or may be longer – a few sentences or a worked calculation. Constructed-response items that require a more extended answer are ideal for collecting information about students' capacity to explain decisions or demonstrate a process of analysis.

The second broad type of item in terms of format and coding is selected response. This kind of item requires students to choose one or more alternatives from a given set of options. The most common type in this category is the simple multiple-choice item, which requires the selection of one from a set of (usually) four options.

A second type of selected-response item is complex multiple choice, in which students respond to a series of “Yes/No”-type questions. Selected-response items are typically regarded as most suitable for assessing items associated with identifying and recognising information, but they are also a useful way of measuring students’ understanding of higher-order concepts that they themselves may not easily be able to express.

Although particular item formats lend themselves to specific types of questions, the format of the item should not affect the interpretation of the results. Research suggests that different groups (for example, boys and girls, and students in different countries) respond differentially to the various item formats. Several research studies on response-format effects based on PISA data suggest that there are strong arguments for retaining a mixture of multiple-choice and constructed-response items. In their study of PISA reading literacy compared with the IEA Reading Literacy Study (IEARLS), Lafontaine and Monseur (2006) found that response format had a significant impact on gender performance. In another study, countries were found to show differential equivalence of item difficulties in PISA reading on items in different formats (Grisay and Monseur, 2007). This finding may relate to the fact that students in different countries are more or less familiar with the particular formats. The PISA financial literacy option includes items in a variety of formats to minimise the possibility that item format affects student performance. Such an influence would be extrinsic to the intended object of measurement – in this case, financial literacy.

When considering the distribution of item formats, the question of resources must be weighed as well as the equity issues discussed in the preceding paragraphs. All except the most simple of constructed-response items are coded by expert judges who must be trained and monitored. Selected-response and short “closed” constructed-response items do not require expert coding and therefore demand fewer resources.

The proportions of constructed- and selected-response items are determined taking of all these considerations into account. Most of the items selected for the PISA 2015 main survey do not require expert judgement.

Most items are coded dichotomously (full credit or no credit), but where appropriate an item’s coding scheme allows for partial credit. Partial credit makes possible more nuanced scoring of items. Some answers, even though incomplete, are better than others. If incomplete answers for a particular question indicate a higher level of financial literacy than inaccurate or incorrect answers, a scoring scheme has been devised that allows partial credit for that question. Such “partial credit” items yield more than one score point.

### Distribution of score points

While each PISA financial literacy item is categorised according to a single content, a single process and a single context category, it is recognised that, since PISA aims to reflect real-life situations and problems, often elements of more than one category are present in a task. In such cases, the item is identified with the category judged most integral to responding successfully to the task.

The target distribution of score points according to financial literacy content areas is shown in Table 5.1. The term “score points” is used in preference to “items”, as some partial credit items are included. The distributions are expressed in terms of ranges, indicating the approximate weighting of the various categories. They contain a mix of original items, developed for the 2012 and 2015 assessments.

The distribution reflects that money and transactions is considered to be to the most immediately relevant content area for 15-year-olds.

Table 5.2 shows the target distribution of score points among the four processes.

The weighting shows that greater importance is attributed to evaluating financial issues and applying financial knowledge and understanding.

Table 5.3 shows the target distribution of score points among the four contexts.

#### Table 5.1 Approximate distribution of score points in financial literacy, by content

| Money and transactions   | Planning and managing finances   | Risk and reward   | Financial landscape   | Total   |
|--------------------------|----------------------------------|-------------------|-----------------------|---------|
| 30% - 40%                | 25% - 35%                        | 15% - 25%         | 10% - 20%             | 100%    |

#### Table 5.2 Approximate distribution of score points in financial literacy, by process

| Identify financial information   | Analyse information in a financial context   | Evaluate financial issues   | Apply financial knowledge and understanding   | Total   |
|----------------------------------|----------------------------------------------|-----------------------------|-----------------------------------------------|---------|
| 15% - 25%                        | 15% - 25%                                    | 25% - 35%                   | 25% - 35%                                     | 100%    |

#### Table 5.3 Approximate distribution of score points in financial literacy, by context

| Education and work   | Home and family   | Individual   | Societal   | Total   |
|----------------------|-------------------|--------------|------------|---------|
| 10% - 20%            | 30% - 40%         | 35% - 45%    | 5% - 15%   | 100%    |

Consistent with an assessment of personal financial literacy of 15-year-olds, there is a clear emphasis on individual, but also a weighting towards the financial interests of the household or family unit. Education and work and societal contexts are given less emphasis, but included in the scheme as they are important elements of financial experience.

## THE IMPACT OF OTHER DOMAIN KNOWLEDGE AND SKILLS ON FINANCIAL LITERACY

A certain level of numeracy (or mathematical literacy) is regarded as a prerequisite for financial literacy. Huston (2010) argues that “if an individual struggles with arithmetic skills, this will certainly impact his/her financial literacy. However, available tools (e.g. calculators) can compensate for these deficiencies; thus, information directly related to successfully navigating personal finances is a more appropriate focus than numeracy skills for a financial literacy measure”. Mathematically-related proficiencies, such as number sense, familiarity with multiple representations of numbers, and skills in mental calculation, estimation and the assessment of reasonableness of results, are intrinsic to some aspects of financial literacy.

On the other hand, there are large areas where the content of mathematical literacy and financial literacy do not intersect. As defined in the PISA 2012 mathematics literacy framework, mathematical literacy incorporates four content areas: change and relationships, space and shape, quantity and uncertainty. Of these, only quantity directly intersects with the content of the PISA financial literacy assessment. Unlike the mathematical literacy content area uncertainty, which requires students to apply probability measures and statistics, the financial literacy content area risk and reward requires an understanding of the features of a particular situation or product that indicate a that there will be a risk of losing money and (sometimes) a possibility of gains. This is a non-numeric appreciation of the way financial well-being can be affected by chance and an awareness of the related products and actions to protect against loss.

In the financial literacy assessment, the quantity-related proficiencies described above are applied to problems requiring more financial knowledge than can be expected in the mathematical literacy assessment. Similarly, knowledge about financial matters and the ability to apply such knowledge and reasoning in financial contexts (in the absence of any specifically mathematical content) characterise much of all four content areas of financial literacy: money and transactions, planning and managing finances, risk and reward and financial landscape. Figure 5.1 represents the relationship between the content of mathematical literacy and financial literacy in PISA.

### Figure 5.1 - Relationship between the content of financial literacy and mathematical literacy in PISA

<!-- image -->

Relationship between the content of financial literacy and mathematical literacy in PISA

**Mathematical literacy**

- Content related to change and relationships, space and shape, uncertainty, and quantity not related to finance
- Content assessed by items that require that students solve mathematical literacy problems in a familiar financial context where the underlying mathematics is not explicit

**Financial literacy**

- Content assessed by a number of basic arithmetic items that require students to apply knowledge in an everyday financial context
- Content assessed by items that require some number sense and require students to apply more specialised financial knowledge. Any mathematics is minimal and explicit
- Financial literacy content (excluding arithmetic processing) related to money and transactions, planning and managing finances, risk and reward and financial landscape

## REPORTING FINANCIAL LITERACY

Operationally, there are few items populating the portion of the diagram where the two circles intersect. In the financial literacy assessment, the nature of the mathematical literacy expected is basic arithmetic: the four operations (addition, subtraction, multiplication and division) with whole numbers, decimals and common percentages. Such arithmetic occurs as an intrinsic part of the financial literacy context and enables financial literacy knowledge to be applied and demonstrated.

Items that require such arithmetic skills involve basic mathematics of a level within the reach of most 15-year-olds. Use of financial formulae (requiring knowledge of algebra) is not considered appropriate. Dependence on calculation is minimised in the assessment; tasks are framed in such a way as to avoid the need for substantial or repetitive calculation. The calculators used by students in their classrooms and on the PISA mathematics assessment are also available in the financial literacy assessment, but success in the items will not depend on their use.

A similar reasoning holds for reading skills. It is assumed that all students taking part in the financial literacy assessment will have some basic reading proficiency, even while it is known from previous PISA surveys that reading skill varies widely both within and across countries (OECD, 2010b). To minimise the level of reading literacy required, stimulus material and task statements are generally designed to be as clear, simple and brief as possible. In some cases, however, the stimulus may deliberately present complex or somewhat technical language. The capacity to read and interpret the language of financial documents or pseudo financial documents is regarded as part of financial literacy.

Highly technical terminology relating to financial matters is avoided. The Financial Literacy Expert Group has advised on terms that it judges reasonable to expect 15-year-olds to understand. Some of these terms may be the focus of assessment tasks.

In practice, the results of the 2012 PISA financial literacy assessment gave a more precise measure of students’ performance in financial literacy in comparison with reading and mathematics performance. The results indicated that around 25% of the financial literacy score reflected factors that are uniquely captured by the financial literacy assessment, while the remaining 75% of the financial literacy score reflected skills measured in the mathematics and/or reading assessments.

The association between financial literacy and other domains indicates that, in general, students who perform at higher levels in mathematics and/or reading also perform well in financial literacy. There were, however, wide variations in financial literacy performance for any given level of performance in mathematics and reading, meaning that the skills measured by the financial literacy assessment went beyond or fell short of the ability to use the knowledge that students acquired from subjects taught in compulsory education. For instance, in Australia, the Flemish Community of Belgium, the Czech Republic, Estonia, New Zealand and the Russian Federation, students performed better in financial literacy than students in other countries with similar performance in mathematics and reading, while, by contrast, in France, Italy and Slovenia, students’ performance in financial literacy was lower, on average, when compared to that of students in the other participating countries and economies who displayed the same level of proficiency in reading and mathematics (OECD, 2014c).

### REPORTING FINANCIAL LITERACY

The data from the 2012 financial literacy assessment is held in a database separate from the main PISA database. In 2015, the data from all domains is presented together. The databases include, for the sampled students, their financial literacy, mathematics and reading cognitive results, the behaviour data from the short questionnaire on financial literacy, and data from the general student questionnaire and school questionnaire.

In each PISA cycle, financial literacy is reported as an independent result, and in relation to performance in other domains, financial behaviour, and to some background variables, such as socio-economic status and immigrant background. The data also allow for the development of further work under the aegis of the OECD Project on Financial Education.

The financial literacy cognitive data is scaled similarly to the other PISA data. A comprehensive description of the modelling technique used for scaling can be found in the *PISA 2012 Technical Report* (OECD, 2014d).

Each item is associated with a particular point on the PISA financial literacy scale that indicates its difficulty, and each student’s performance is associated with a particular point on the same scale that indicates the student’s estimated proficiency.

As with the other PISA domains, the relative difficulty of tasks in a test is estimated by considering the proportion of test takers answering each question correctly. The relative proficiency of students taking a particular test is estimated by considering the proportion of test items that they answer correctly. A single continuous scale showing the relationship between the difficulty of items and the proficiency of students is constructed.

Starting from the 2012 assessment, the scale was divided into levels, according to a set of statistical principles, and then descriptions were generated based on the tasks located within each level, to encapsulate the kinds of skills and knowledge needed to complete those tasks successfully. The scale and set of descriptions are known as a described proficiency scale.

By calibrating the difficulty of each item, it is possible to locate the degree of financial literacy that the item represents. By showing the proficiency of each student on the same scale, it is possible to describe the degree of financial literacy that the student possesses. The described proficiency scale helps to interpret what students' financial literacy scores mean in substantive terms.

Following PISA practice, a scale is constructed (based on participating OECD countries) having a mean of 500 and a standard deviation of 100. Five levels of proficiency in financial literacy are described in the assessment, as a first step in reporting how financial literacy develops, and to compare student performance between and within participating countries and economies (see OECD, 2014c, Chapter 2).

